{
 "cells": [
  {
   "cell_type": "code",
   "id": "ad318955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T18:54:05.494479Z",
     "start_time": "2025-11-01T18:54:05.449729Z"
    }
   },
   "source": [
    "from neo4j import GraphDatabase\n",
    "import csv\n",
    "import pandas as pd, unicodedata, regex, json\n",
    "from pathlib import Path\n",
    "from ftfy import fix_text\n",
    "from pathlib import Path\n",
    "import pysbd\n",
    "import regex as re "
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neo4j'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mneo4j\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GraphDatabase\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcsv\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m,\u001B[38;5;250m \u001B[39m\u001B[34;01municodedata\u001B[39;00m,\u001B[38;5;250m \u001B[39m\u001B[34;01mregex\u001B[39;00m,\u001B[38;5;250m \u001B[39m\u001B[34;01mjson\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'neo4j'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "2750cfcf",
   "metadata": {},
   "source": [
    "# Ingesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406ac341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura tus credenciales y URL de conexión\n",
    "URI = \"bolt://localhost:7687\" \n",
    "USER = \"neo4j\"\n",
    "PASSWORD = \"password\"\n",
    "\n",
    "# Crear driver\n",
    "driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43a54a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doi', 'author_count', 'publication_date', 'abstract', 'title', 'scopus_id', 'neo4jImportId', 'affiliation_count', 'pk', 'name', 'country', 'city', 'auth_name', 'citation_count', 'initials', 'current_affiliation', 'first_name', 'last_name', 'updated', 'cursor', 'next_url']\n"
     ]
    }
   ],
   "source": [
    "node_properties_query = \"\"\"\n",
    "MATCH (n)\n",
    "UNWIND keys(n) AS prop\n",
    "RETURN DISTINCT prop AS property_name\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(node_properties_query)\n",
    "    columns = [record[\"property_name\"] for record in result]\n",
    "\n",
    "print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df91f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\n    MATCH (a:Article)\\n    WHERE a.scopus_id IS NOT NULL\\n      AND a.title IS NOT NULL AND a.title <> \"\"\\n      AND a.abstract IS NOT NULL AND a.abstract <> \"\"\\n      AND a.doi IS NOT NULL AND a.doi <> \"\"\\n    MATCH (au:Author)-[:WROTE]->(a)\\n    WITH a, collect(DISTINCT au.first_name + \" \" + au.last_name) AS authors\\n    WHERE size(authors) > 0\\n    OPTIONAL MATCH (a)-[:BELONGS_TO]->(af:Affiliation)\\n    WITH a, authors,\\n         collect(DISTINCT af.name)    AS affiliations,\\n         collect(DISTINCT af.city)    AS affiliation_cities,\\n         collect(DISTINCT af.country) AS affiliation_countries\\n    RETURN\\n      a.scopus_id                    AS scopus_id,\\n      a.title                        AS title,\\n      a.abstract                     AS abstract,\\n      a.doi                          AS doi,\\n      authors                        AS authors,\\n      affiliations                   AS affiliations,\\n      affiliation_cities             AS affiliation_cities,\\n      affiliation_countries          AS affiliation_countries,\\n      coalesce(a.citation_count, 0)  AS citation_count\\n    ORDER BY scopus_id\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportación completada: scopusdata.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def export_articles_to_csv():\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Article)\n",
    "    WHERE a.scopus_id IS NOT NULL\n",
    "      AND a.title IS NOT NULL AND a.title <> \"\"\n",
    "      AND a.abstract IS NOT NULL AND a.abstract <> \"\"\n",
    "      AND a.doi IS NOT NULL AND a.doi <> \"\"\n",
    "    MATCH (au:Author)-[:WROTE]->(a)\n",
    "    WITH a, collect(DISTINCT au.first_name + \" \" + au.last_name) AS authors\n",
    "    WHERE size(authors) > 0\n",
    "    OPTIONAL MATCH (a)-[:BELONGS_TO]->(af:Affiliation)\n",
    "    WITH a, authors,\n",
    "         collect(DISTINCT af.name)    AS affiliations,\n",
    "         collect(DISTINCT af.city)    AS affiliation_cities,\n",
    "         collect(DISTINCT af.country) AS affiliation_countries\n",
    "    RETURN\n",
    "      a.scopus_id                    AS scopus_id,\n",
    "      a.title                        AS title,\n",
    "      a.abstract                     AS abstract,\n",
    "      a.doi                          AS doi,\n",
    "      authors                        AS authors,\n",
    "      affiliations                   AS affiliations,\n",
    "      affiliation_cities             AS affiliation_cities,\n",
    "      affiliation_countries          AS affiliation_countries,\n",
    "      coalesce(a.citation_count, 0)  AS citation_count\n",
    "    ORDER BY scopus_id\n",
    "    \"\"\"\n",
    "\n",
    "    # Ejecuta consulta y arma DataFrame\n",
    "    with driver.session() as session:\n",
    "        rows = [dict(r) for r in session.run(query)]\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Une listas con '; ' (evita introducir comas que confundan a quien lo lea a mano)\n",
    "    def join_list(x):\n",
    "        return \"; \".join(str(v) for v in x if v) if isinstance(x, list) else x\n",
    "\n",
    "    for col in [\"authors\", \"affiliations\", \"affiliation_cities\", \"affiliation_countries\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(join_list)\n",
    "\n",
    "    # Orden de columnas\n",
    "    df = df[[\n",
    "        \"title\", \"abstract\", \"doi\", \"authors\",\n",
    "        \"affiliations\", \"affiliation_cities\", \"affiliation_countries\",\n",
    "        \"citation_count\", \"scopus_id\"\n",
    "    ]]\n",
    "\n",
    "    # Exporta con separador '|'\n",
    "    # - quoting=QUOTE_MINIMAL: si algún campo contiene el separador '|', Pandas lo pondrá entre comillas.\n",
    "    # - lineterminator=\"\\n\": EOL consistente.\n",
    "    df.to_csv(\n",
    "        \"scopusdata.csv\",\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "        sep=\"|\",\n",
    "        quoting=csv.QUOTE_MINIMAL,\n",
    "        lineterminator=\"\\n\",\n",
    "    )\n",
    "    print(\"Exportación completada: scopusdata.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_articles_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c781b1",
   "metadata": {},
   "source": [
    "pseudocodigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf779b",
   "metadata": {},
   "source": [
    "# normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788aa3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo → /run/media/alech/backup/Github/tesis/processed.parquet\n",
      "                                          title_norm  \\\n",
      "0  thou shalt not die in this place : an ethnomet...   \n",
      "1  use of learning frames in climate change commu...   \n",
      "2  free access to public ecuadorian universities:...   \n",
      "\n",
      "                                       abstract_norm  \n",
      "0  ecuador, located in south america, has a popul...  \n",
      "1  differences in climate change learning frames ...  \n",
      "2  a free higher education policy was implemented...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- Configuración --------\n",
    "INPUT_CSV   = \"scopusdata.csv\"      # archivo con separador '|'\n",
    "OUTPUT_PATH = \"processed.parquet\"   # salida recomendada (parquet)\n",
    "REMOVE_ISOLATED_NUMBERS = False     # True si quieres quitar números sueltos\n",
    "\n",
    "# -------- Funciones --------\n",
    "def normalize_unicode_and_case(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = fix_text(s)                        # corrige codificación/caracteres raros\n",
    "    s = s.replace(\"\\u00A0\", \" \")           # NBSP -> espacio normal\n",
    "    s = unicodedata.normalize(\"NFC\", s)    # Unicode canónica\n",
    "    s = s.lower()                          # minúsculas\n",
    "    return s\n",
    "\n",
    "def strip_non_informative(s: str, remove_numbers: bool = False) -> str:\n",
    "    # Conserva letras/números/espacios y signos básicos de textos científicos\n",
    "    s = regex.sub(r\"[^\\p{L}\\p{N}\\s\\-\\.,;:()\\[\\]/%]\", \" \", s)\n",
    "    if remove_numbers:\n",
    "        # Elimina números aislados; conserva casos como \"co2\", \"iso-9001\"\n",
    "        s = regex.sub(r\"\\b\\d+\\b\", \" \", s)\n",
    "    s = regex.sub(r\"\\s+\", \" \", s).strip()  # espacios\n",
    "    return s\n",
    "\n",
    "def normalize_text(s: str, remove_numbers: bool = False) -> str:\n",
    "    s = normalize_unicode_and_case(s)\n",
    "    s = strip_non_informative(s, remove_numbers=remove_numbers)\n",
    "    return s\n",
    "\n",
    "def safe_convert_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convierte a dtypes 'seguros' sin depender de opciones globales.\"\"\"\n",
    "    try:\n",
    "        return df.convert_dtypes(dtype_backend=\"numpy_nullable\")  # pandas nuevos\n",
    "    except TypeError:\n",
    "        return df.convert_dtypes()  # pandas más viejos\n",
    "\n",
    "def sanitize_objects(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convierte objetos no escalares (listas/dicts) a JSON string.\"\"\"\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            df[c] = df[c].map(\n",
    "                lambda x: x if isinstance(x, (str, int, float, bool, type(None)))\n",
    "                else json.dumps(x, ensure_ascii=False)\n",
    "            )\n",
    "    return df\n",
    "\n",
    "def try_save_parquet(df: pd.DataFrame, path: str) -> bool:\n",
    "    \"\"\"Intenta guardar con fastparquet, luego pyarrow. Devuelve True si logra parquet.\"\"\"\n",
    "    # 1) fastparquet\n",
    "    try:\n",
    "        import fastparquet  # noqa: F401\n",
    "        df.to_parquet(path, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) pyarrow\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        df.to_parquet(path, index=False, engine=\"pyarrow\")  # compresión por defecto\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# -------- Proceso --------\n",
    "# Lee CSV con separador pipe. Si tu exportación puso comillas cuando había '|',\n",
    "# pandas las respeta automáticamente.\n",
    "df = pd.read_csv(INPUT_CSV, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "# Asegura presencia de columnas requeridas\n",
    "for col in [\"title\", \"abstract\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "# Normalización SOLO sobre texto analizable\n",
    "df[\"title_norm\"]    = df[\"title\"].fillna(\"\").map(lambda x: normalize_text(x, REMOVE_ISOLATED_NUMBERS))\n",
    "df[\"abstract_norm\"] = df[\"abstract\"].fillna(\"\").map(lambda x: normalize_text(x, REMOVE_ISOLATED_NUMBERS))\n",
    "\n",
    "# Guardar salida con metadatos originales + columnas normalizadas\n",
    "cols_out = list(df.columns)\n",
    "for c in [\"title_norm\", \"abstract_norm\"]:\n",
    "    if c not in cols_out:\n",
    "        cols_out.append(c)\n",
    "\n",
    "# Copia de salida + saneo de tipos\n",
    "out = df[cols_out].copy()\n",
    "out = safe_convert_dtypes(out)\n",
    "\n",
    "# Asegura strings planos en columnas de texto clave\n",
    "for c in [\"title\", \"abstract\", \"title_norm\", \"abstract_norm\"]:\n",
    "    if c in out.columns:\n",
    "        out[c] = out[c].astype(str)\n",
    "\n",
    "# Serializa objetos complejos a JSON para evitar fallos de parquet\n",
    "out = sanitize_objects(out)\n",
    "\n",
    "# -------- Guardado robusto --------\n",
    "parquet_ok = try_save_parquet(out, OUTPUT_PATH)\n",
    "\n",
    "if parquet_ok:\n",
    "    print(f\"Listo → {Path(OUTPUT_PATH).resolve()}\")\n",
    "else:\n",
    "    # Fallback a CSV para no perder progreso\n",
    "    fallback = Path(OUTPUT_PATH).with_suffix(\".csv\")\n",
    "    out.to_csv(fallback, index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "    print(\"No se pudo escribir Parquet con fastparquet ni pyarrow. \"\n",
    "          f\"Se guardó CSV en → {fallback.resolve()}\")\n",
    "\n",
    "# Vista rápida\n",
    "print(out[[\"title_norm\", \"abstract_norm\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d8ac3",
   "metadata": {},
   "source": [
    "### deteccion de idioma"
   ]
  },
  {
   "cell_type": "code",
   "id": "ff63916c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T18:53:41.192800Z",
     "start_time": "2025-11-01T18:52:16.972863Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "PATH = \"processed.parquet\"  \n",
    "OUT  = \"processed_lbl.parquet\"   \n",
    "\n",
    "DetectorFactory.seed = 0  # resultados más estables\n",
    "\n",
    "def detect_lang_safe(text: str) -> str:\n",
    "    t = (text or \"\").strip()\n",
    "    if not t:\n",
    "        return \"und\"  # indeterminado\n",
    "    try:\n",
    "        return detect(t)\n",
    "    except Exception:\n",
    "        return \"und\"\n",
    "\n",
    "# --- cargar parquet ---\n",
    "# intenta fastparquet y luego pyarrow\n",
    "try:\n",
    "    df = pd.read_parquet(PATH, engine=\"fastparquet\")\n",
    "except Exception:\n",
    "    df = pd.read_parquet(PATH, engine=\"pyarrow\")\n",
    "\n",
    "# --- elegir fuente para detección\n",
    "source_col = \"abstract_norm\" \n",
    "if source_col not in df.columns:\n",
    "    # si no existe ninguna, crea vacía para no romper\n",
    "    df[source_col] = \"\"\n",
    "\n",
    "# --- detectar idioma ---\n",
    "df[\"lang\"] = df[source_col].map(detect_lang_safe)\n",
    "\n",
    "# --- guardar ---\n",
    "try:\n",
    "    df.to_parquet(OUT, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        df.to_parquet(OUT, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        # último recurso: CSV para no perder el trabajo\n",
    "        Path(OUT).with_suffix(\".csv\")\n",
    "        df.to_csv(Path(OUT).with_suffix(\".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(df[[\"lang\", source_col]].head(5))\n"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 33\u001B[39m\n\u001B[32m     30\u001B[39m     df[source_col] = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# --- detectar idioma ---\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m df[\u001B[33m\"\u001B[39m\u001B[33mlang\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43msource_col\u001B[49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdetect_lang_safe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# --- guardar ---\u001B[39;00m\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.conda/envs/nlp311clean/lib/python3.11/site-packages/pandas/core/series.py:4700\u001B[39m, in \u001B[36mSeries.map\u001B[39m\u001B[34m(self, arg, na_action)\u001B[39m\n\u001B[32m   4620\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmap\u001B[39m(\n\u001B[32m   4621\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4622\u001B[39m     arg: Callable | Mapping | Series,\n\u001B[32m   4623\u001B[39m     na_action: Literal[\u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   4624\u001B[39m ) -> Series:\n\u001B[32m   4625\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4626\u001B[39m \u001B[33;03m    Map values of Series according to an input mapping or function.\u001B[39;00m\n\u001B[32m   4627\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4698\u001B[39m \u001B[33;03m    dtype: object\u001B[39;00m\n\u001B[32m   4699\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4700\u001B[39m     new_values = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4701\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._constructor(new_values, index=\u001B[38;5;28mself\u001B[39m.index, copy=\u001B[38;5;28;01mFalse\u001B[39;00m).__finalize__(\n\u001B[32m   4702\u001B[39m         \u001B[38;5;28mself\u001B[39m, method=\u001B[33m\"\u001B[39m\u001B[33mmap\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4703\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.conda/envs/nlp311clean/lib/python3.11/site-packages/pandas/core/base.py:921\u001B[39m, in \u001B[36mIndexOpsMixin._map_values\u001B[39m\u001B[34m(self, mapper, na_action, convert)\u001B[39m\n\u001B[32m    918\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[32m    919\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.map(mapper, na_action=na_action)\n\u001B[32m--> \u001B[39m\u001B[32m921\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.conda/envs/nlp311clean/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001B[39m, in \u001B[36mmap_array\u001B[39m\u001B[34m(arr, mapper, na_action, convert)\u001B[39m\n\u001B[32m   1741\u001B[39m values = arr.astype(\u001B[38;5;28mobject\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1743\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer_mask(\n\u001B[32m   1746\u001B[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001B[32m   1747\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mlib.pyx:2972\u001B[39m, in \u001B[36mpandas._libs.lib.map_infer\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 15\u001B[39m, in \u001B[36mdetect_lang_safe\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mund\u001B[39m\u001B[33m\"\u001B[39m  \u001B[38;5;66;03m# indeterminado\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdetect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m     17\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mund\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.conda/envs/nlp311clean/lib/python3.11/site-packages/langdetect/detector_factory.py:129\u001B[39m, in \u001B[36mdetect\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m    127\u001B[39m init_factory()\n\u001B[32m    128\u001B[39m detector = _factory.create()\n\u001B[32m--> \u001B[39m\u001B[32m129\u001B[39m \u001B[43mdetector\u001B[49m\u001B[43m.\u001B[49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m detector.detect()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.conda/envs/nlp311clean/lib/python3.11/site-packages/langdetect/detector.py:112\u001B[39m, in \u001B[36mDetector.append\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    110\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ch != \u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m pre != \u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m:\n\u001B[32m    111\u001B[39m     \u001B[38;5;28mself\u001B[39m.text += ch\n\u001B[32m--> \u001B[39m\u001B[32m112\u001B[39m pre = ch\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "aa0979f1",
   "metadata": {},
   "source": [
    "## segmentar oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63baabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo:\n",
      "   row_id_original  sentence_idx  \\\n",
      "0                0             0   \n",
      "1                0             1   \n",
      "2                0             2   \n",
      "3                0             3   \n",
      "4                0             4   \n",
      "5                0             5   \n",
      "6                0             6   \n",
      "7                0             7   \n",
      "8                0             8   \n",
      "9                0             9   \n",
      "\n",
      "                                            sentence  \n",
      "0  ecuador, located in south america, has a popul...  \n",
      "1  according to the national institution of stati...  \n",
      "2  palliative care and hospice are relatively new...  \n",
      "3  in ecuador people usually die at home, in hosp...  \n",
      "4  in 2012, the first ecuadorian hospice was crea...  \n",
      "5  according to symbolic interactionism theory, r...  \n",
      "6  symbolic interactionism proposes that human be...  \n",
      "7  through an ethnomethodological approach, the f...  \n",
      "8  results emerge from the introspection of real ...  \n",
      "9  based on a thematic analysis, the following st...  \n",
      "\n",
      "Guardado → /run/media/alech/backup/Github/tesis/processed_sentences.parquet\n"
     ]
    }
   ],
   "source": [
    "# -------- Configuración --------\n",
    "INPUT_PARQUET  = \"processed_lbl.parquet\"      # entrada\n",
    "OUTPUT_PARQUET = \"processed_sentences.parquet\"  # salida\n",
    "SOURCE_COL     = \"abstract_norm\"               # columna a segmentar\n",
    "\n",
    "# -------- Carga robusta --------\n",
    "def read_parquet_any(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_parquet(path, engine=\"fastparquet\")\n",
    "    except Exception:\n",
    "        return pd.read_parquet(path, engine=\"pyarrow\")\n",
    "\n",
    "# -------- Segmentadores (ES / EN) --------\n",
    "seg_es = pysbd.Segmenter(language=\"es\", clean=False)\n",
    "seg_en = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "\n",
    "def split_by_lang(text: str, lang: str = \"es\") -> list[str]:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    lang = (lang or \"es\").lower()\n",
    "    seg = seg_en if lang.startswith(\"en\") else seg_es\n",
    "    try:\n",
    "        return seg.segment(text.strip())\n",
    "    except Exception:\n",
    "        # fallback simple si falla pysbd\n",
    "        return [text.strip()]\n",
    "\n",
    "# -------- Proceso --------\n",
    "df = read_parquet_any(INPUT_PARQUET)\n",
    "\n",
    "if SOURCE_COL not in df.columns:\n",
    "    df[SOURCE_COL] = \"\"\n",
    "\n",
    "# si tienes columna de idioma, úsala; si no, asume \"es\"\n",
    "lang_series = df[\"lang\"] if \"lang\" in df.columns else [\"es\"] * len(df)\n",
    "\n",
    "# segmentar\n",
    "df[\"sentences\"] = [\n",
    "    split_by_lang(text, lang)\n",
    "    for text, lang in zip(df[SOURCE_COL], lang_series)\n",
    "]\n",
    "\n",
    "# una oración por fila\n",
    "out = df.explode(\"sentences\", ignore_index=False)\n",
    "out = out.rename(columns={\"sentences\": \"sentence\"})\n",
    "out = out.reset_index(names=\"row_id_original\")\n",
    "out[\"sentence_idx\"] = out.groupby(\"row_id_original\").cumcount()\n",
    "\n",
    "# columnas finales\n",
    "keep = []\n",
    "for c in [\"scopus_id\", \"title\", \"abstract\", \"abstract_norm\", \"lang\"]:\n",
    "    if c in out.columns:\n",
    "        keep.append(c)\n",
    "keep += [\"row_id_original\", \"sentence_idx\", \"sentence\"]\n",
    "out = out[keep]\n",
    "\n",
    "# -------- Guardar --------\n",
    "try:\n",
    "    out.to_parquet(OUTPUT_PARQUET, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        out.to_parquet(OUTPUT_PARQUET, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        out.to_csv(Path(OUTPUT_PARQUET).with_suffix(\".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Ejemplo:\")\n",
    "print(out[[\"row_id_original\", \"sentence_idx\", \"sentence\"]].head(10))\n",
    "print(f\"\\nGuardado → {Path(OUTPUT_PARQUET).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbe64b",
   "metadata": {},
   "source": [
    "## tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae16b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0  ecuador, located in south america, has a popul...   \n",
      "1  according to the national institution of stati...   \n",
      "2  palliative care and hospice are relatively new...   \n",
      "3  in ecuador people usually die at home, in hosp...   \n",
      "4  in 2012, the first ecuadorian hospice was crea...   \n",
      "5  according to symbolic interactionism theory, r...   \n",
      "6  symbolic interactionism proposes that human be...   \n",
      "7  through an ethnomethodological approach, the f...   \n",
      "\n",
      "                                          tokens_csv  \n",
      "0  ecuador,located,in south america,has,a,populat...  \n",
      "1  according to the,national,institution,of,stati...  \n",
      "2  palliative care,and,hospice,are,relatively new...  \n",
      "3  in,ecuador,people,usually,die,at home,in,hospi...  \n",
      "4   in,2012,the,first,ecuadorian,hospice,was created  \n",
      "5  according to,symbolic,interactionism,theory,re...  \n",
      "6  symbolic,interactionism,proposes,that,human be...  \n",
      "7  through,an,ethnomethodological,approach,the,fo...  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "INPUT_PARQUET  = \"processed_sentences.parquet\"\n",
    "OUTPUT_PARQUET = \"corpus_token\"\n",
    "SENT_COL = \"sentence\"\n",
    "\n",
    "TOKEN_RE = re.compile(r\"(?:[^\\W_]+(?:[-_][^\\W_]+)+|\\d+(?:\\.\\d+)+|[^\\W_]+)\", re.VERBOSE | re.IGNORECASE | re.UNICODE)\n",
    "def simple_tokenize(s:str):\n",
    "    if not isinstance(s, str): return []\n",
    "    return TOKEN_RE.findall(re.sub(r\"\\s+\", \" \", s.strip()))\n",
    "\n",
    "# 1) Carga y tokeniza todas las oraciones\n",
    "def read_parquet_any(p):\n",
    "    try: return pd.read_parquet(p, engine=\"fastparquet\")\n",
    "    except Exception: return pd.read_parquet(p, engine=\"pyarrow\")\n",
    "\n",
    "df = read_parquet_any(INPUT_PARQUET)\n",
    "df[\"tokens_base\"] = df[SENT_COL].map(simple_tokenize)\n",
    "\n",
    "# 2) Entrena bigramas y (opcional) trigramas\n",
    "sentences = df[\"tokens_base\"].tolist()\n",
    "\n",
    "# ⚠️ delimiter must be str if tokens are str\n",
    "bigram = Phrases(sentences, min_count=5, threshold=10.0, delimiter=\" \")\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "trigram = Phrases(bigram_phraser[sentences], min_count=5, threshold=10.0, delimiter=\" \")\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# 3) Aplica: pega frases automáticamente (p.ej., aprendizaje_automático)\n",
    "df[\"tokens\"] = [trigram_phraser[bigram_phraser[toks]] for toks in df[\"tokens_base\"]]\n",
    "df[\"tokens_csv\"] = df[\"tokens\"].map(lambda xs: \",\".join(xs))\n",
    "df[\"n_tokens\"] = df[\"tokens\"].map(len)\n",
    "\n",
    "# 4) Guarda\n",
    "try:\n",
    "    df.to_parquet(OUTPUT_PARQUET, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try: df.to_parquet(OUTPUT_PARQUET, index=False, engine=\"pyarrow\")\n",
    "    except Exception: df.to_csv(Path(OUTPUT_PARQUET).with_suffix(\".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(df[[SENT_COL, \"tokens_csv\"]].head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0786c93",
   "metadata": {},
   "source": [
    "### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f7801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0  ecuador, located in south america, has a popul...   \n",
      "1  according to the national institution of stati...   \n",
      "2  palliative care and hospice are relatively new...   \n",
      "3  in ecuador people usually die at home, in hosp...   \n",
      "4  in 2012, the first ecuadorian hospice was crea...   \n",
      "5  according to symbolic interactionism theory, r...   \n",
      "6  symbolic interactionism proposes that human be...   \n",
      "7  through an ethnomethodological approach, the f...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [ecuador, located, in south america, has, a, p...   \n",
      "1  [according to the, national, institution, of, ...   \n",
      "2  [palliative care, and, hospice, are, relativel...   \n",
      "3  [in, ecuador, people, usually, die, at home, i...   \n",
      "4  [in, 2012, the, first, ecuadorian, hospice, wa...   \n",
      "5  [according to, symbolic, interactionism, theor...   \n",
      "6  [symbolic, interactionism, proposes, that, hum...   \n",
      "7  [through, an, ethnomethodological, approach, t...   \n",
      "\n",
      "                                       tokens_nostop  \\\n",
      "0  [ecuador, located, in south america, populatio...   \n",
      "1  [according to the, national, institution, stat...   \n",
      "2  [palliative care, hospice, relatively new, con...   \n",
      "3  [ecuador, people, usually, die, at home, hospi...   \n",
      "4          [first, ecuadorian, hospice, was created]   \n",
      "5  [according to, symbolic, interactionism, theor...   \n",
      "6  [symbolic, interactionism, proposes, human bei...   \n",
      "7  [ethnomethodological, approach, following, res...   \n",
      "\n",
      "                                      text_for_embed  \n",
      "0  ecuador located in south america population mi...  \n",
      "1  according to the national institution statisti...  \n",
      "2  palliative care hospice relatively new concept...  \n",
      "3  ecuador people usually die at home hospitals n...  \n",
      "4               first ecuadorian hospice was created  \n",
      "5  according to symbolic interactionism theory re...  \n",
      "6  symbolic interactionism proposes human beings ...  \n",
      "7  ethnomethodological approach following researc...  \n",
      "Guardado en: corpus_token_nostop.parquet\n"
     ]
    }
   ],
   "source": [
    "# === Quitar stopwords sobre df[\"tokens\"] con n-gramas separados por espacio ===\n",
    "import re, unicodedata, os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stop\n",
    "\n",
    "# Asegura recurso stopwords NLTK\n",
    "try:\n",
    "    _ = nltk_stop.words(\"spanish\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "# Idiomas a filtrar (ajusta a [\"spanish\"] si quieres solo ES)\n",
    "LANGS = [\"spanish\", \"english\"]\n",
    "\n",
    "# Construye set de stopwords\n",
    "STOPSET = set()\n",
    "for lang in LANGS:\n",
    "    try:\n",
    "        STOPSET |= set(nltk_stop.words(lang))\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "STOPSET_NORM = {_norm(w) for w in STOPSET}\n",
    "\n",
    "# Frases que no se filtran nunca (escribe aquí con ESPACIOS)\n",
    "PROTECT_PHRASES = {\n",
    "    \"in south america\",\n",
    "    # añade más si quieres: \"public health\", \"quality of life\", ...\n",
    "}\n",
    "\n",
    "def is_stop(tok: str) -> bool:\n",
    "    \"\"\"\n",
    "    Mantén n-gramas con contenido: elimina solo si TODAS las partes\n",
    "    (separadas por espacio o guion) son stopwords; protege frases explícitas.\n",
    "    \"\"\"\n",
    "    if not isinstance(tok, str) or not tok:\n",
    "        return True  # vacío o no-string -> descartar\n",
    "\n",
    "    t = _norm(tok).strip()\n",
    "\n",
    "    # Protección explícita\n",
    "    if t in PROTECT_PHRASES:\n",
    "        return False\n",
    "\n",
    "    # Token simple (sin espacios ni guiones)\n",
    "    if (\" \" not in t) and (\"-\" not in t):\n",
    "        return t in STOPSET_NORM\n",
    "\n",
    "    # Token compuesto: separa por espacios o guiones (uno o más)\n",
    "    parts = [p for p in re.split(r\"[ \\-]+\", t) if p]\n",
    "    if not parts:\n",
    "        return True\n",
    "\n",
    "    # Elimina SOLO si *todas* las partes son stopwords\n",
    "    return all(p in STOPSET_NORM for p in parts)\n",
    "\n",
    "def filter_tokens(tokens, min_len=2, drop_numeric=True):\n",
    "    out = []\n",
    "    if not isinstance(tokens, (list, tuple)):\n",
    "        return out\n",
    "    for t in tokens:\n",
    "        if not isinstance(t, str) or not t:\n",
    "            continue\n",
    "        if drop_numeric and t.isnumeric():\n",
    "            continue\n",
    "        if len(t) < min_len:\n",
    "            continue\n",
    "        if is_stop(t):\n",
    "            continue\n",
    "        out.append(t)\n",
    "    return out\n",
    "\n",
    "# --- Aplicar al DF (requiere df[\"tokens\"] como lista de strings) ---\n",
    "if \"tokens\" not in df.columns:\n",
    "    raise KeyError(\"Se esperaba df['tokens']. Asegúrate de haber generado los n-gramas antes.\")\n",
    "\n",
    "df[\"tokens_nostop\"] = df[\"tokens\"].map(filter_tokens)\n",
    "\n",
    "# (Opcional) Texto para embeddings (bi-encoder): tokens unidos por espacio\n",
    "df[\"text_for_embed\"] = df[\"tokens_nostop\"].map(lambda xs: \" \".join(xs))\n",
    "\n",
    "# Vistazo rápido (muestra si existen)\n",
    "cols_show = [c for c in [\"sentence\", \"tokens\", \"tokens_nostop\", \"text_for_embed\"] if c in df.columns]\n",
    "print(df[cols_show].head(8))\n",
    "\n",
    "# --- Guardar a nuevo archivo para no sobrescribir el original ---\n",
    "OUT_BASE = \"corpus_token_nostop\"\n",
    "parquet_path = f\"{OUT_BASE}.parquet\"\n",
    "try:\n",
    "    df.to_parquet(parquet_path, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        df.to_parquet(parquet_path, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        df.to_csv(f\"{OUT_BASE}.csv\", index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Guardado en:\",\n",
    "      parquet_path if os.path.exists(parquet_path) else f\"{OUT_BASE}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263b065",
   "metadata": {},
   "source": [
    "## lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef675ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      text_for_embed  \\\n",
      "0  ecuador located in south america population mi...   \n",
      "1  according to the national institution statisti...   \n",
      "2  palliative care hospice relatively new concept...   \n",
      "3  ecuador people usually die at home hospitals n...   \n",
      "4               first ecuadorian hospice was created   \n",
      "5  according to symbolic interactionism theory re...   \n",
      "6  symbolic interactionism proposes human beings ...   \n",
      "7  ethnomethodological approach following researc...   \n",
      "\n",
      "                                          text_lemma  \n",
      "0  ecuador locate in south america population mil...  \n",
      "1  accord to the national institution statistic e...  \n",
      "2  palliative care hospice relatively new concept...  \n",
      "3  ecuador people usually die at home hospital nu...  \n",
      "4                 first ecuadorian hospice be create  \n",
      "5  accord to symbolic interactionism theory resea...  \n",
      "6  symbolic interactionism propose human being ca...  \n",
      "7  ethnomethodological approach follow research a...  \n",
      "Guardado en: corpus_token_nostop_lemma.parquet\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZE text_for_embed (ES/EN) con spaCy en batch\n",
    "import re, unicodedata, os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"corpus_token_nostop.parquet\")\n",
    "\n",
    "assert \"text_for_embed\" in df.columns, \"Falta la columna 'text_for_embed'.\"\n",
    "\n",
    "# Carga modelos spaCy (puedes usar *_md/_lg si los tienes)\n",
    "nlp_es = spacy.load(\"es_core_news_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "nlp_en = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "\n",
    "# Heurística ligera para detectar español\n",
    "SPANISH_CUES = {\"de\",\"la\",\"el\",\"los\",\"las\",\"y\",\"en\",\"para\",\"con\",\"por\",\"del\",\"al\",\"un\",\"una\",\"unos\",\"unas\",\"se\",\"su\",\"sus\"}\n",
    "ACCENTS_RE = re.compile(r\"[áéíóúñüÁÉÍÓÚÑÜ]\")\n",
    "\n",
    "def _is_spanish_like(text: str) -> bool:\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    if ACCENTS_RE.search(text):\n",
    "        return True\n",
    "    words = [w.lower() for w in re.split(r\"\\s+\", text.strip()) if w]\n",
    "    # si al menos una palabra típica española aparece -> ES\n",
    "    return any(w in SPANISH_CUES for w in words)\n",
    "\n",
    "def _lemma_doc(doc):\n",
    "    # cuida pronombres (algunos modelos antiguos devuelven \"-PRON-\")\n",
    "    toks = []\n",
    "    for t in doc:\n",
    "        lem = t.lemma_ if t.lemma_ and t.lemma_ != \"-PRON-\" else t.text\n",
    "        toks.append(lem.lower())\n",
    "    # une con espacios (mismo formato que text_for_embed)\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# Divide índices por idioma con la heurística\n",
    "idx_es = df.index[df[\"text_for_embed\"].apply(_is_spanish_like)].tolist()\n",
    "idx_en = df.index.difference(idx_es).tolist()\n",
    "\n",
    "# Lematiza en batch por idioma (más rápido que fila a fila)\n",
    "text_lemma = pd.Series(index=df.index, dtype=object)\n",
    "\n",
    "if idx_es:\n",
    "    docs_es = nlp_es.pipe(df.loc[idx_es, \"text_for_embed\"].fillna(\"\"), batch_size=512, n_process=1)\n",
    "    for i, doc in zip(idx_es, docs_es):\n",
    "        text_lemma.loc[i] = _lemma_doc(doc)\n",
    "\n",
    "if idx_en:\n",
    "    docs_en = nlp_en.pipe(df.loc[idx_en, \"text_for_embed\"].fillna(\"\"), batch_size=512, n_process=1)\n",
    "    for i, doc in zip(idx_en, docs_en):\n",
    "        text_lemma.loc[i] = _lemma_doc(doc)\n",
    "\n",
    "# Asigna columna nueva\n",
    "df[\"text_lemma\"] = text_lemma.fillna(\"\")\n",
    "\n",
    "# Vista rápida\n",
    "print(df[[\"text_for_embed\", \"text_lemma\"]].head(8))\n",
    "\n",
    "# Guardar (nuevo archivo para no pisar el anterior)\n",
    "OUT = \"corpus_token_nostop_lemma.parquet\"\n",
    "try:\n",
    "    df.to_parquet(OUT, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        df.to_parquet(OUT, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        df.to_csv(\"corpus_token_nostop_lemma.csv\", index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Guardado en:\", OUT if os.path.exists(OUT) else \"corpus_token_nostop_lemma.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788917fb",
   "metadata": {},
   "source": [
    "## chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07593bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc_id  chunk_id chunk_uid    scopus_id  start_token  end_token  \\\n",
      "0       0         0       0-0  85059061481            0        156   \n",
      "1       1         0       1-0  85061967853            0         94   \n",
      "2       2         0       2-0  85067792389            0         60   \n",
      "3       3         0       3-0  85068192726            0        255   \n",
      "4       4         0       4-0  85069901345            0        173   \n",
      "5       5         0       5-0  85070472925            0        205   \n",
      "6       6         0       6-0  85071977997            0        195   \n",
      "7       7         0       7-0  85072017885            0        169   \n",
      "\n",
      "   token_count                                         text_chunk  \n",
      "0          156  ecuador locate in south america population mil...  \n",
      "1           94  difference climate change learning frame pedag...  \n",
      "2           60  free high education policy be implement ecuado...  \n",
      "3          255  this study explore influence family member lif...  \n",
      "4          173  rapid adoption diversification cloud compute t...  \n",
      "5          205  traditional pretest prove homoscedasticity e g...  \n",
      "6          195  polymer electrolyte fuel cell pefc produce ele...  \n",
      "7          169  traffic prediction high accuracy have become v...  \n",
      "N docs (únicos scopus_id): 19233 | N chunks: 21005\n",
      "Guardado en: corpus_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "# ====== CHUNKING *SIEMPRE* DESDE text_lemma (agrupado por scopus_id) ======\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "IN_PARQUET  = os.environ.get(\"IN_PARQUET\", \"corpus_token_nostop_lemma.parquet\")\n",
    "OUT_CHUNKS  = os.environ.get(\"OUT_CHUNKS\", \"corpus_chunks.parquet\")\n",
    "\n",
    "# --- Parámetros de chunking ---\n",
    "MAX_TOKENS      = int(os.environ.get(\"MAX_TOKENS\", \"300\"))      # 200–400 recomendado\n",
    "OVERLAP_RATIO   = float(os.environ.get(\"OVERLAP_RATIO\", \"0.2\"))  # 15–30% recomendado\n",
    "OVERLAP_TOKENS  = int(MAX_TOKENS * OVERLAP_RATIO)\n",
    "STRIDE          = max(1, MAX_TOKENS - OVERLAP_TOKENS)\n",
    "\n",
    "# Tokenizer E5 (coherente con embeddings e5*)\n",
    "TOKENIZER_NAME = \"intfloat/multilingual-e5-base\"\n",
    "tok = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "# ---------- 0) Carga y validación ----------\n",
    "df = pd.read_parquet(IN_PARQUET)\n",
    "\n",
    "assert \"scopus_id\" in df.columns, \"Falta columna 'scopus_id' en el parquet.\"\n",
    "assert \"text_lemma\" in df.columns, \"Falta columna 'text_lemma' (se usa siempre).\"\n",
    "\n",
    "# Normaliza tipos / limpieza básica\n",
    "df[\"scopus_id\"]  = df[\"scopus_id\"].astype(str)\n",
    "df[\"text_lemma\"] = (\n",
    "    df[\"text_lemma\"].fillna(\"\").astype(str)\n",
    "      .str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    ")\n",
    "\n",
    "# ---------- 1) Construir texto lematizado por documento ----------\n",
    "# Orden preferido: sentence_idx > row_id_original > orden actual\n",
    "sort_keys = [\"scopus_id\"]\n",
    "if \"sentence_idx\" in df.columns:\n",
    "    sort_keys += [\"sentence_idx\"]\n",
    "elif \"row_id_original\" in df.columns:\n",
    "    sort_keys += [\"row_id_original\"]\n",
    "\n",
    "df = df.sort_values(sort_keys, kind=\"mergesort\")\n",
    "\n",
    "# Texto lematizado consolidado por scopus_id\n",
    "agg_text = df.groupby(\"scopus_id\")[\"text_lemma\"].apply(\n",
    "    lambda s: \" \".join([t for t in s.astype(str) if t])\n",
    ").rename(\"text_for_chunk\")\n",
    "\n",
    "# Solo scopus_id + texto consolidado\n",
    "doc_df = agg_text.to_frame().reset_index()\n",
    "doc_df[\"doc_id\"] = np.arange(len(doc_df), dtype=\"int64\")\n",
    "\n",
    "# Limpieza final del texto\n",
    "doc_df[\"text_for_chunk\"] = (\n",
    "    doc_df[\"text_for_chunk\"].fillna(\"\").astype(str)\n",
    "       .str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    ")\n",
    "doc_df = doc_df[doc_df[\"text_for_chunk\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# ---------- 2) Chunker por tokens ----------\n",
    "def chunk_text_by_tokens(text: str, max_tokens: int = MAX_TOKENS, stride: int = STRIDE):\n",
    "    ids = tok.encode(text, add_special_tokens=False)\n",
    "    n = len(ids)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = min(start + max_tokens, n)\n",
    "        sl = ids[start:end]\n",
    "        chunk_txt = tok.decode(sl, skip_special_tokens=True).strip()\n",
    "        if chunk_txt:\n",
    "            chunks.append({\n",
    "                \"start_token\": start,\n",
    "                \"end_token\": end,\n",
    "                \"token_count\": end - start,\n",
    "                \"text_chunk\": chunk_txt\n",
    "            })\n",
    "        if end == n:\n",
    "            break\n",
    "        start += stride\n",
    "    return chunks\n",
    "\n",
    "# ---------- 3) Generar filas de chunks (solo campos mínimos + scopus_id) ----------\n",
    "rows = []\n",
    "for _, r in doc_df.iterrows():\n",
    "    doc_id = int(r[\"doc_id\"])\n",
    "    scid   = str(r[\"scopus_id\"])\n",
    "    text   = r[\"text_for_chunk\"]\n",
    "    for j, ch in enumerate(chunk_text_by_tokens(text, MAX_TOKENS, STRIDE)):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": j,\n",
    "            \"chunk_uid\": f\"{doc_id}-{j}\",\n",
    "            \"scopus_id\": scid,\n",
    "            \"start_token\": ch[\"start_token\"],\n",
    "            \"end_token\": ch[\"end_token\"],\n",
    "            \"token_count\": ch[\"token_count\"],\n",
    "            \"text_chunk\": ch[\"text_chunk\"],\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(rows)\n",
    "\n",
    "print(chunks_df.head(8))\n",
    "print(\"N docs (únicos scopus_id):\", doc_df.shape[0], \"| N chunks:\", chunks_df.shape[0])\n",
    "\n",
    "# ---------- 4) Guardar (solo mínimos) ----------\n",
    "save_cols = [\n",
    "    \"doc_id\",\"chunk_id\",\"chunk_uid\",\"scopus_id\",\n",
    "    \"start_token\",\"end_token\",\"token_count\",\"text_chunk\"\n",
    "]\n",
    "chunks_out = chunks_df[save_cols].copy()\n",
    "\n",
    "# Parquet (pyarrow preferente)\n",
    "try:\n",
    "    chunks_out.to_parquet(OUT_CHUNKS, index=False, engine=\"pyarrow\")\n",
    "except Exception:\n",
    "    try:\n",
    "        chunks_out.to_parquet(OUT_CHUNKS, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "    except Exception:\n",
    "        chunks_out.to_csv(OUT_CHUNKS.replace(\".parquet\", \".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Guardado en:\", OUT_CHUNKS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afbb74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device encode: cpu; model=intfloat/multilingual-e5-small\n",
      "[INFO] model.max_seq_length = 300\n",
      "[INFO] FAISS-CPU\n",
      "[PROG] 32/21005 (0.2%) - bs=32\n",
      "[PROG] 64/21005 (0.3%) - bs=32\n",
      "[PROG] 96/21005 (0.5%) - bs=32\n",
      "[PROG] 128/21005 (0.6%) - bs=32\n",
      "[PROG] 160/21005 (0.8%) - bs=32\n",
      "[PROG] 192/21005 (0.9%) - bs=32\n",
      "[PROG] 224/21005 (1.1%) - bs=32\n",
      "[PROG] 256/21005 (1.2%) - bs=32\n",
      "[PROG] 288/21005 (1.4%) - bs=32\n",
      "[PROG] 320/21005 (1.5%) - bs=32\n",
      "[PROG] 352/21005 (1.7%) - bs=32\n",
      "[PROG] 384/21005 (1.8%) - bs=32\n",
      "[PROG] 416/21005 (2.0%) - bs=32\n",
      "[PROG] 448/21005 (2.1%) - bs=32\n",
      "[PROG] 480/21005 (2.3%) - bs=32\n",
      "[PROG] 512/21005 (2.4%) - bs=32\n",
      "[PROG] 544/21005 (2.6%) - bs=32\n",
      "[PROG] 576/21005 (2.7%) - bs=32\n",
      "[PROG] 608/21005 (2.9%) - bs=32\n",
      "[PROG] 640/21005 (3.0%) - bs=32\n",
      "[PROG] 672/21005 (3.2%) - bs=32\n",
      "[PROG] 704/21005 (3.4%) - bs=32\n",
      "[PROG] 736/21005 (3.5%) - bs=32\n",
      "[PROG] 768/21005 (3.7%) - bs=32\n",
      "[PROG] 800/21005 (3.8%) - bs=32\n",
      "[PROG] 832/21005 (4.0%) - bs=32\n",
      "[PROG] 864/21005 (4.1%) - bs=32\n",
      "[PROG] 896/21005 (4.3%) - bs=32\n",
      "[PROG] 928/21005 (4.4%) - bs=32\n",
      "[PROG] 960/21005 (4.6%) - bs=32\n",
      "[PROG] 992/21005 (4.7%) - bs=32\n",
      "[PROG] 1024/21005 (4.9%) - bs=32\n",
      "[PROG] 1056/21005 (5.0%) - bs=32\n",
      "[PROG] 1088/21005 (5.2%) - bs=32\n",
      "[PROG] 1120/21005 (5.3%) - bs=32\n",
      "[PROG] 1152/21005 (5.5%) - bs=32\n",
      "[PROG] 1184/21005 (5.6%) - bs=32\n",
      "[PROG] 1216/21005 (5.8%) - bs=32\n",
      "[PROG] 1248/21005 (5.9%) - bs=32\n",
      "[PROG] 1280/21005 (6.1%) - bs=32\n",
      "[PROG] 1312/21005 (6.2%) - bs=32\n",
      "[PROG] 1344/21005 (6.4%) - bs=32\n",
      "[PROG] 1376/21005 (6.6%) - bs=32\n",
      "[PROG] 1408/21005 (6.7%) - bs=32\n",
      "[PROG] 1440/21005 (6.9%) - bs=32\n",
      "[PROG] 1472/21005 (7.0%) - bs=32\n",
      "[PROG] 1504/21005 (7.2%) - bs=32\n",
      "[PROG] 1536/21005 (7.3%) - bs=32\n",
      "[PROG] 1568/21005 (7.5%) - bs=32\n",
      "[PROG] 1600/21005 (7.6%) - bs=32\n",
      "[PROG] 1632/21005 (7.8%) - bs=32\n",
      "[PROG] 1664/21005 (7.9%) - bs=32\n",
      "[PROG] 1696/21005 (8.1%) - bs=32\n",
      "[PROG] 1728/21005 (8.2%) - bs=32\n",
      "[PROG] 1760/21005 (8.4%) - bs=32\n",
      "[PROG] 1792/21005 (8.5%) - bs=32\n",
      "[PROG] 1824/21005 (8.7%) - bs=32\n",
      "[PROG] 1856/21005 (8.8%) - bs=32\n",
      "[PROG] 1888/21005 (9.0%) - bs=32\n",
      "[PROG] 1920/21005 (9.1%) - bs=32\n",
      "[PROG] 1952/21005 (9.3%) - bs=32\n",
      "[PROG] 1984/21005 (9.4%) - bs=32\n",
      "[PROG] 2016/21005 (9.6%) - bs=32\n",
      "[PROG] 2048/21005 (9.8%) - bs=32\n",
      "[PROG] 2080/21005 (9.9%) - bs=32\n",
      "[PROG] 2112/21005 (10.1%) - bs=32\n",
      "[PROG] 2144/21005 (10.2%) - bs=32\n",
      "[PROG] 2176/21005 (10.4%) - bs=32\n",
      "[PROG] 2208/21005 (10.5%) - bs=32\n",
      "[PROG] 2240/21005 (10.7%) - bs=32\n",
      "[PROG] 2272/21005 (10.8%) - bs=32\n",
      "[PROG] 2304/21005 (11.0%) - bs=32\n",
      "[PROG] 2336/21005 (11.1%) - bs=32\n",
      "[PROG] 2368/21005 (11.3%) - bs=32\n",
      "[PROG] 2400/21005 (11.4%) - bs=32\n",
      "[PROG] 2432/21005 (11.6%) - bs=32\n",
      "[PROG] 2464/21005 (11.7%) - bs=32\n",
      "[PROG] 2496/21005 (11.9%) - bs=32\n",
      "[PROG] 2528/21005 (12.0%) - bs=32\n",
      "[PROG] 2560/21005 (12.2%) - bs=32\n",
      "[PROG] 2592/21005 (12.3%) - bs=32\n",
      "[PROG] 2624/21005 (12.5%) - bs=32\n",
      "[PROG] 2656/21005 (12.6%) - bs=32\n",
      "[PROG] 2688/21005 (12.8%) - bs=32\n",
      "[PROG] 2720/21005 (12.9%) - bs=32\n",
      "[PROG] 2752/21005 (13.1%) - bs=32\n",
      "[PROG] 2784/21005 (13.3%) - bs=32\n",
      "[PROG] 2816/21005 (13.4%) - bs=32\n",
      "[PROG] 2848/21005 (13.6%) - bs=32\n",
      "[PROG] 2880/21005 (13.7%) - bs=32\n",
      "[PROG] 2912/21005 (13.9%) - bs=32\n",
      "[PROG] 2944/21005 (14.0%) - bs=32\n",
      "[PROG] 2976/21005 (14.2%) - bs=32\n",
      "[PROG] 3008/21005 (14.3%) - bs=32\n",
      "[PROG] 3040/21005 (14.5%) - bs=32\n",
      "[PROG] 3072/21005 (14.6%) - bs=32\n",
      "[PROG] 3104/21005 (14.8%) - bs=32\n",
      "[PROG] 3136/21005 (14.9%) - bs=32\n",
      "[PROG] 3168/21005 (15.1%) - bs=32\n",
      "[PROG] 3200/21005 (15.2%) - bs=32\n",
      "[PROG] 3232/21005 (15.4%) - bs=32\n",
      "[PROG] 3264/21005 (15.5%) - bs=32\n",
      "[PROG] 3296/21005 (15.7%) - bs=32\n",
      "[PROG] 3328/21005 (15.8%) - bs=32\n",
      "[PROG] 3360/21005 (16.0%) - bs=32\n",
      "[PROG] 3392/21005 (16.1%) - bs=32\n",
      "[PROG] 3424/21005 (16.3%) - bs=32\n",
      "[PROG] 3456/21005 (16.5%) - bs=32\n",
      "[PROG] 3488/21005 (16.6%) - bs=32\n",
      "[PROG] 3520/21005 (16.8%) - bs=32\n",
      "[PROG] 3552/21005 (16.9%) - bs=32\n",
      "[PROG] 3584/21005 (17.1%) - bs=32\n",
      "[PROG] 3616/21005 (17.2%) - bs=32\n",
      "[PROG] 3648/21005 (17.4%) - bs=32\n",
      "[PROG] 3680/21005 (17.5%) - bs=32\n",
      "[PROG] 3712/21005 (17.7%) - bs=32\n",
      "[PROG] 3744/21005 (17.8%) - bs=32\n",
      "[PROG] 3776/21005 (18.0%) - bs=32\n",
      "[PROG] 3808/21005 (18.1%) - bs=32\n",
      "[PROG] 3840/21005 (18.3%) - bs=32\n",
      "[PROG] 3872/21005 (18.4%) - bs=32\n",
      "[PROG] 3904/21005 (18.6%) - bs=32\n",
      "[PROG] 3936/21005 (18.7%) - bs=32\n",
      "[PROG] 3968/21005 (18.9%) - bs=32\n",
      "[PROG] 4000/21005 (19.0%) - bs=32\n",
      "[PROG] 4032/21005 (19.2%) - bs=32\n",
      "[PROG] 4064/21005 (19.3%) - bs=32\n",
      "[PROG] 4096/21005 (19.5%) - bs=32\n",
      "[PROG] 4128/21005 (19.7%) - bs=32\n",
      "[PROG] 4160/21005 (19.8%) - bs=32\n",
      "[PROG] 4192/21005 (20.0%) - bs=32\n",
      "[PROG] 4224/21005 (20.1%) - bs=32\n",
      "[PROG] 4256/21005 (20.3%) - bs=32\n",
      "[PROG] 4288/21005 (20.4%) - bs=32\n",
      "[PROG] 4320/21005 (20.6%) - bs=32\n",
      "[PROG] 4352/21005 (20.7%) - bs=32\n",
      "[PROG] 4384/21005 (20.9%) - bs=32\n",
      "[PROG] 4416/21005 (21.0%) - bs=32\n",
      "[PROG] 4448/21005 (21.2%) - bs=32\n",
      "[PROG] 4480/21005 (21.3%) - bs=32\n",
      "[PROG] 4512/21005 (21.5%) - bs=32\n",
      "[PROG] 4544/21005 (21.6%) - bs=32\n",
      "[PROG] 4576/21005 (21.8%) - bs=32\n",
      "[PROG] 4608/21005 (21.9%) - bs=32\n",
      "[PROG] 4640/21005 (22.1%) - bs=32\n",
      "[PROG] 4672/21005 (22.2%) - bs=32\n",
      "[PROG] 4704/21005 (22.4%) - bs=32\n",
      "[PROG] 4736/21005 (22.5%) - bs=32\n",
      "[PROG] 4768/21005 (22.7%) - bs=32\n",
      "[PROG] 4800/21005 (22.9%) - bs=32\n",
      "[PROG] 4832/21005 (23.0%) - bs=32\n",
      "[PROG] 4864/21005 (23.2%) - bs=32\n",
      "[PROG] 4896/21005 (23.3%) - bs=32\n",
      "[PROG] 4928/21005 (23.5%) - bs=32\n",
      "[PROG] 4960/21005 (23.6%) - bs=32\n",
      "[PROG] 4992/21005 (23.8%) - bs=32\n",
      "[PROG] 5024/21005 (23.9%) - bs=32\n",
      "[PROG] 5056/21005 (24.1%) - bs=32\n",
      "[PROG] 5088/21005 (24.2%) - bs=32\n",
      "[PROG] 5120/21005 (24.4%) - bs=32\n",
      "[PROG] 5152/21005 (24.5%) - bs=32\n",
      "[PROG] 5184/21005 (24.7%) - bs=32\n",
      "[PROG] 5216/21005 (24.8%) - bs=32\n",
      "[PROG] 5248/21005 (25.0%) - bs=32\n",
      "[PROG] 5280/21005 (25.1%) - bs=32\n",
      "[PROG] 5312/21005 (25.3%) - bs=32\n",
      "[PROG] 5344/21005 (25.4%) - bs=32\n",
      "[PROG] 5376/21005 (25.6%) - bs=32\n",
      "[PROG] 5408/21005 (25.7%) - bs=32\n",
      "[PROG] 5440/21005 (25.9%) - bs=32\n",
      "[PROG] 5472/21005 (26.1%) - bs=32\n",
      "[PROG] 5504/21005 (26.2%) - bs=32\n",
      "[PROG] 5536/21005 (26.4%) - bs=32\n",
      "[PROG] 5568/21005 (26.5%) - bs=32\n",
      "[PROG] 5600/21005 (26.7%) - bs=32\n",
      "[PROG] 5632/21005 (26.8%) - bs=32\n",
      "[PROG] 5664/21005 (27.0%) - bs=32\n",
      "[PROG] 5696/21005 (27.1%) - bs=32\n",
      "[PROG] 5728/21005 (27.3%) - bs=32\n",
      "[PROG] 5760/21005 (27.4%) - bs=32\n",
      "[PROG] 5792/21005 (27.6%) - bs=32\n",
      "[PROG] 5824/21005 (27.7%) - bs=32\n",
      "[PROG] 5856/21005 (27.9%) - bs=32\n",
      "[PROG] 5888/21005 (28.0%) - bs=32\n",
      "[PROG] 5920/21005 (28.2%) - bs=32\n",
      "[PROG] 5952/21005 (28.3%) - bs=32\n",
      "[PROG] 5984/21005 (28.5%) - bs=32\n",
      "[PROG] 6016/21005 (28.6%) - bs=32\n",
      "[PROG] 6048/21005 (28.8%) - bs=32\n",
      "[PROG] 6080/21005 (28.9%) - bs=32\n",
      "[PROG] 6112/21005 (29.1%) - bs=32\n",
      "[PROG] 6144/21005 (29.3%) - bs=32\n",
      "[PROG] 6176/21005 (29.4%) - bs=32\n",
      "[PROG] 6208/21005 (29.6%) - bs=32\n",
      "[PROG] 6240/21005 (29.7%) - bs=32\n",
      "[PROG] 6272/21005 (29.9%) - bs=32\n",
      "[PROG] 6304/21005 (30.0%) - bs=32\n",
      "[PROG] 6336/21005 (30.2%) - bs=32\n",
      "[PROG] 6368/21005 (30.3%) - bs=32\n",
      "[PROG] 6400/21005 (30.5%) - bs=32\n",
      "[PROG] 6432/21005 (30.6%) - bs=32\n",
      "[PROG] 6464/21005 (30.8%) - bs=32\n",
      "[PROG] 6496/21005 (30.9%) - bs=32\n",
      "[PROG] 6528/21005 (31.1%) - bs=32\n",
      "[PROG] 6560/21005 (31.2%) - bs=32\n",
      "[PROG] 6592/21005 (31.4%) - bs=32\n",
      "[PROG] 6624/21005 (31.5%) - bs=32\n",
      "[PROG] 6656/21005 (31.7%) - bs=32\n",
      "[PROG] 6688/21005 (31.8%) - bs=32\n",
      "[PROG] 6720/21005 (32.0%) - bs=32\n",
      "[PROG] 6752/21005 (32.1%) - bs=32\n",
      "[PROG] 6784/21005 (32.3%) - bs=32\n",
      "[PROG] 6816/21005 (32.4%) - bs=32\n",
      "[PROG] 6848/21005 (32.6%) - bs=32\n",
      "[PROG] 6880/21005 (32.8%) - bs=32\n",
      "[PROG] 6912/21005 (32.9%) - bs=32\n",
      "[PROG] 6944/21005 (33.1%) - bs=32\n",
      "[PROG] 6976/21005 (33.2%) - bs=32\n",
      "[PROG] 7008/21005 (33.4%) - bs=32\n",
      "[PROG] 7040/21005 (33.5%) - bs=32\n",
      "[PROG] 7072/21005 (33.7%) - bs=32\n",
      "[PROG] 7104/21005 (33.8%) - bs=32\n",
      "[PROG] 7136/21005 (34.0%) - bs=32\n",
      "[PROG] 7168/21005 (34.1%) - bs=32\n",
      "[PROG] 7200/21005 (34.3%) - bs=32\n",
      "[PROG] 7232/21005 (34.4%) - bs=32\n",
      "[PROG] 7264/21005 (34.6%) - bs=32\n",
      "[PROG] 7296/21005 (34.7%) - bs=32\n",
      "[PROG] 7328/21005 (34.9%) - bs=32\n",
      "[PROG] 7360/21005 (35.0%) - bs=32\n",
      "[PROG] 7392/21005 (35.2%) - bs=32\n",
      "[PROG] 7424/21005 (35.3%) - bs=32\n",
      "[PROG] 7456/21005 (35.5%) - bs=32\n",
      "[PROG] 7488/21005 (35.6%) - bs=32\n",
      "[PROG] 7520/21005 (35.8%) - bs=32\n",
      "[PROG] 7552/21005 (36.0%) - bs=32\n",
      "[PROG] 7584/21005 (36.1%) - bs=32\n",
      "[PROG] 7616/21005 (36.3%) - bs=32\n",
      "[PROG] 7648/21005 (36.4%) - bs=32\n",
      "[PROG] 7680/21005 (36.6%) - bs=32\n",
      "[PROG] 7712/21005 (36.7%) - bs=32\n",
      "[PROG] 7744/21005 (36.9%) - bs=32\n",
      "[PROG] 7776/21005 (37.0%) - bs=32\n",
      "[PROG] 7808/21005 (37.2%) - bs=32\n",
      "[PROG] 7840/21005 (37.3%) - bs=32\n",
      "[PROG] 7872/21005 (37.5%) - bs=32\n",
      "[PROG] 7904/21005 (37.6%) - bs=32\n",
      "[PROG] 7936/21005 (37.8%) - bs=32\n",
      "[PROG] 7968/21005 (37.9%) - bs=32\n",
      "[PROG] 8000/21005 (38.1%) - bs=32\n",
      "[PROG] 8032/21005 (38.2%) - bs=32\n",
      "[PROG] 8064/21005 (38.4%) - bs=32\n",
      "[PROG] 8096/21005 (38.5%) - bs=32\n",
      "[PROG] 8128/21005 (38.7%) - bs=32\n",
      "[PROG] 8160/21005 (38.8%) - bs=32\n",
      "[PROG] 8192/21005 (39.0%) - bs=32\n",
      "[PROG] 8224/21005 (39.2%) - bs=32\n",
      "[PROG] 8256/21005 (39.3%) - bs=32\n",
      "[PROG] 8288/21005 (39.5%) - bs=32\n",
      "[PROG] 8320/21005 (39.6%) - bs=32\n",
      "[PROG] 8352/21005 (39.8%) - bs=32\n",
      "[PROG] 8384/21005 (39.9%) - bs=32\n",
      "[PROG] 8416/21005 (40.1%) - bs=32\n",
      "[PROG] 8448/21005 (40.2%) - bs=32\n",
      "[PROG] 8480/21005 (40.4%) - bs=32\n",
      "[PROG] 8512/21005 (40.5%) - bs=32\n",
      "[PROG] 8544/21005 (40.7%) - bs=32\n",
      "[PROG] 8576/21005 (40.8%) - bs=32\n",
      "[PROG] 8608/21005 (41.0%) - bs=32\n",
      "[PROG] 8640/21005 (41.1%) - bs=32\n",
      "[PROG] 8672/21005 (41.3%) - bs=32\n",
      "[PROG] 8704/21005 (41.4%) - bs=32\n",
      "[PROG] 8736/21005 (41.6%) - bs=32\n",
      "[PROG] 8768/21005 (41.7%) - bs=32\n",
      "[PROG] 8800/21005 (41.9%) - bs=32\n",
      "[PROG] 8832/21005 (42.0%) - bs=32\n",
      "[PROG] 8864/21005 (42.2%) - bs=32\n",
      "[PROG] 8896/21005 (42.4%) - bs=32\n",
      "[PROG] 8928/21005 (42.5%) - bs=32\n",
      "[PROG] 8960/21005 (42.7%) - bs=32\n",
      "[PROG] 8992/21005 (42.8%) - bs=32\n",
      "[PROG] 9024/21005 (43.0%) - bs=32\n",
      "[PROG] 9056/21005 (43.1%) - bs=32\n",
      "[PROG] 9088/21005 (43.3%) - bs=32\n",
      "[PROG] 9120/21005 (43.4%) - bs=32\n",
      "[PROG] 9152/21005 (43.6%) - bs=32\n",
      "[PROG] 9184/21005 (43.7%) - bs=32\n",
      "[PROG] 9216/21005 (43.9%) - bs=32\n",
      "[PROG] 9248/21005 (44.0%) - bs=32\n",
      "[PROG] 9280/21005 (44.2%) - bs=32\n",
      "[PROG] 9312/21005 (44.3%) - bs=32\n",
      "[PROG] 9344/21005 (44.5%) - bs=32\n",
      "[PROG] 9376/21005 (44.6%) - bs=32\n",
      "[PROG] 9408/21005 (44.8%) - bs=32\n",
      "[PROG] 9440/21005 (44.9%) - bs=32\n",
      "[PROG] 9472/21005 (45.1%) - bs=32\n",
      "[PROG] 9504/21005 (45.2%) - bs=32\n",
      "[PROG] 9536/21005 (45.4%) - bs=32\n",
      "[PROG] 9568/21005 (45.6%) - bs=32\n",
      "[PROG] 9600/21005 (45.7%) - bs=32\n",
      "[PROG] 9632/21005 (45.9%) - bs=32\n",
      "[PROG] 9664/21005 (46.0%) - bs=32\n",
      "[PROG] 9696/21005 (46.2%) - bs=32\n",
      "[PROG] 9728/21005 (46.3%) - bs=32\n",
      "[PROG] 9760/21005 (46.5%) - bs=32\n",
      "[PROG] 9792/21005 (46.6%) - bs=32\n",
      "[PROG] 9824/21005 (46.8%) - bs=32\n",
      "[PROG] 9856/21005 (46.9%) - bs=32\n",
      "[PROG] 9888/21005 (47.1%) - bs=32\n",
      "[PROG] 9920/21005 (47.2%) - bs=32\n",
      "[PROG] 9952/21005 (47.4%) - bs=32\n",
      "[PROG] 9984/21005 (47.5%) - bs=32\n",
      "[PROG] 10016/21005 (47.7%) - bs=32\n",
      "[PROG] 10048/21005 (47.8%) - bs=32\n",
      "[PROG] 10080/21005 (48.0%) - bs=32\n",
      "[PROG] 10112/21005 (48.1%) - bs=32\n",
      "[PROG] 10144/21005 (48.3%) - bs=32\n",
      "[PROG] 10176/21005 (48.4%) - bs=32\n",
      "[PROG] 10208/21005 (48.6%) - bs=32\n",
      "[PROG] 10240/21005 (48.8%) - bs=32\n",
      "[PROG] 10272/21005 (48.9%) - bs=32\n",
      "[PROG] 10304/21005 (49.1%) - bs=32\n",
      "[PROG] 10336/21005 (49.2%) - bs=32\n",
      "[PROG] 10368/21005 (49.4%) - bs=32\n",
      "[PROG] 10400/21005 (49.5%) - bs=32\n",
      "[PROG] 10432/21005 (49.7%) - bs=32\n",
      "[PROG] 10464/21005 (49.8%) - bs=32\n",
      "[PROG] 10496/21005 (50.0%) - bs=32\n",
      "[PROG] 10528/21005 (50.1%) - bs=32\n",
      "[PROG] 10560/21005 (50.3%) - bs=32\n",
      "[PROG] 10592/21005 (50.4%) - bs=32\n",
      "[PROG] 10624/21005 (50.6%) - bs=32\n",
      "[PROG] 10656/21005 (50.7%) - bs=32\n",
      "[PROG] 10688/21005 (50.9%) - bs=32\n",
      "[PROG] 10720/21005 (51.0%) - bs=32\n",
      "[PROG] 10752/21005 (51.2%) - bs=32\n",
      "[PROG] 10784/21005 (51.3%) - bs=32\n",
      "[PROG] 10816/21005 (51.5%) - bs=32\n",
      "[PROG] 10848/21005 (51.6%) - bs=32\n",
      "[PROG] 10880/21005 (51.8%) - bs=32\n",
      "[PROG] 10912/21005 (51.9%) - bs=32\n",
      "[PROG] 10944/21005 (52.1%) - bs=32\n",
      "[PROG] 10976/21005 (52.3%) - bs=32\n",
      "[PROG] 11008/21005 (52.4%) - bs=32\n",
      "[PROG] 11040/21005 (52.6%) - bs=32\n",
      "[PROG] 11072/21005 (52.7%) - bs=32\n",
      "[PROG] 11104/21005 (52.9%) - bs=32\n",
      "[PROG] 11136/21005 (53.0%) - bs=32\n",
      "[PROG] 11168/21005 (53.2%) - bs=32\n",
      "[PROG] 11200/21005 (53.3%) - bs=32\n",
      "[PROG] 11232/21005 (53.5%) - bs=32\n",
      "[PROG] 11264/21005 (53.6%) - bs=32\n",
      "[PROG] 11296/21005 (53.8%) - bs=32\n",
      "[PROG] 11328/21005 (53.9%) - bs=32\n",
      "[PROG] 11360/21005 (54.1%) - bs=32\n",
      "[PROG] 11392/21005 (54.2%) - bs=32\n",
      "[PROG] 11424/21005 (54.4%) - bs=32\n",
      "[PROG] 11456/21005 (54.5%) - bs=32\n",
      "[PROG] 11488/21005 (54.7%) - bs=32\n",
      "[PROG] 11520/21005 (54.8%) - bs=32\n",
      "[PROG] 11552/21005 (55.0%) - bs=32\n",
      "[PROG] 11584/21005 (55.1%) - bs=32\n",
      "[PROG] 11616/21005 (55.3%) - bs=32\n",
      "[PROG] 11648/21005 (55.5%) - bs=32\n",
      "[PROG] 11680/21005 (55.6%) - bs=32\n",
      "[PROG] 11712/21005 (55.8%) - bs=32\n",
      "[PROG] 11744/21005 (55.9%) - bs=32\n",
      "[PROG] 11776/21005 (56.1%) - bs=32\n",
      "[PROG] 11808/21005 (56.2%) - bs=32\n",
      "[PROG] 11840/21005 (56.4%) - bs=32\n",
      "[PROG] 11872/21005 (56.5%) - bs=32\n",
      "[PROG] 11904/21005 (56.7%) - bs=32\n",
      "[PROG] 11936/21005 (56.8%) - bs=32\n",
      "[PROG] 11968/21005 (57.0%) - bs=32\n",
      "[PROG] 12000/21005 (57.1%) - bs=32\n",
      "[PROG] 12032/21005 (57.3%) - bs=32\n",
      "[PROG] 12064/21005 (57.4%) - bs=32\n",
      "[PROG] 12096/21005 (57.6%) - bs=32\n",
      "[PROG] 12128/21005 (57.7%) - bs=32\n",
      "[PROG] 12160/21005 (57.9%) - bs=32\n",
      "[PROG] 12192/21005 (58.0%) - bs=32\n",
      "[PROG] 12224/21005 (58.2%) - bs=32\n",
      "[PROG] 12256/21005 (58.3%) - bs=32\n",
      "[PROG] 12288/21005 (58.5%) - bs=32\n",
      "[PROG] 12320/21005 (58.7%) - bs=32\n",
      "[PROG] 12352/21005 (58.8%) - bs=32\n",
      "[PROG] 12384/21005 (59.0%) - bs=32\n",
      "[PROG] 12416/21005 (59.1%) - bs=32\n",
      "[PROG] 12448/21005 (59.3%) - bs=32\n",
      "[PROG] 12480/21005 (59.4%) - bs=32\n",
      "[PROG] 12512/21005 (59.6%) - bs=32\n",
      "[PROG] 12544/21005 (59.7%) - bs=32\n",
      "[PROG] 12576/21005 (59.9%) - bs=32\n",
      "[PROG] 12608/21005 (60.0%) - bs=32\n",
      "[PROG] 12640/21005 (60.2%) - bs=32\n",
      "[PROG] 12672/21005 (60.3%) - bs=32\n",
      "[PROG] 12704/21005 (60.5%) - bs=32\n",
      "[PROG] 12736/21005 (60.6%) - bs=32\n",
      "[PROG] 12768/21005 (60.8%) - bs=32\n",
      "[PROG] 12800/21005 (60.9%) - bs=32\n",
      "[PROG] 12832/21005 (61.1%) - bs=32\n",
      "[PROG] 12864/21005 (61.2%) - bs=32\n",
      "[PROG] 12896/21005 (61.4%) - bs=32\n",
      "[PROG] 12928/21005 (61.5%) - bs=32\n",
      "[PROG] 12960/21005 (61.7%) - bs=32\n",
      "[PROG] 12992/21005 (61.9%) - bs=32\n",
      "[PROG] 13024/21005 (62.0%) - bs=32\n",
      "[PROG] 13056/21005 (62.2%) - bs=32\n",
      "[PROG] 13088/21005 (62.3%) - bs=32\n",
      "[PROG] 13120/21005 (62.5%) - bs=32\n",
      "[PROG] 13152/21005 (62.6%) - bs=32\n",
      "[PROG] 13184/21005 (62.8%) - bs=32\n",
      "[PROG] 13216/21005 (62.9%) - bs=32\n",
      "[PROG] 13248/21005 (63.1%) - bs=32\n",
      "[PROG] 13280/21005 (63.2%) - bs=32\n",
      "[PROG] 13312/21005 (63.4%) - bs=32\n",
      "[PROG] 13344/21005 (63.5%) - bs=32\n",
      "[PROG] 13376/21005 (63.7%) - bs=32\n",
      "[PROG] 13408/21005 (63.8%) - bs=32\n",
      "[PROG] 13440/21005 (64.0%) - bs=32\n",
      "[PROG] 13472/21005 (64.1%) - bs=32\n",
      "[PROG] 13504/21005 (64.3%) - bs=32\n",
      "[PROG] 13536/21005 (64.4%) - bs=32\n",
      "[PROG] 13568/21005 (64.6%) - bs=32\n",
      "[PROG] 13600/21005 (64.7%) - bs=32\n",
      "[PROG] 13632/21005 (64.9%) - bs=32\n",
      "[PROG] 13664/21005 (65.1%) - bs=32\n",
      "[PROG] 13696/21005 (65.2%) - bs=32\n",
      "[PROG] 13728/21005 (65.4%) - bs=32\n",
      "[PROG] 13760/21005 (65.5%) - bs=32\n",
      "[PROG] 13792/21005 (65.7%) - bs=32\n",
      "[PROG] 13824/21005 (65.8%) - bs=32\n",
      "[PROG] 13856/21005 (66.0%) - bs=32\n",
      "[PROG] 13888/21005 (66.1%) - bs=32\n",
      "[PROG] 13920/21005 (66.3%) - bs=32\n",
      "[PROG] 13952/21005 (66.4%) - bs=32\n",
      "[PROG] 13984/21005 (66.6%) - bs=32\n",
      "[PROG] 14016/21005 (66.7%) - bs=32\n",
      "[PROG] 14048/21005 (66.9%) - bs=32\n",
      "[PROG] 14080/21005 (67.0%) - bs=32\n",
      "[PROG] 14112/21005 (67.2%) - bs=32\n",
      "[PROG] 14144/21005 (67.3%) - bs=32\n",
      "[PROG] 14176/21005 (67.5%) - bs=32\n",
      "[PROG] 14208/21005 (67.6%) - bs=32\n",
      "[PROG] 14240/21005 (67.8%) - bs=32\n",
      "[PROG] 14272/21005 (67.9%) - bs=32\n",
      "[PROG] 14304/21005 (68.1%) - bs=32\n",
      "[PROG] 14336/21005 (68.3%) - bs=32\n",
      "[PROG] 14368/21005 (68.4%) - bs=32\n",
      "[PROG] 14400/21005 (68.6%) - bs=32\n",
      "[PROG] 14432/21005 (68.7%) - bs=32\n",
      "[PROG] 14464/21005 (68.9%) - bs=32\n",
      "[PROG] 14496/21005 (69.0%) - bs=32\n",
      "[PROG] 14528/21005 (69.2%) - bs=32\n",
      "[PROG] 14560/21005 (69.3%) - bs=32\n",
      "[PROG] 14592/21005 (69.5%) - bs=32\n",
      "[PROG] 14624/21005 (69.6%) - bs=32\n",
      "[PROG] 14656/21005 (69.8%) - bs=32\n",
      "[PROG] 14688/21005 (69.9%) - bs=32\n",
      "[PROG] 14720/21005 (70.1%) - bs=32\n",
      "[PROG] 14752/21005 (70.2%) - bs=32\n",
      "[PROG] 14784/21005 (70.4%) - bs=32\n",
      "[PROG] 14816/21005 (70.5%) - bs=32\n",
      "[PROG] 14848/21005 (70.7%) - bs=32\n",
      "[PROG] 14880/21005 (70.8%) - bs=32\n",
      "[PROG] 14912/21005 (71.0%) - bs=32\n",
      "[PROG] 14944/21005 (71.1%) - bs=32\n",
      "[PROG] 14976/21005 (71.3%) - bs=32\n",
      "[PROG] 15008/21005 (71.4%) - bs=32\n",
      "[PROG] 15040/21005 (71.6%) - bs=32\n",
      "[PROG] 15072/21005 (71.8%) - bs=32\n",
      "[PROG] 15104/21005 (71.9%) - bs=32\n",
      "[PROG] 15136/21005 (72.1%) - bs=32\n",
      "[PROG] 15168/21005 (72.2%) - bs=32\n",
      "[PROG] 15200/21005 (72.4%) - bs=32\n",
      "[PROG] 15232/21005 (72.5%) - bs=32\n",
      "[PROG] 15264/21005 (72.7%) - bs=32\n",
      "[PROG] 15296/21005 (72.8%) - bs=32\n",
      "[PROG] 15328/21005 (73.0%) - bs=32\n",
      "[PROG] 15360/21005 (73.1%) - bs=32\n",
      "[PROG] 15392/21005 (73.3%) - bs=32\n",
      "[PROG] 15424/21005 (73.4%) - bs=32\n",
      "[PROG] 15456/21005 (73.6%) - bs=32\n",
      "[PROG] 15488/21005 (73.7%) - bs=32\n",
      "[PROG] 15520/21005 (73.9%) - bs=32\n",
      "[PROG] 15552/21005 (74.0%) - bs=32\n",
      "[PROG] 15584/21005 (74.2%) - bs=32\n",
      "[PROG] 15616/21005 (74.3%) - bs=32\n",
      "[PROG] 15648/21005 (74.5%) - bs=32\n",
      "[PROG] 15680/21005 (74.6%) - bs=32\n",
      "[PROG] 15712/21005 (74.8%) - bs=32\n",
      "[PROG] 15744/21005 (75.0%) - bs=32\n",
      "[PROG] 15776/21005 (75.1%) - bs=32\n",
      "[PROG] 15808/21005 (75.3%) - bs=32\n",
      "[PROG] 15840/21005 (75.4%) - bs=32\n",
      "[PROG] 15872/21005 (75.6%) - bs=32\n",
      "[PROG] 15904/21005 (75.7%) - bs=32\n",
      "[PROG] 15936/21005 (75.9%) - bs=32\n",
      "[PROG] 15968/21005 (76.0%) - bs=32\n",
      "[PROG] 16000/21005 (76.2%) - bs=32\n",
      "[PROG] 16032/21005 (76.3%) - bs=32\n",
      "[PROG] 16064/21005 (76.5%) - bs=32\n",
      "[PROG] 16096/21005 (76.6%) - bs=32\n",
      "[PROG] 16128/21005 (76.8%) - bs=32\n",
      "[PROG] 16160/21005 (76.9%) - bs=32\n",
      "[PROG] 16192/21005 (77.1%) - bs=32\n",
      "[PROG] 16224/21005 (77.2%) - bs=32\n",
      "[PROG] 16256/21005 (77.4%) - bs=32\n",
      "[PROG] 16288/21005 (77.5%) - bs=32\n",
      "[PROG] 16320/21005 (77.7%) - bs=32\n",
      "[PROG] 16352/21005 (77.8%) - bs=32\n",
      "[PROG] 16384/21005 (78.0%) - bs=32\n",
      "[PROG] 16416/21005 (78.2%) - bs=32\n",
      "[PROG] 16448/21005 (78.3%) - bs=32\n",
      "[PROG] 16480/21005 (78.5%) - bs=32\n",
      "[PROG] 16512/21005 (78.6%) - bs=32\n",
      "[PROG] 16544/21005 (78.8%) - bs=32\n",
      "[PROG] 16576/21005 (78.9%) - bs=32\n",
      "[PROG] 16608/21005 (79.1%) - bs=32\n",
      "[PROG] 16640/21005 (79.2%) - bs=32\n",
      "[PROG] 16672/21005 (79.4%) - bs=32\n",
      "[PROG] 16704/21005 (79.5%) - bs=32\n",
      "[PROG] 16736/21005 (79.7%) - bs=32\n",
      "[PROG] 16768/21005 (79.8%) - bs=32\n",
      "[PROG] 16800/21005 (80.0%) - bs=32\n",
      "[PROG] 16832/21005 (80.1%) - bs=32\n",
      "[PROG] 16864/21005 (80.3%) - bs=32\n",
      "[PROG] 16896/21005 (80.4%) - bs=32\n",
      "[PROG] 16928/21005 (80.6%) - bs=32\n",
      "[PROG] 16960/21005 (80.7%) - bs=32\n",
      "[PROG] 16992/21005 (80.9%) - bs=32\n",
      "[PROG] 17024/21005 (81.0%) - bs=32\n",
      "[PROG] 17056/21005 (81.2%) - bs=32\n",
      "[PROG] 17088/21005 (81.4%) - bs=32\n",
      "[PROG] 17120/21005 (81.5%) - bs=32\n",
      "[PROG] 17152/21005 (81.7%) - bs=32\n",
      "[PROG] 17184/21005 (81.8%) - bs=32\n",
      "[PROG] 17216/21005 (82.0%) - bs=32\n",
      "[PROG] 17248/21005 (82.1%) - bs=32\n",
      "[PROG] 17280/21005 (82.3%) - bs=32\n",
      "[PROG] 17312/21005 (82.4%) - bs=32\n",
      "[PROG] 17344/21005 (82.6%) - bs=32\n",
      "[PROG] 17376/21005 (82.7%) - bs=32\n",
      "[PROG] 17408/21005 (82.9%) - bs=32\n",
      "[PROG] 17440/21005 (83.0%) - bs=32\n",
      "[PROG] 17472/21005 (83.2%) - bs=32\n",
      "[PROG] 17504/21005 (83.3%) - bs=32\n",
      "[PROG] 17536/21005 (83.5%) - bs=32\n",
      "[PROG] 17568/21005 (83.6%) - bs=32\n",
      "[PROG] 17600/21005 (83.8%) - bs=32\n",
      "[PROG] 17632/21005 (83.9%) - bs=32\n",
      "[PROG] 17664/21005 (84.1%) - bs=32\n",
      "[PROG] 17696/21005 (84.2%) - bs=32\n",
      "[PROG] 17728/21005 (84.4%) - bs=32\n",
      "[PROG] 17760/21005 (84.6%) - bs=32\n",
      "[PROG] 17792/21005 (84.7%) - bs=32\n",
      "[PROG] 17824/21005 (84.9%) - bs=32\n",
      "[PROG] 17856/21005 (85.0%) - bs=32\n",
      "[PROG] 17888/21005 (85.2%) - bs=32\n",
      "[PROG] 17920/21005 (85.3%) - bs=32\n",
      "[PROG] 17952/21005 (85.5%) - bs=32\n",
      "[PROG] 17984/21005 (85.6%) - bs=32\n",
      "[PROG] 18016/21005 (85.8%) - bs=32\n",
      "[PROG] 18048/21005 (85.9%) - bs=32\n",
      "[PROG] 18080/21005 (86.1%) - bs=32\n",
      "[PROG] 18112/21005 (86.2%) - bs=32\n",
      "[PROG] 18144/21005 (86.4%) - bs=32\n",
      "[PROG] 18176/21005 (86.5%) - bs=32\n",
      "[PROG] 18208/21005 (86.7%) - bs=32\n",
      "[PROG] 18240/21005 (86.8%) - bs=32\n",
      "[PROG] 18272/21005 (87.0%) - bs=32\n",
      "[PROG] 18304/21005 (87.1%) - bs=32\n",
      "[PROG] 18336/21005 (87.3%) - bs=32\n",
      "[PROG] 18368/21005 (87.4%) - bs=32\n",
      "[PROG] 18400/21005 (87.6%) - bs=32\n",
      "[PROG] 18432/21005 (87.8%) - bs=32\n",
      "[PROG] 18464/21005 (87.9%) - bs=32\n",
      "[PROG] 18496/21005 (88.1%) - bs=32\n",
      "[PROG] 18528/21005 (88.2%) - bs=32\n",
      "[PROG] 18560/21005 (88.4%) - bs=32\n",
      "[PROG] 18592/21005 (88.5%) - bs=32\n",
      "[PROG] 18624/21005 (88.7%) - bs=32\n",
      "[PROG] 18656/21005 (88.8%) - bs=32\n",
      "[PROG] 18688/21005 (89.0%) - bs=32\n",
      "[PROG] 18720/21005 (89.1%) - bs=32\n",
      "[PROG] 18752/21005 (89.3%) - bs=32\n",
      "[PROG] 18784/21005 (89.4%) - bs=32\n",
      "[PROG] 18816/21005 (89.6%) - bs=32\n",
      "[PROG] 18848/21005 (89.7%) - bs=32\n",
      "[PROG] 18880/21005 (89.9%) - bs=32\n",
      "[PROG] 18912/21005 (90.0%) - bs=32\n",
      "[PROG] 18944/21005 (90.2%) - bs=32\n",
      "[PROG] 18976/21005 (90.3%) - bs=32\n",
      "[PROG] 19008/21005 (90.5%) - bs=32\n",
      "[PROG] 19040/21005 (90.6%) - bs=32\n",
      "[PROG] 19072/21005 (90.8%) - bs=32\n",
      "[PROG] 19104/21005 (90.9%) - bs=32\n",
      "[PROG] 19136/21005 (91.1%) - bs=32\n",
      "[PROG] 19168/21005 (91.3%) - bs=32\n",
      "[PROG] 19200/21005 (91.4%) - bs=32\n",
      "[PROG] 19232/21005 (91.6%) - bs=32\n",
      "[PROG] 19264/21005 (91.7%) - bs=32\n",
      "[PROG] 19296/21005 (91.9%) - bs=32\n",
      "[PROG] 19328/21005 (92.0%) - bs=32\n",
      "[PROG] 19360/21005 (92.2%) - bs=32\n",
      "[PROG] 19392/21005 (92.3%) - bs=32\n",
      "[PROG] 19424/21005 (92.5%) - bs=32\n",
      "[PROG] 19456/21005 (92.6%) - bs=32\n",
      "[PROG] 19488/21005 (92.8%) - bs=32\n",
      "[PROG] 19520/21005 (92.9%) - bs=32\n",
      "[PROG] 19552/21005 (93.1%) - bs=32\n",
      "[PROG] 19584/21005 (93.2%) - bs=32\n",
      "[PROG] 19616/21005 (93.4%) - bs=32\n",
      "[PROG] 19648/21005 (93.5%) - bs=32\n",
      "[PROG] 19680/21005 (93.7%) - bs=32\n",
      "[PROG] 19712/21005 (93.8%) - bs=32\n",
      "[PROG] 19744/21005 (94.0%) - bs=32\n",
      "[PROG] 19776/21005 (94.1%) - bs=32\n",
      "[PROG] 19808/21005 (94.3%) - bs=32\n",
      "[PROG] 19840/21005 (94.5%) - bs=32\n",
      "[PROG] 19872/21005 (94.6%) - bs=32\n",
      "[PROG] 19904/21005 (94.8%) - bs=32\n",
      "[PROG] 19936/21005 (94.9%) - bs=32\n",
      "[PROG] 19968/21005 (95.1%) - bs=32\n",
      "[PROG] 20000/21005 (95.2%) - bs=32\n",
      "[PROG] 20032/21005 (95.4%) - bs=32\n",
      "[PROG] 20064/21005 (95.5%) - bs=32\n",
      "[PROG] 20096/21005 (95.7%) - bs=32\n",
      "[PROG] 20128/21005 (95.8%) - bs=32\n",
      "[PROG] 20160/21005 (96.0%) - bs=32\n",
      "[PROG] 20192/21005 (96.1%) - bs=32\n",
      "[PROG] 20224/21005 (96.3%) - bs=32\n",
      "[PROG] 20256/21005 (96.4%) - bs=32\n",
      "[PROG] 20288/21005 (96.6%) - bs=32\n",
      "[PROG] 20320/21005 (96.7%) - bs=32\n",
      "[PROG] 20352/21005 (96.9%) - bs=32\n",
      "[PROG] 20384/21005 (97.0%) - bs=32\n",
      "[PROG] 20416/21005 (97.2%) - bs=32\n",
      "[PROG] 20448/21005 (97.3%) - bs=32\n",
      "[PROG] 20480/21005 (97.5%) - bs=32\n",
      "[PROG] 20512/21005 (97.7%) - bs=32\n",
      "[PROG] 20544/21005 (97.8%) - bs=32\n",
      "[PROG] 20576/21005 (98.0%) - bs=32\n",
      "[PROG] 20608/21005 (98.1%) - bs=32\n",
      "[PROG] 20640/21005 (98.3%) - bs=32\n",
      "[PROG] 20672/21005 (98.4%) - bs=32\n",
      "[PROG] 20704/21005 (98.6%) - bs=32\n",
      "[PROG] 20736/21005 (98.7%) - bs=32\n",
      "[PROG] 20768/21005 (98.9%) - bs=32\n",
      "[PROG] 20800/21005 (99.0%) - bs=32\n",
      "[PROG] 20832/21005 (99.2%) - bs=32\n",
      "[PROG] 20864/21005 (99.3%) - bs=32\n",
      "[PROG] 20896/21005 (99.5%) - bs=32\n",
      "[PROG] 20928/21005 (99.6%) - bs=32\n",
      "[PROG] 20960/21005 (99.8%) - bs=32\n",
      "[PROG] 20992/21005 (99.9%) - bs=32\n",
      "[PROG] 21005/21005 (100.0%) - bs=32\n",
      "[OK] FAISS guardado: faiss_index_ip.bin | ntotal=21005 | dim=384\n",
      "[OK] PKL (meta_min) guardado: embeddings_meta_min.pkl\n"
     ]
    }
   ],
   "source": [
    "import os, gc, pickle, numpy as np, pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import Optional\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "PARQUET_PATH   = os.environ.get(\"PARQUET_PATH\", \"corpus_chunks.parquet\")   # <-- solo contiene los chunks\n",
    "FAISS_PATH     = os.environ.get(\"FAISS_PATH\", \"faiss_index_ip.bin\")\n",
    "PKL_MIN_PATH   = os.environ.get(\"PKL_MIN_PATH\", \"embeddings_meta_min.pkl\")\n",
    "\n",
    "\n",
    "# Modelo recomendado para CPU\n",
    "EMB_MODEL        = os.environ.get(\"EMB_MODEL\", \"intfloat/multilingual-e5-small\")\n",
    "EMB_MAX_SEQ_LEN  = int(os.environ.get(\"EMB_MAX_SEQ_LEN\", \"300\"))   # <=512\n",
    "INIT_BATCH       = int(os.environ.get(\"BATCH_SIZE\", \"32\"))\n",
    "MIN_BATCH        = 1\n",
    "\n",
    "# -------------------- 0) Carga parquet de chunks + limpieza --------------------\n",
    "try:\n",
    "    chunks_df = pd.read_parquet(PARQUET_PATH, engine=\"pyarrow\")\n",
    "except Exception:\n",
    "    chunks_df = pd.read_parquet(PARQUET_PATH, engine=\"fastparquet\")\n",
    "\n",
    "required_cols = {\"doc_id\",\"chunk_id\",\"start_token\",\"end_token\",\"text_chunk\"}\n",
    "missing = required_cols - set(chunks_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas requeridas en {PARQUET_PATH}: {missing}\")\n",
    "\n",
    "# Normaliza tipos\n",
    "for c in [\"doc_id\",\"chunk_id\",\"start_token\",\"end_token\"]:\n",
    "    if chunks_df[c].dtype.kind not in \"iu\":\n",
    "        chunks_df[c] = pd.to_numeric(chunks_df[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "# Limpieza texto\n",
    "chunks_df[\"text_chunk\"] = chunks_df[\"text_chunk\"].astype(str).str.strip()\n",
    "chunks_df = chunks_df[chunks_df[\"text_chunk\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# chunk_uid y scopus_id opcional\n",
    "if \"chunk_uid\" not in chunks_df.columns:\n",
    "    chunks_df[\"chunk_uid\"] = chunks_df[\"doc_id\"].astype(str) + \"-\" + chunks_df[\"chunk_id\"].astype(str)\n",
    "if \"scopus_id\" in chunks_df.columns:\n",
    "    chunks_df[\"scopus_id\"] = chunks_df[\"scopus_id\"].astype(str)\n",
    "\n",
    "# IDs vectoriales alineados 0..N-1\n",
    "N = len(chunks_df)\n",
    "chunks_df[\"vec_id\"] = np.arange(N, dtype=\"int64\")\n",
    "chunks_df[\"embedding_model\"] = EMB_MODEL\n",
    "\n",
    "# -------------------- 1) Modelo (CPU) alineado con chunking --------------------\n",
    "print(f\"[INFO] device encode: cpu; model={EMB_MODEL}\")\n",
    "model = SentenceTransformer(EMB_MODEL, device=\"cpu\")\n",
    "model.max_seq_length = min(EMB_MAX_SEQ_LEN, 512)\n",
    "print(f\"[INFO] model.max_seq_length = {model.max_seq_length}\")\n",
    "\n",
    "# Prefijo E5\n",
    "passages = (\"passage: \" + chunks_df[\"text_chunk\"]).tolist()\n",
    "\n",
    "# -------------------- 2) FAISS (IP con embeddings normalizados -> coseno) --------------------\n",
    "def make_faiss_index(dim: int):\n",
    "    print(\"[INFO] FAISS-CPU\")\n",
    "    return faiss.IndexFlatIP(dim)\n",
    "\n",
    "def st_encode_cpu(texts, batch_size, normalize=True, to_numpy=True):\n",
    "    embs = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=False,\n",
    "        normalize_embeddings=normalize,\n",
    "        convert_to_numpy=to_numpy\n",
    "    )\n",
    "    return np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "# -------------------- 3) Bucle por lotes (CPU) --------------------\n",
    "def stream_encode_and_build(passages, init_bs=INIT_BATCH, min_bs=MIN_BATCH):\n",
    "    i, bs = 0, init_bs\n",
    "    index = None\n",
    "    dim = None\n",
    "\n",
    "    while i < N:\n",
    "        j = min(i + bs, N)\n",
    "        batch = passages[i:j]\n",
    "        try:\n",
    "            emb = st_encode_cpu(batch, batch_size=bs, normalize=True, to_numpy=True)\n",
    "\n",
    "            if dim is None:\n",
    "                dim = emb.shape[1]\n",
    "                index = make_faiss_index(dim)\n",
    "\n",
    "            index.add(emb)\n",
    "\n",
    "            i = j\n",
    "            print(f\"[PROG] {i}/{N} ({100.0*i/N:.1f}%) - bs={bs}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            prev_bs = bs\n",
    "            bs = max(min_bs, bs // 2)\n",
    "            gc.collect()\n",
    "            if prev_bs == bs and bs == min_bs:\n",
    "                raise RuntimeError(f\"Fallo persistente en CPU con batch={bs}: {e}\") from e\n",
    "            print(f\"[WARN] Error en i={i}. Bajo batch {prev_bs}->{bs} y reintento…\")\n",
    "            continue\n",
    "\n",
    "    return index, dim\n",
    "\n",
    "# -------------------- 4) Ejecutar pipeline --------------------\n",
    "index_cpu, dim = stream_encode_and_build(passages)\n",
    "\n",
    "# -------------------- 5) Guardar FAISS + PKL (mapa mínimo) --------------------\n",
    "faiss.write_index(index_cpu, FAISS_PATH)\n",
    "print(f\"[OK] FAISS guardado: {FAISS_PATH} | ntotal={index_cpu.ntotal} | dim={dim}\")\n",
    "\n",
    "# PKL: guardar meta_min con scopus_id si existe\n",
    "min_cols = [\"vec_id\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"start_token\",\"end_token\"]\n",
    "if \"scopus_id\" in chunks_df.columns:\n",
    "    min_cols.append(\"scopus_id\")\n",
    "\n",
    "with open(PKL_MIN_PATH, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"model\": EMB_MODEL,\n",
    "        \"device_used\": \"cpu\",\n",
    "        \"dim\": dim,\n",
    "        \"meta_min\": chunks_df[min_cols].copy()\n",
    "    }, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"[OK] PKL (meta_min) guardado: {PKL_MIN_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a9a44",
   "metadata": {},
   "source": [
    "## recuperacion"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T16:30:13.942323Z",
     "start_time": "2025-11-05T16:30:08.575261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os, pickle\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---- Rutas (ajústalas o usa variables de entorno) ----\n",
    "PKL_MIN_PATH = os.environ.get(\"PKL_MIN_PATH\", \"embeddings_meta_min.pkl\")\n",
    "FAISS_PATH   = os.environ.get(\"FAISS_PATH\", \"faiss_index_ip.bin\")\n",
    "SCOPUS_CSV   = os.environ.get(\"SCOPUS_CSV\", \"scopusdata.csv\")\n",
    "SCOPUS_SEP   = os.environ.get(\"SCOPUS_SEP\", \"|\")\n",
    "\n",
    "# ---- Caches simples ----\n",
    "_model_cache = None\n",
    "_meta_min_cache = None\n",
    "_index_cache = None\n",
    "_scopus_cache = None\n",
    "\n",
    "def load_pkl_and_model(emb_max_seq_len=300):\n",
    "    global _model_cache, _meta_min_cache\n",
    "    if _model_cache is not None and _meta_min_cache is not None:\n",
    "        return _model_cache, _meta_min_cache\n",
    "    with open(PKL_MIN_PATH, \"rb\") as f:\n",
    "        pkl = pickle.load(f)\n",
    "\n",
    "    meta_min = pkl[\"meta_min\"].copy()  # DataFrame: vec_id, chunk_uid, doc_id, chunk_id, (scopus_id), start/end\n",
    "    _meta_min_cache = meta_min\n",
    "\n",
    "    model_name = pkl.get(\"model\", \"intfloat/multilingual-e5-large\")\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.max_seq_length = min(int(emb_max_seq_len), 512)\n",
    "    _model_cache = model\n",
    "\n",
    "    print(f\"[INFO] Modelo: {model_name} | max_seq_length={model.max_seq_length}\")\n",
    "    print(f\"[INFO] meta_min columnas: {list(meta_min.columns)} | filas={len(meta_min)}\")\n",
    "    return _model_cache, _meta_min_cache\n",
    "\n",
    "def load_faiss():\n",
    "    global _index_cache\n",
    "    if _index_cache is None:\n",
    "        _index_cache = faiss.read_index(FAISS_PATH)\n",
    "        print(f\"[INFO] Índice FAISS cargado: ntotal={_index_cache.ntotal}\")\n",
    "    return _index_cache\n",
    "\n",
    "def load_scopus_csv():\n",
    "    global _scopus_cache\n",
    "    if _scopus_cache is None:\n",
    "        df = pd.read_csv(SCOPUS_CSV, sep=SCOPUS_SEP)\n",
    "        if \"scopus_id\" not in df.columns:\n",
    "            raise ValueError(f\"{SCOPUS_CSV} no tiene columna 'scopus_id'\")\n",
    "        df[\"scopus_id\"] = df[\"scopus_id\"].astype(str)\n",
    "        _scopus_cache = df\n",
    "        print(f\"[INFO] scoupusdata.csv: filas={len(df)} | cols={len(df.columns)}\")\n",
    "    return _scopus_cache\n",
    "\n",
    "def e5_encode_query(model, query_text: str):\n",
    "    return model.encode([f\"query: {query_text}\"],\n",
    "                        normalize_embeddings=True,\n",
    "                        convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "def search_min(query_text: str, topk: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve SOLO el meta mínimo del PKL (sin CSV):\n",
    "    vec_id, score, chunk_uid, doc_id, chunk_id, (scopus_id si existe), start/end\n",
    "    \"\"\"\n",
    "    model, meta_min = load_pkl_and_model()\n",
    "    index = load_faiss()\n",
    "\n",
    "    q = e5_encode_query(model, query_text)\n",
    "    D, I = index.search(q, topk)\n",
    "    vec_ids = I[0].tolist()\n",
    "\n",
    "    hits = meta_min.set_index(\"vec_id\").loc[vec_ids].reset_index()\n",
    "    hits.insert(1, \"score\", D[0])\n",
    "\n",
    "    cols_front = [c for c in [\"vec_id\",\"score\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"scopus_id\",\"start_token\",\"end_token\"] if c in hits.columns]\n",
    "    rest = [c for c in hits.columns if c not in cols_front]\n",
    "    return hits[cols_front + rest].reset_index(drop=True)\n",
    "\n",
    "def search_full_scopus(query_text: str, topk: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Une el TOP-K con TODAS las columnas de scoupusdata.csv por scopus_id.\n",
    "    \"\"\"\n",
    "    model, meta_min = load_pkl_and_model()\n",
    "    index = load_faiss()\n",
    "    sc = load_scopus_csv()\n",
    "\n",
    "    q = e5_encode_query(model, query_text)\n",
    "    D, I = index.search(q, topk)\n",
    "    vec_ids = I[0].tolist()\n",
    "\n",
    "    hits = meta_min.set_index(\"vec_id\").loc[vec_ids].reset_index()\n",
    "    hits.insert(1, \"score\", D[0])\n",
    "\n",
    "    if \"scopus_id\" not in hits.columns:\n",
    "        raise ValueError(\"meta_min en PKL no contiene 'scopus_id'; no puedo unir con el CSV.\")\n",
    "\n",
    "    out = hits.merge(sc, how=\"left\", on=\"scopus_id\")\n",
    "\n",
    "    # Orden: primero claves/score/offsets, luego TODO el CSV\n",
    "    front = [c for c in [\"vec_id\",\"score\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"scopus_id\",\"start_token\",\"end_token\"] if c in out.columns]\n",
    "    csv_cols = [c for c in sc.columns if c not in front]\n",
    "    return out[front + csv_cols].reset_index(drop=True)\n",
    "\n",
    "# ====== DEMO RÁPIDA ======\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"¿Qué variables clínicas aparecen como más influyentes en la predicción de mortalidad por COVID-19 en Ecuador?\"\n",
    "    print(\"\\n=== TOP-K + TODA la metadata del CSV ===\")\n",
    "    df = search_full_scopus(query, topk=15)\n",
    "    # Si quieres guardar para revisar en Excel:\n",
    "    df.to_csv(\"ground.csv\", sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "    print(df.head(10))\n"
   ],
   "id": "bd1a55c4bcfd5416",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP-K + TODA la metadata del CSV ===\n",
      "[INFO] Modelo: intfloat/multilingual-e5-small | max_seq_length=300\n",
      "[INFO] meta_min columnas: ['vec_id', 'chunk_uid', 'doc_id', 'chunk_id', 'start_token', 'end_token', 'scopus_id'] | filas=21005\n",
      "[INFO] Índice FAISS cargado: ntotal=21005\n",
      "[INFO] scoupusdata.csv: filas=19233 | cols=9\n",
      "   vec_id     score chunk_uid  doc_id  chunk_id    scopus_id  start_token  \\\n",
      "0   14468  0.872843   13207-0   13207         0  85175981768            0   \n",
      "1    9230  0.871996    8419-0    8419         0  85147783425            0   \n",
      "2    2449  0.870473    2186-0    2186         0  85119425145            0   \n",
      "3   17014  0.870369   15545-0   15545         0  85187569644            0   \n",
      "4   17622  0.868678   16100-1   16100         1  85189720260          240   \n",
      "5    4111  0.867389    3693-0    3693         0  85127559439            0   \n",
      "6   10938  0.865886    9989-0    9989         0  85158148579            0   \n",
      "7   15051  0.862978   13739-0   13739         0  85178875327            0   \n",
      "8     748  0.862967     662-0     662         0  85104914659            0   \n",
      "9   17251  0.861720   15760-0   15760         0  85188518934            0   \n",
      "\n",
      "   end_token                                              title  \\\n",
      "0        125  Recurrent Neural Networks (RNN) to Predict the...   \n",
      "1        230  A competing risk survival analysis of the soci...   \n",
      "2        150  Covid-19 in Ecuador: Radiography of Hospital D...   \n",
      "3        184  Psychometric properties of the Escala de Grave...   \n",
      "4        322  Maternal mortality and COVID-19: A nationwide ...   \n",
      "5        200  Do COVID-19 Worries, Resilience and Emotional ...   \n",
      "6        286  The deadly impact of COVID-19 among children f...   \n",
      "7        200  Multivariate Forecasting Model for COVID-19 Sp...   \n",
      "8        168  A critical narrative of Ecuador's preparedness...   \n",
      "9        120  XSTATIS method for the study of the main cause...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  In Ecuador, COVID-19 disease became a mortal e...   \n",
      "1  This study aimed to analyze the effect of soci...   \n",
      "2  Exploring the behavior and impact that the Cov...   \n",
      "3  Objective: This study aims to assess the valid...   \n",
      "4  Background: During the COVID-19 pandemic, dist...   \n",
      "5  COVID-19 and the measures adopted have been a ...   \n",
      "6  Background: The SARS-CoV-2 pandemic remains a ...   \n",
      "7  So far, about 770.1 million confirmed cases of...   \n",
      "8  Ecuador's National Health System has been seve...   \n",
      "9  Since the beginning of 2020, countless scienti...   \n",
      "\n",
      "                              doi  \\\n",
      "0     10.1007/978-3-031-45682-4_5   \n",
      "1       10.1590/0102-311XEN294721   \n",
      "2  10.1109/ETCM53643.2021.9590641   \n",
      "3      10.1016/j.ejtd.2024.100400   \n",
      "4       10.1177/17455057231219607   \n",
      "5         10.3390/children9030439   \n",
      "6       10.3389/fped.2023.1060311   \n",
      "7            10.3390/math11234721   \n",
      "8     10.1016/j.puhip.2021.100127   \n",
      "9   10.22185/24487147.2023.116.13   \n",
      "\n",
      "                                             authors  \\\n",
      "0  Charles M. Pérez-Espinoza; Genesis Rodriguez C...   \n",
      "1  Jorge Andrés Talledo-Delgado; German Josuet La...   \n",
      "2  Hover Torres; Guillermo Rodriguez; Marlon Faja...   \n",
      "3  Alberto Rodríguez-Lorenzana; Giovanni Sebastiá...   \n",
      "4  Sarah J. Carrington; Diana Checa-Jaramillo; Ed...   \n",
      "5  Selene Valero-Moreno; Inmaculada Montoya-Casti...   \n",
      "6  Raúl Fernández-Naranjo; Doménica Revelo-Bastid...   \n",
      "7  Paúl Arias-Muñoz; Juan C. Guamán; Gabriel Jáco...   \n",
      "8                     Angel Guevara; Juan José Alava   \n",
      "9  Sandra García-Bustos; Omar H. Ruiz-Barzola; Mi...   \n",
      "\n",
      "                                        affiliations  \\\n",
      "0                    Universidad Agraria del Ecuador   \n",
      "1                      Universidad Técnica de Manabí   \n",
      "2                     Universidad Católica de Cuenca   \n",
      "3  Universidad de las Americas - Ecuador; Univers...   \n",
      "4  Hospital Axxis; Hospital Luis Gabriel Dávila; ...   \n",
      "5     Universidad del Azuay; Universitat de València   \n",
      "6  Universidad de las Americas - Ecuador; Hospita...   \n",
      "7                      Universidad Técnica del Norte   \n",
      "8  Simon Fraser University; Universidad Central d...   \n",
      "9   Escuela Superior Politecnica del Litoral Ecuador   \n",
      "\n",
      "          affiliation_cities affiliation_countries  citation_count  \n",
      "0                  Guayaquil               Ecuador               0  \n",
      "1                 Portoviejo               Ecuador               0  \n",
      "2                     Cuenca               Ecuador               0  \n",
      "3    Quito; Pamplona; Ambato        Ecuador; Spain               0  \n",
      "4              Quito; Tulcan               Ecuador               0  \n",
      "5           Cuenca; Valencia        Ecuador; Spain               0  \n",
      "6            Quito; San Jose   Ecuador; Costa Rica               0  \n",
      "7                     Ibarra               Ecuador               0  \n",
      "8  Burnaby; Quito; Vancouver       Canada; Ecuador               0  \n",
      "9                  Guayaquil               Ecuador               0  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# reranking cross encoder",
   "id": "dc8c6633bf3f08bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T16:30:33.850467Z",
     "start_time": "2025-11-05T16:30:32.414993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Sequence, Optional, Tuple\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# ---------- Configuración ----------\n",
    "CROSS_ENCODER_MODEL = os.environ.get(\"CROSS_ENCODER_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "CE_BATCH_SIZE       = int(os.environ.get(\"CE_BATCH_SIZE\", \"64\"))\n",
    "W_CE, W_DENSE       = float(os.environ.get(\"W_CE\", \"0.7\")), float(os.environ.get(\"W_DENSE\", \"0.3\"))\n",
    "\n",
    "# columnas de texto (orden de prioridad)\n",
    "TEXT_COLS = [\"title\", \"abstract\"]  # <- ajustado a tu CSV\n",
    "\n",
    "# ---------- Utilidades ----------\n",
    "_ce_cache: Optional[CrossEncoder] = None\n",
    "\n",
    "def get_cross_encoder(model_name: str = CROSS_ENCODER_MODEL) -> CrossEncoder:\n",
    "    global _ce_cache\n",
    "    if _ce_cache is None:\n",
    "        _ce_cache = CrossEncoder(model_name, device=\"cpu\")\n",
    "        print(f\"[INFO] Cross-Encoder cargado: {model_name}\")\n",
    "    return _ce_cache\n",
    "\n",
    "def _first_nonempty(row: pd.Series, cols: Sequence[str]) -> str:\n",
    "    \"\"\"Devuelve el primer texto no vacío según prioridad en 'cols'.\n",
    "       Si no encuentra, intenta concatenar campos semánticos.\"\"\"\n",
    "    for c in cols:\n",
    "        if c in row and isinstance(row[c], str) and row[c].strip():\n",
    "            return row[c]\n",
    "    parts = []\n",
    "    for c in row.index:\n",
    "        name = c.lower()\n",
    "        if any(tok in name for tok in (\"title\",\"abstract\",\"summary\",\"keywords\",\"chunk\",\"desc\")):\n",
    "            v = row[c]\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                parts.append(v.strip())\n",
    "    return \" \".join(parts)[:4096]  # recorte defensivo\n",
    "\n",
    "def _build_pairs(query_text: str, df_topk: pd.DataFrame, text_cols: Optional[List[str]]) -> Tuple[List[Tuple[str,str]], List[int]]:\n",
    "    cols = text_cols or TEXT_COLS\n",
    "    pairs, idx_map = [], []\n",
    "    for i, row in df_topk.iterrows():\n",
    "        txt = _first_nonempty(row, cols)\n",
    "        pairs.append((query_text, txt if isinstance(txt, str) else \"\"))\n",
    "        idx_map.append(i)\n",
    "    return pairs, idx_map\n",
    "\n",
    "def _minmax(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    mn, mx = float(np.nanmin(x)), float(np.nanmax(x))\n",
    "    if not np.isfinite(mn) or not np.isfinite(mx) or (mx - mn) <= 1e-12:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - mn) / (mx - mn + 1e-12)\n",
    "\n",
    "def rerank_with_cross_encoder(query_text: str,\n",
    "                              df_topk: pd.DataFrame,\n",
    "                              text_cols: Optional[List[str]] = None,\n",
    "                              score_dense_col: str = \"score\",\n",
    "                              fuse_with_dense: bool = True,\n",
    "                              batch_size: int = CE_BATCH_SIZE,\n",
    "                              model_name: str = CROSS_ENCODER_MODEL) -> pd.DataFrame:\n",
    "    \"\"\"Reordena df_topk usando un Cross-Encoder y devuelve un nuevo DataFrame ordenado.\"\"\"\n",
    "    if df_topk is None or len(df_topk) == 0:\n",
    "        raise ValueError(\"df_topk está vacío; ejecuta primero search_full_scopus(query, topk=N).\")\n",
    "    ce = get_cross_encoder(model_name)\n",
    "    pairs, idx_map = _build_pairs(query_text, df_topk, text_cols)\n",
    "\n",
    "    # Predicción por lotes\n",
    "    scores_ce = []\n",
    "    for start in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[start:start+batch_size]\n",
    "        s = ce.predict(batch)  # lista/ndarray de floats\n",
    "        scores_ce.append(np.asarray(s, dtype=np.float32))\n",
    "    scores_ce = np.concatenate(scores_ce, axis=0) if scores_ce else np.zeros(len(df_topk), dtype=np.float32)\n",
    "\n",
    "    # Ensamble y orden\n",
    "    out = df_topk.copy()\n",
    "    out.loc[idx_map, \"score_ce\"] = scores_ce\n",
    "    ce_norm = _minmax(out[\"score_ce\"].values)\n",
    "\n",
    "    if fuse_with_dense and score_dense_col in out.columns:\n",
    "        dense_norm = _minmax(out[score_dense_col].values)\n",
    "        out[\"score_dense_norm\"] = dense_norm\n",
    "        out[\"score_final\"] = W_CE * ce_norm + W_DENSE * dense_norm\n",
    "        order_col = \"score_final\"\n",
    "    else:\n",
    "        out[\"score_final\"] = ce_norm\n",
    "        order_col = \"score_final\"\n",
    "\n",
    "    out = out.sort_values(order_col, ascending=False).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ---------- Ejemplo de uso: ejecuta después de definir tu 'query' ----------\n",
    "# 1) Recupera candidatos con tu función (puedes subir TOPK para un mejor re-ranking).\n",
    "query = os.environ.get(\"QUERY\", \"turismo en galapagos\")\n",
    "TOPK  = int(os.environ.get(\"TOPK\", \"100\"))\n",
    "\n",
    "try:\n",
    "    df_topk = search_full_scopus(query, topk=TOPK)   # usa tu función definida arriba\n",
    "except NameError:\n",
    "    raise RuntimeError(\"No encuentro search_full_scopus(). Ejecuta primero la celda donde la defines.\")\n",
    "\n",
    "# 2) Reranking con Cross-Encoder (ajusta columnas si conoces tus nombres exactos).\n",
    "#    Ejemplo: text_cols=[\"chunk_text\"] o [\"title\",\"abstract\",\"authkeywords\"]\n",
    "reranked = rerank_with_cross_encoder(\n",
    "    query_text=query,\n",
    "    df_topk=df_topk,\n",
    "    text_cols=None,            # autodetección robusta; cambia a lista explícita si la tienes\n",
    "    score_dense_col=\"score\",   # 'score' = puntaje FAISS de tu búsqueda\n",
    "    fuse_with_dense=True\n",
    ")\n",
    "\n",
    "# 3) Visualiza y/o guarda\n",
    "display_cols = [c for c in [\"vec_id\",\"score\",\"score_ce\",\"score_final\",\"scopus_id\",\"title\",\"abstract\",\"chunk_text\"] if c in reranked.columns]\n",
    "print(\"[OK] Top 10 (ya re-rankeado):\")\n",
    "display(reranked.head(10)[display_cols])"
   ],
   "id": "d9038908",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cross-Encoder cargado: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[OK] Top 10 (ya re-rankeado):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   vec_id     score  score_ce  score_final    scopus_id  \\\n",
       "0    5168  0.864254 -3.670709     0.823718  85132240597   \n",
       "1   14203  0.841950 -2.713060     0.744398  85174965454   \n",
       "2   10216  0.848282 -3.679054     0.720161  85151140601   \n",
       "3     775  0.820772 -1.053562     0.719617  85105310828   \n",
       "4    9580  0.860996 -4.914887     0.718905  85148845556   \n",
       "5   18344  0.844240 -3.452024     0.709393  85191855593   \n",
       "6    5447  0.843322 -3.669117     0.688851  85133488337   \n",
       "7   14590  0.855215 -5.482558     0.643393  85176328476   \n",
       "8   15962  0.843961 -4.802355     0.616639  85182709765   \n",
       "9   11028  0.826736 -3.237062     0.611005  85159179257   \n",
       "\n",
       "                                               title  \\\n",
       "0  “Rethink and reset” tourism in the Galapagos I...   \n",
       "1  Agrobiodiversity in four Islands of the Galapa...   \n",
       "2  Residents, conservation, development and touri...   \n",
       "3  The Galápagos as penal colony: Exile, peonage,...   \n",
       "4              Imperiled Ecosystems: Galápagos Scrub   \n",
       "5  SUSTAINABLE MANAGEMENT APPLIED TO THE HOTEL SE...   \n",
       "6  Volcanic event management in the Galápagos Isl...   \n",
       "7  A Bunch of Books, a Suitcase, and Many Trips b...   \n",
       "8  The impact of the COVID-19 pandemic on the Gal...   \n",
       "9  Tourist planning of an emblematic destination....   \n",
       "\n",
       "                                            abstract  \n",
       "0  Tourism growth in biodiversity conservation ar...  \n",
       "1  The Galapagos Islands, officially annexed to t...  \n",
       "2  Tourism from its initial stage generates socio...  \n",
       "3  Transportation to remote islands has been a wa...  \n",
       "4  The Galápagos Islands are an extraordinary loc...  \n",
       "5  This research analyzes the current situation o...  \n",
       "6  The volcanoes of Galápagos, Ecuador, are among...  \n",
       "7  The Galapagos Islands (Galapagos Province, Ecu...  \n",
       "8  The COVID-19 pandemic's early stages severely ...  \n",
       "9  The article through the analysis of the constr...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec_id</th>\n",
       "      <th>score</th>\n",
       "      <th>score_ce</th>\n",
       "      <th>score_final</th>\n",
       "      <th>scopus_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5168</td>\n",
       "      <td>0.864254</td>\n",
       "      <td>-3.670709</td>\n",
       "      <td>0.823718</td>\n",
       "      <td>85132240597</td>\n",
       "      <td>“Rethink and reset” tourism in the Galapagos I...</td>\n",
       "      <td>Tourism growth in biodiversity conservation ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14203</td>\n",
       "      <td>0.841950</td>\n",
       "      <td>-2.713060</td>\n",
       "      <td>0.744398</td>\n",
       "      <td>85174965454</td>\n",
       "      <td>Agrobiodiversity in four Islands of the Galapa...</td>\n",
       "      <td>The Galapagos Islands, officially annexed to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10216</td>\n",
       "      <td>0.848282</td>\n",
       "      <td>-3.679054</td>\n",
       "      <td>0.720161</td>\n",
       "      <td>85151140601</td>\n",
       "      <td>Residents, conservation, development and touri...</td>\n",
       "      <td>Tourism from its initial stage generates socio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>775</td>\n",
       "      <td>0.820772</td>\n",
       "      <td>-1.053562</td>\n",
       "      <td>0.719617</td>\n",
       "      <td>85105310828</td>\n",
       "      <td>The Galápagos as penal colony: Exile, peonage,...</td>\n",
       "      <td>Transportation to remote islands has been a wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9580</td>\n",
       "      <td>0.860996</td>\n",
       "      <td>-4.914887</td>\n",
       "      <td>0.718905</td>\n",
       "      <td>85148845556</td>\n",
       "      <td>Imperiled Ecosystems: Galápagos Scrub</td>\n",
       "      <td>The Galápagos Islands are an extraordinary loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18344</td>\n",
       "      <td>0.844240</td>\n",
       "      <td>-3.452024</td>\n",
       "      <td>0.709393</td>\n",
       "      <td>85191855593</td>\n",
       "      <td>SUSTAINABLE MANAGEMENT APPLIED TO THE HOTEL SE...</td>\n",
       "      <td>This research analyzes the current situation o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5447</td>\n",
       "      <td>0.843322</td>\n",
       "      <td>-3.669117</td>\n",
       "      <td>0.688851</td>\n",
       "      <td>85133488337</td>\n",
       "      <td>Volcanic event management in the Galápagos Isl...</td>\n",
       "      <td>The volcanoes of Galápagos, Ecuador, are among...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14590</td>\n",
       "      <td>0.855215</td>\n",
       "      <td>-5.482558</td>\n",
       "      <td>0.643393</td>\n",
       "      <td>85176328476</td>\n",
       "      <td>A Bunch of Books, a Suitcase, and Many Trips b...</td>\n",
       "      <td>The Galapagos Islands (Galapagos Province, Ecu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15962</td>\n",
       "      <td>0.843961</td>\n",
       "      <td>-4.802355</td>\n",
       "      <td>0.616639</td>\n",
       "      <td>85182709765</td>\n",
       "      <td>The impact of the COVID-19 pandemic on the Gal...</td>\n",
       "      <td>The COVID-19 pandemic's early stages severely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11028</td>\n",
       "      <td>0.826736</td>\n",
       "      <td>-3.237062</td>\n",
       "      <td>0.611005</td>\n",
       "      <td>85159179257</td>\n",
       "      <td>Tourist planning of an emblematic destination....</td>\n",
       "      <td>The article through the analysis of the constr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "2c8d1319",
   "metadata": {},
   "source": [
    "# Contruccion y Generación (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a18a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt (vista previa) ===\n",
      "Eres un asistente académico. Responde de forma clara, concisa y basada en evidencia, en español y estilo APA (7ª). Utiliza exclusivamente los fragmentos proporcionados. Cada párrafo debe incluir una mención autoral explícita (‘según <autor>’ o ‘de acuerdo con <autor>’) seguida de la cita en corchetes [n]. Si no hay autor disponible, usa ‘según la fuente [n]’. No imprimas tu propia sección de 'Fuentes'; solo escribe el cuerpo con citas [n]. Pregunta: el turismo en las islas galagos que tan bueno es? Fragmentos (con autor/año si disponible): [1] “Rethink and reset” tourism in the Galapagos Islands: Stakeholders' views on the sustainability of tourism development — Autor(es): Ferri et al. Tourism growth in biodiversity conservation areas presents both challenges and opportunities for sustainability. The COVID-19 pandemic brought both into focus in the Galapagos. This study engages with tourism service providers and regulators in Puerto Ayora, Santa Cruz Island, to explore how ...\n",
      "\n",
      "=== Respuesta (LLM - Ollama HTTP) ===\n",
      "\n",
      "El turismo en las Islas Galápagos presenta tanto desafíos como oportunidades para la sostenibilidad, según Ferri et al. [1].  La implementación del Modelo de Ecoturismo a una década de distancia ha generado debates sobre cómo se integra la sostenibilidad en las relaciones entre el turismo y la conservación, y los resultados sugieren que para lograr la sostenibilidad, se deben priorizar el compromiso de los actores locales en una visión compartida para el desarrollo turístico, abordar las preocupaciones de la comunidad, especialmente en lo referente a los servicios básicos, la salud y la educación, y evaluar y gestionar el equilibrio entre el turismo de alta gama regulado y el turismo informal de bajo costo.\n",
      "\n",
      "Según Allauca et al. [2], las Islas Galápagos, reconocidas como Patrimonio de la Humanidad, albergan una importante diversidad agrícola, aunque esta no ha sido completamente descrita. La presencia de especies agrícolas, medicinales y nativas en los cuatro islas habitadas (Isabela, Santa Cruz, Floreana y San Cristobal) es significativa, y se ven amenazadas por factores como el turismo, el cambio climático y las especies invasoras.\n",
      "\n",
      "Las percepciones de los residentes con respecto a la actividad turística en Puerto Ayora, Galápagos, son complejas, según Mosquera y Barriga [3].  Estos autores señalan que los impactos socioambientales del turismo, desde su inicio, generan cambios en las actitudes de los residentes, influenciados por factores ambientales, económicos, socioculturales y políticos, y que la planificación y el desarrollo turístico deben basarse en los tres pilares de la sostenibilidad.\n",
      "\n",
      "Jamieson y Astudillo [4] destacan que el uso de las islas como colonia penal, ejemplificado por la hacienda El Progreso entre 1878 y 1904, ilustra cómo las autoridades han abordado la criminalidad desde antes del surgimiento del estado moderno.  Esta operación demuestra que el concepto patriarcal de la hacienda continuó desempeñando un papel clave en la disciplina de la criminalidad percibida en América Latina a finales del siglo XIX, fuera de los roles de las fuerzas militares y policiales.\n",
      "\n",
      "La situación de las Islas Galápagos se considera crítica/en peligro por organizaciones como WWF y Conservation International [5], debido a la amenaza de especies invasoras, el cambio climático, el desarrollo económico y el turismo.  Estas amenazas, junto con la necesidad de controlar el turismo, impulsan iniciativas de biocontrol y otras medidas para proteger los ecosistemas terrestres, incluyendo manglares, matorral deciduous y bosques de montaña.\n",
      "\n",
      "Finalmente, Molina y Barriga [6] argumentan que, a pesar de los esfuerzos para promover el turismo sostenible, es difícil clasificar a las Islas Galápagos como un destino completamente sostenible, especialmente en términos de turismo basado en la comunidad.  La evaluación de las medidas adoptadas por los establecimientos hoteleros, incluyendo los instrumentos de gestión sostenible, revela que algunos se limitan a una estrategia de \"greenwashing\" para aparentar ser sostenibles, lo que subraya la necesidad de una planificación turística basada en los tres pilares de la sostenibilidad.\n",
      "\n",
      "=== Auditoría de citas usadas ===\n",
      "Citas usadas en el texto:\n",
      " - [1] Diego Quiroga Ferri; Diana V. Burbano; Thomas C. Meredith; Juan Carlos Valdivieso; Juan Carlos Izurieta; \"“Rethink and reset” tourism in the Galapagos Islands: Stakeholders' views on the sustainability of tourism development\"; https://doi.org/10.1016/j.annale.2022.100057\n",
      " - [2] Joanna Allauca; Luis O. Escudero; Angélica C. Zapata; Víctor Hugo Barrera; Álvaro R. Monteros-Altamirano; Marilú Valverde; \"Agrobiodiversity in four Islands of the Galapagos Archipelago, Ecuador\"; https://doi.org/10.1007/s10722-023-01759-6\n",
      " - [3] Paula Moya Mosquera; Andrea Muñoz Barriga; \"Residents, conservation, development and tourism in Galapagos\"; https://doi.org/10.4067/S0718-34022022000300201\n",
      " - [4] Ross Jamieson; Fernando Astudillo; \"The Galápagos as penal colony: Exile, peonage, and state control at the Hacienda El Progreso, 1878–1904\"; https://doi.org/10.1177/14624745211013100\n",
      " - [5] Gonzalo Francisco Rivas-Torres; Nejem Raheem; \"Imperiled Ecosystems: Galápagos Scrub\"; https://doi.org/10.1016/B978-0-12-821139-7.00196-3\n",
      " - [6] Samanta Victoria Torres Molina; Andrea Muñoz Barriga; \"SUSTAINABLE MANAGEMENT APPLIED TO THE HOTEL SECTOR IN GALÁPAGOS\"; https://doi.org/10.18601/01207555.n31.10\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, textwrap, requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# =================== Configuración ===================\n",
    "OLLAMA_HOST       = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL      = os.environ.get(\"OLLAMA_MODEL\", \"gemma3:4b\")\n",
    "TOP_CONTEXT       = int(os.environ.get(\"RAG_TOP_CONTEXT\", \"6\"))\n",
    "MAX_INPUT_CHARS   = int(os.environ.get(\"RAG_MAX_INPUT_CHARS\", \"7000\"))\n",
    "MAX_CHUNK_CHARS   = int(os.environ.get(\"RAG_MAX_CHUNK_CHARS\", \"900\"))\n",
    "TEMPERATURE       = float(os.environ.get(\"RAG_TEMPERATURE\", \"0.2\"))\n",
    "MAX_NEW_TOKENS    = int(os.environ.get(\"RAG_MAX_NEW_TOKENS\", \"768\"))  # ↑ un poco por seguridad\n",
    "HTTP_TIMEOUT_SECS = int(os.environ.get(\"RAG_HTTP_TIMEOUT_SECS\", \"300\"))\n",
    "DO_TRIM_ABSTRACT  = os.environ.get(\"RAG_TRIM_ABSTRACT\", \"1\") == \"1\"\n",
    "\n",
    "# =================== Utilidades generales ===================\n",
    "def _safe_str(x) -> str:\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, float) and x != x:  # NaN\n",
    "        return \"\"\n",
    "    try:\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _id_for_row(row: pd.Series) -> str:\n",
    "    for k in (\"doi\", \"scopus_id\", \"vec_id\"):\n",
    "        vs = _safe_str(row.get(k)).strip()\n",
    "        if vs:\n",
    "            return vs\n",
    "    return f\"row{row.name}\"\n",
    "\n",
    "def _first_nonempty(row: pd.Series, cols: List[str]) -> str:\n",
    "    for c in cols:\n",
    "        v = _safe_str(row.get(c, \"\"))\n",
    "        if v.strip():\n",
    "            return v.strip()\n",
    "    return \"\"\n",
    "\n",
    "def _shorten(txt, lim: int) -> str:\n",
    "    s = _safe_str(txt)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return (s[:lim-3] + \"...\") if len(s) > lim else s\n",
    "\n",
    "# ---------- Autores/Año para mención autoral ----------\n",
    "def _split_authors(raw: str) -> List[str]:\n",
    "    s = _safe_str(raw)\n",
    "    if not s.strip(): return []\n",
    "    s = s.replace(\"|\", \";\").replace(\" and \", \";\")\n",
    "    parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "    if len(parts) <= 1:\n",
    "        parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def _last_name(name: str) -> str:\n",
    "    n = _safe_str(name).strip()\n",
    "    if not n: return \"\"\n",
    "    if \",\" in n: return n.split(\",\")[0].strip()\n",
    "    tokens = n.split()\n",
    "    return tokens[-1].strip() if tokens else n\n",
    "\n",
    "def _format_authors_for_mention(raw: str, max_names: int = 2) -> Optional[str]:\n",
    "    authors = _split_authors(raw)\n",
    "    if not authors: return None\n",
    "    last_names = [_last_name(a) for a in authors if _last_name(a)]\n",
    "    if not last_names: return None\n",
    "    if len(last_names) == 1: return last_names[0]\n",
    "    if len(last_names) == 2: return f\"{last_names[0]} y {last_names[1]}\"\n",
    "    return f\"{last_names[0]} et al.\"\n",
    "\n",
    "def _extract_year(row: pd.Series) -> Optional[str]:\n",
    "    for c in [\"year\", \"publication_year\", \"cover_date\", \"date\"]:\n",
    "        val = _safe_str(row.get(c))\n",
    "        m = re.search(r\"(19|20)\\d{2}\", val)\n",
    "        if m: return m.group(0)\n",
    "    return None\n",
    "\n",
    "# =================== Construcción de bloques ===================\n",
    "def build_context_blocks(df_reranked: pd.DataFrame,\n",
    "                         top_k: int = TOP_CONTEXT,\n",
    "                         max_chunk_chars: int = MAX_CHUNK_CHARS) -> List[Dict]:\n",
    "    if df_reranked is None or len(df_reranked) == 0:\n",
    "        raise ValueError(\"df_reranked está vacío.\")\n",
    "    cols_title = [c for c in [\"title\",\"chunk_title\"] if c in df_reranked.columns] or [\"title\"]\n",
    "    cols_abs   = [c for c in [\"abstract\",\"chunk_text\",\"summary\"] if c in df_reranked.columns] or [\"abstract\"]\n",
    "    blocks = []\n",
    "    for i in range(min(top_k, len(df_reranked))):\n",
    "        row = df_reranked.iloc[i]\n",
    "        title  = _first_nonempty(row, cols_title) or \"Sin título\"\n",
    "        body   = _first_nonempty(row, cols_abs)\n",
    "        if DO_TRIM_ABSTRACT: body = _shorten(body, max_chunk_chars)\n",
    "        authors_raw = _safe_str(row.get(\"authors\", \"\"))\n",
    "        year  = _extract_year(row)\n",
    "        doi_raw = _safe_str(row.get(\"doi\", \"\"))\n",
    "        blocks.append({\n",
    "            \"cite_id\": _id_for_row(row),   # para [n]\n",
    "            \"title\": title,\n",
    "            \"text\": body,\n",
    "            \"authors_mention\": _format_authors_for_mention(authors_raw),\n",
    "            \"authors_raw\": authors_raw,    # para 'Fuentes'\n",
    "            \"year\": year,\n",
    "            \"doi_raw\": doi_raw             # para 'Fuentes'\n",
    "        })\n",
    "    return blocks\n",
    "\n",
    "# =================== Fuentes: DOI y autores ===================\n",
    "def _doi_url(doi_raw: str) -> str:\n",
    "    doi = _safe_str(doi_raw).strip()\n",
    "    if not doi: return \"s/d\"\n",
    "    return doi if doi.lower().startswith(\"http\") else f\"https://doi.org/{doi}\"\n",
    "\n",
    "def _authors_cite_line(raw_authors: str) -> str:\n",
    "    names = _split_authors(_safe_str(raw_authors))\n",
    "    if not names: return \"Autor(es) no disponibles\"\n",
    "    return \"; \".join([_safe_str(n).strip() for n in names if _safe_str(n).strip()])\n",
    "\n",
    "def render_fuentes_from_blocks(blocks: List[Dict]) -> str:\n",
    "    lines = []\n",
    "    for i, b in enumerate(blocks, start=1):\n",
    "        autores = _authors_cite_line(b.get(\"authors_raw\", \"\"))\n",
    "        titulo  = _safe_str(b.get(\"title\", \"Sin título\"))\n",
    "        doiurl  = _doi_url(b.get(\"doi_raw\", \"\"))\n",
    "        lines.append(f\"[{i}] {autores}; \\\"{titulo}\\\"; {doiurl}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# =================== Prompt con instrucción de mención autoral ===================\n",
    "def compose_prompt(query: str, blocks: List[Dict], max_chars: int = MAX_INPUT_CHARS) -> str:\n",
    "    header = (\n",
    "        \"Eres un asistente académico. Responde de forma clara, concisa y basada en evidencia, \"\n",
    "        \"en español y estilo APA (7ª). Utiliza exclusivamente los fragmentos proporcionados. \"\n",
    "        \"Cada párrafo debe incluir una mención autoral explícita (‘según <autor>’ o ‘de acuerdo con <autor>’) \"\n",
    "        \"seguida de la cita en corchetes [n]. Si no hay autor disponible, usa ‘según la fuente [n]’. \"\n",
    "        \"No imprimas tu propia sección de 'Fuentes'; solo escribe el cuerpo con citas [n].\\n\\n\"\n",
    "        f\"Pregunta: {query}\\n\\n\"\n",
    "        \"Fragmentos (con autor/año si disponible):\\n\"\n",
    "    )\n",
    "    parts = []\n",
    "    for i, b in enumerate(blocks, start=1):\n",
    "        autor_m = b.get(\"authors_mention\") or f\"fuente [{i}]\"\n",
    "        anio    = f\" ({b['year']})\" if b.get(\"year\") else \"\"\n",
    "        head    = f\"[{i}] {b['title']} — Autor(es): {autor_m}{anio}\"\n",
    "        parts.append(f\"{head}\\n{_safe_str(b['text'])}\\n\")\n",
    "\n",
    "    footer = (\n",
    "        \"\\nInstrucciones de redacción:\\n\"\n",
    "        f\"- Debes usar las referencias [1..{len(blocks)}] tal como están definidas.\\n\"\n",
    "        \"- No inventes datos fuera de los fragmentos.\\n\"\n",
    "        \"- Cada párrafo debe incluir al menos una mención autoral y su [n] correspondiente.\\n\"\n",
    "        \"- No generes la sección 'Fuentes'.\\n\"\n",
    "    )\n",
    "    # Incluimos igualmente las 'Fuentes' en el prompt como guía, pero pedimos explícitamente que NO las imprima.\n",
    "    fuentes_prompt = render_fuentes_from_blocks(blocks)\n",
    "    prompt = header + \"\\n\".join(parts) + footer + \"\\nFuentes (guía, no imprimir):\\n\" + fuentes_prompt\n",
    "    return prompt[:max_chars]\n",
    "\n",
    "# =================== Generación vía servicio HTTP de Ollama (sin fallbacks) ===================\n",
    "def generate_with_ollama_http(prompt: str,\n",
    "                              model: str = OLLAMA_MODEL,\n",
    "                              temperature: float = TEMPERATURE,\n",
    "                              max_new_tokens: int = MAX_NEW_TOKENS,\n",
    "                              base_url: str = OLLAMA_HOST,\n",
    "                              timeout: int = HTTP_TIMEOUT_SECS) -> str:\n",
    "    url = f\"{base_url.rstrip('/')}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente académico que escribe en español (APA 7ª), conciso y basado en evidencia.\"},\n",
    "            {\"role\": \"user\",   \"content\": _safe_str(prompt)}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"temperature\": float(temperature),\n",
    "            \"num_predict\": int(max_new_tokens),\n",
    "            # Cortamos cuando el modelo intente empezar su propia sección de fuentes:\n",
    "            \"stop\": [\"\\nFuentes\", \"\\nFUENTES\", \"\\nReferences\", \"\\nREFERENCIAS\"]\n",
    "        },\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(url, json=payload, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Fallo al invocar el servicio HTTP de Ollama. \"\n",
    "            \"Verifica daemon, modelo descargado y conectividad.\\nDetalle: {e}\"\n",
    "        )\n",
    "    content = _safe_str(data.get(\"message\", {}).get(\"content\", \"\"))\n",
    "    if not content.strip():\n",
    "        raise RuntimeError(\"La respuesta de Ollama está vacía.\")\n",
    "    return content\n",
    "\n",
    "# =================== Auditoría de citas usadas ===================\n",
    "def extract_used_refs(answer_text: str, n_max: int) -> List[int]:\n",
    "    \"\"\"Extrae números [n] usados en el texto, filtrando por rango 1..n_max y preservando orden de aparición.\"\"\"\n",
    "    nums = [int(m.group(1)) for m in re.finditer(r\"\\[(\\d+)\\]\", _safe_str(answer_text))]\n",
    "    seen, used = set(), []\n",
    "    for n in nums:\n",
    "        if 1 <= n <= n_max and n not in seen:\n",
    "            used.append(n); seen.add(n)\n",
    "    return used\n",
    "\n",
    "def render_used_refs_report(answer_text: str, blocks: List[Dict]) -> str:\n",
    "    used = extract_used_refs(answer_text, len(blocks))\n",
    "    if not used:\n",
    "        return \"No se detectaron citas [n] en el texto.\"\n",
    "    lines = [\"Citas usadas en el texto:\"]\n",
    "    for n in used:\n",
    "        b = blocks[n-1]\n",
    "        lines.append(f\" - [{n}] {_authors_cite_line(b.get('authors_raw',''))}; \\\"{_safe_str(b.get('title','Sin título'))}\\\"; {_doi_url(b.get('doi_raw',''))}\")\n",
    "    # Aviso si el modelo no usó alguna fuente disponible\n",
    "    missing = [i for i in range(1, len(blocks)+1) if i not in used]\n",
    "    if missing:\n",
    "        lines.append(f\"No usadas: {missing}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# =================== Ejecución (ejemplo) ===================\n",
    "if 'reranked' not in globals():\n",
    "    raise RuntimeError(\"No encuentro 'reranked'. Ejecuta primero tu celda de re-ranking.\")\n",
    "\n",
    "query  = os.environ.get(\"QUERY\", \"el turismo en las islas galagos que tan bueno es?\")\n",
    "blocks = build_context_blocks(reranked, top_k=TOP_CONTEXT, max_chunk_chars=MAX_CHUNK_CHARS)\n",
    "prompt = compose_prompt(query, blocks, max_chars=MAX_INPUT_CHARS)\n",
    "\n",
    "print(\"=== Prompt (vista previa) ===\")\n",
    "print(textwrap.shorten(prompt, width=1000, placeholder=\" ...\"))\n",
    "\n",
    "answer = generate_with_ollama_http(prompt)\n",
    "print(\"\\n=== Respuesta (LLM - Ollama HTTP) ===\\n\")\n",
    "print(answer)\n",
    "\n",
    "# Resumen de citas usadas por el modelo\n",
    "print(\"\\n=== Auditoría de citas usadas ===\")\n",
    "print(render_used_refs_report(answer, blocks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89214b3",
   "metadata": {},
   "source": [
    "# Evaluación"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## retriever",
   "id": "826fce3761740b91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a8c0279fea0a4e90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:22:30.350986Z",
     "start_time": "2025-11-05T15:22:30.222241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# eval_chunk_uid_multi.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Set, Optional, Iterable\n",
    "\n",
    "# ====== importa tu búsqueda ======\n",
    "# from tu_modulo import search_min   # Debe devolver DataFrame con columna 'chunk_uid'\n",
    "\n",
    "K_LIST = [5, 10, 15]\n",
    "GT_PATH = os.environ.get(\"GT_PATH\", \"groundtruth.csv\")  # tab-separado por defecto\n",
    "\n",
    "def _split_list(cell: str) -> List[str]:\n",
    "    if pd.isna(cell) or not str(cell).strip():\n",
    "        return []\n",
    "    s = str(cell).replace(\"\\n\", \" \").strip()\n",
    "    parts = [p.strip() for p in s.replace(\";\", \",\").split(\",\")]\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def recall_at_k(hits: pd.DataFrame, relevant: Set[str], k: int) -> float:\n",
    "    if \"chunk_uid\" not in hits.columns or not relevant:\n",
    "        return 0.0\n",
    "    top = hits.head(k)\n",
    "    retrieved = set(top[\"chunk_uid\"].astype(str))\n",
    "    inter = len(retrieved & relevant)\n",
    "    denom = min(k, len(relevant))\n",
    "    return inter / denom if denom > 0 else 0.0\n",
    "\n",
    "def mrr_at_k(hits: pd.DataFrame, relevant: Set[str], k: int) -> float:\n",
    "    if \"chunk_uid\" not in hits.columns or not relevant:\n",
    "        return 0.0\n",
    "    top = hits.head(k)[\"chunk_uid\"].astype(str).tolist()\n",
    "    for i, uid in enumerate(top, start=1):\n",
    "        if uid in relevant:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_chunk_uid_multi(gt_path: str = GT_PATH, ks: Iterable[int] = K_LIST) -> pd.DataFrame:\n",
    "    ks = sorted(set(int(x) for x in ks))\n",
    "    gt = pd.read_csv(gt_path, sep=\"\\t\", dtype=str).fillna(\"\")\n",
    "    need = {\"id\", \"consulta\", \"fragmentos\"}\n",
    "    if not need.issubset(gt.columns):\n",
    "        missing = need - set(gt.columns)\n",
    "        raise ValueError(f\"Faltan columnas en GT: {missing}\")\n",
    "\n",
    "    # contenedores de macro\n",
    "    macro_recalls = {k: [] for k in ks}\n",
    "    macro_mrrs    = {k: [] for k in ks}\n",
    "\n",
    "    rows = []\n",
    "    for _, r in gt.iterrows():\n",
    "        qid   = r[\"id\"]\n",
    "        query = r[\"consulta\"]\n",
    "        gold  = set(_split_list(r[\"fragmentos\"]))\n",
    "\n",
    "        hits = search_min(query, topk=max(ks))  # buscamos 1 vez con el mayor k\n",
    "        if \"chunk_uid\" not in hits.columns:\n",
    "            raise ValueError(\"search_min no devuelve columna 'chunk_uid'.\")\n",
    "\n",
    "        row_out = {\"id\": qid, \"tipo\": r.get(\"tipo\", \"\"), \"consulta\": query}\n",
    "        for k in ks:\n",
    "            rec = recall_at_k(hits, gold, k)\n",
    "            mrr = mrr_at_k(hits, gold, k)\n",
    "            row_out[f\"recall@{k}\"] = rec\n",
    "            row_out[f\"mrr@{k}\"]    = mrr\n",
    "            macro_recalls[k].append(rec)\n",
    "            macro_mrrs[k].append(mrr)\n",
    "\n",
    "        rows.append(row_out)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # imprimir macro\n",
    "    print(\"\\n== MACRO ==\")\n",
    "    for k in ks:\n",
    "        mr = sum(macro_recalls[k]) / len(macro_recalls[k]) if macro_recalls[k] else float(\"nan\")\n",
    "        mm = sum(macro_mrrs[k]) / len(macro_mrrs[k])       if macro_mrrs[k] else float(\"nan\")\n",
    "        print(f\"Recall@{k}: {mr:.3f} | MRR@{k}: {mm:.3f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    out = evaluate_chunk_uid_multi(GT_PATH, ks=[5,10,15])\n",
    "    out.to_csv(\"retrieval_eval_chunk_multi.tsv\", sep=\"\\t\", index=False)\n",
    "    print(\"Guardado: retrieval_eval_chunk_multi.tsv\")\n"
   ],
   "id": "29a813efe0dc9257",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== MACRO ==\n",
      "Recall@5: 0.533 | MRR@5: 0.370\n",
      "Recall@10: 0.533 | MRR@10: 0.370\n",
      "Recall@15: 0.567 | MRR@15: 0.370\n",
      "Guardado: retrieval_eval_chunk_multi.tsv\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## augmentation",
   "id": "c4d0d8cd5ac10f43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T17:02:06.926804Z",
     "start_time": "2025-11-05T16:57:29.749418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, csv, re, json, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- Config ---\n",
    "GT_PATH     = os.environ.get(\"GT_PATH\", \"groundtruth.csv\")  # TSV (tabs)\n",
    "ID_COL      = \"chunkuid\"                  # tus resultados deben tener esta columna\n",
    "TEXT_COLS   = [\"title\",\"abstract\"]        # o [\"chunk_text\"]\n",
    "CTX_K_BASIC = int(os.environ.get(\"CTX_K_BASIC\", \"6\"))\n",
    "CTX_K_FULL  = int(os.environ.get(\"CTX_K_FULL\", \"6\"))\n",
    "EMB_MODEL   = os.environ.get(\"EMB_MODEL\", \"intfloat/multilingual-e5-small\")\n",
    "\n",
    "# Ollama\n",
    "OLLAMA_HOST  = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = os.environ.get(\"OLLAMA_MODEL\", \"gemma3:4b\")\n",
    "_OLLAMA_CHAT_URL = f\"{OLLAMA_HOST.rstrip('/')}/api/chat\"\n",
    "\n",
    "# ==================== GT Loader (TSV, columnas exactas) ====================\n",
    "def load_gt(path: str = GT_PATH, sep: str = \"\\t\", id_sep: str = \"|\") -> Dict[str, Dict]:\n",
    "    df = pd.read_csv(path, sep=sep, engine=\"python\",\n",
    "                     quoting=csv.QUOTE_MINIMAL, encoding=\"utf-8\",\n",
    "                     keep_default_na=False, on_bad_lines=\"error\")\n",
    "    required = {\"consulta\", \"fragmentos\", \"Respuesta-ideal\"}\n",
    "    missing  = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Faltan columnas {missing}. Presentes: {list(df.columns)}\")\n",
    "    def parse_ids(s: str) -> List[str]:\n",
    "        return [t.strip() for t in (s or \"\").split(id_sep) if t.strip()]\n",
    "    out = {}\n",
    "    for _, r in df.iterrows():\n",
    "        q     = str(r[\"consulta\"]).strip()\n",
    "        rel   = set(parse_ids(str(r[\"fragmentos\"]).strip()))   # chunkuid\n",
    "        ideal = str(r[\"Respuesta-ideal\"]).strip()\n",
    "        tipo  = str(r.get(\"tipo\",\"\")).strip()\n",
    "        out[q] = {\"relevant_ids\": rel, \"ideal_answer\": ideal, \"tipo\": tipo}\n",
    "    return out\n",
    "\n",
    "# ==================== Utilidades de contexto ====================\n",
    "def ensure_chunkuid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if ID_COL in df.columns:\n",
    "        return df\n",
    "    for alt in [\"chunk_uid\",\"chunk_id\",\"chunkId\",\"vec_id\",\"scopus_id\",\"id\"]:\n",
    "        if alt in df.columns:\n",
    "            return df.rename(columns={alt: ID_COL})\n",
    "    raise KeyError(\"El DataFrame no tiene 'chunkuid' (nib variantes).\")\n",
    "\n",
    "def first_text(row: pd.Series, cols: Sequence[str]) -> str:\n",
    "    for c in cols:\n",
    "        if c in row and isinstance(row[c], str) and row[c].strip():\n",
    "            return row[c]\n",
    "    for c in row.index:\n",
    "        if any(t in c.lower() for t in (\"title\",\"abstract\",\"summary\",\"chunk\",\"keywords\",\"desc\")):\n",
    "            v = row[c]\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v\n",
    "    return \"\"\n",
    "\n",
    "def build_context(df: pd.DataFrame, cols: Sequence[str], k_ctx: int) -> List[Dict]:\n",
    "    return [{\"id\": str(row[ID_COL]), \"text\": first_text(row, cols)}\n",
    "            for _, row in df.head(k_ctx).iterrows()]\n",
    "\n",
    "# ==================== Cliente LLM (Ollama gemma3:4b) ====================\n",
    "def llm_call(system_prompt: str, user_prompt: str, *,\n",
    "             temperature: float = 0.2,\n",
    "             num_predict: int | None = None,\n",
    "             timeout: float = 120.0) -> str:\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt or \"\"},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt or \"\"},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": { \"temperature\": temperature }\n",
    "    }\n",
    "    if num_predict is not None:\n",
    "        payload[\"options\"][\"num_predict\"] = int(num_predict)\n",
    "    try:\n",
    "        resp = requests.post(_OLLAMA_CHAT_URL, json=payload, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return (data.get(\"message\") or {}).get(\"content\", \"\") or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Ollama request failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ==================== Generadores con prompts estrictos ====================\n",
    "_ID_PATTERN = re.compile(r'\\[([A-Za-z0-9_\\-:./]+)\\]')\n",
    "\n",
    "def extract_ids(text: str) -> List[str]:\n",
    "    return _ID_PATTERN.findall(text or \"\")\n",
    "\n",
    "def truncate_text(t: str, max_chars: int = 4000) -> str:\n",
    "    t = t or \"\"\n",
    "    return t if len(t) <= max_chars else t[:max_chars]\n",
    "\n",
    "def gen_llm_only(query: str) -> Dict:\n",
    "    ans = llm_call(\n",
    "        \"Responde brevemente y con precisión. No cites IDs.\",\n",
    "        f\"Pregunta: {query}\"\n",
    "    )\n",
    "    return {\"answer\": ans, \"cited_ids\": []}\n",
    "\n",
    "def gen_rag_basic(query: str, ctx_docs: List[Dict]) -> Dict:\n",
    "    # Concatenación directa de top-k (pre-rerank)\n",
    "    ctx = \"\\n\\n\".join(f\"[{d['id']}]\\n{truncate_text(d['text'])}\" for d in ctx_docs)\n",
    "    prompt = (\n",
    "        \"Lee los CONTEXTOS y responde SOLO con información presente en ellos. \"\n",
    "        \"No utilices conocimiento previo. Si no hay suficiente información, responde exactamente: 'No hay evidencia suficiente'. \"\n",
    "        \"Incluye entre corchetes los [ID] de los fragmentos realmente usados.\\n\\n\"\n",
    "        f\"CONTEXTOS:\\n{ctx}\\n\\nPREGUNTA:\\n{query}\"\n",
    "    )\n",
    "    ans = llm_call(\"Eres un asistente factual y literal.\", prompt)\n",
    "    return {\"answer\": ans, \"cited_ids\": extract_ids(ans)}\n",
    "\n",
    "def gen_rag_full(query: str, ctx_docs: List[Dict]) -> Dict:\n",
    "    # Builder: resumen intermedio por bloques (post-rerank)\n",
    "    blocks = \"\\n\\n\".join(f\"[{d['id']}]\\n{truncate_text(d['text'])}\" for d in ctx_docs)\n",
    "    summary = llm_call(\n",
    "        \"Eres un sintetizador extractivo. No inventes. Conserva referencias [ID] cuando cites hechos.\",\n",
    "        \"Resume con viñetas, agrupando por [ID]. Extrae SOLO hechos verificables; no agregues conocimiento fuera de CONTEXTOS.\\n\\n\"\n",
    "        f\"CONTEXTOS:\\n{blocks}\"\n",
    "    )\n",
    "    prompt = (\n",
    "        \"Usa SOLO la información de los CONTEXTOS siguientes. \"\n",
    "        \"Cada afirmación DEBE incluir entre corchetes el [ID] del fragmento donde aparece. \"\n",
    "        \"Si no hay información suficiente, responde exactamente: 'No hay evidencia suficiente'.\"\n",
    "    )\n",
    "    ans = llm_call(\"Eres estricto con el soporte textual; cada afirmación debe acabar con [ID].\", prompt)\n",
    "    return {\"answer\": ans, \"cited_ids\": extract_ids(ans)}\n",
    "\n",
    "# ==================== Embeddings y métricas ====================\n",
    "_emb = None\n",
    "def _emb_model():\n",
    "    global _emb\n",
    "    if _emb is None:\n",
    "        _emb = SentenceTransformer(EMB_MODEL, device=\"cpu\")\n",
    "        print(f\"[INFO] Embedder: {EMB_MODEL}\")\n",
    "    return _emb\n",
    "\n",
    "def _cos_sim(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    return float(np.clip(np.dot(u, v), -1.0, 1.0))\n",
    "\n",
    "def _embed_norm(texts: List[str]) -> np.ndarray:\n",
    "    m = _emb_model()\n",
    "    return m.encode(texts, normalize_embeddings=True, convert_to_numpy=True)\n",
    "\n",
    "def exact_match(pred: str, gold: str) -> float:\n",
    "    a = (pred or \"\").strip().lower()\n",
    "    b = (gold or \"\").strip().lower()\n",
    "    return 1.0 if a and b and a == b else 0.0\n",
    "\n",
    "def context_support_score(answer: str, ctx_docs: List[Dict]) -> float:\n",
    "    \"\"\"Proxy de fidelidad: similitud entre respuesta y el texto agregado del contexto.\"\"\"\n",
    "    if not answer or not ctx_docs:\n",
    "        return 0.0\n",
    "    ctx_text = \" \".join(d[\"text\"] or \"\" for d in ctx_docs)\n",
    "    emb = _emb_model()\n",
    "    a_vec = emb.encode(answer, normalize_embeddings=True)\n",
    "    c_vec = emb.encode(ctx_text, normalize_embeddings=True)\n",
    "    return _cos_sim(a_vec, c_vec)\n",
    "\n",
    "def gen_metrics(answer: str, ideal_answer: str, cited_ids: List[str],\n",
    "                rel_ids: set, ctx_docs: List[Dict] | None = None) -> Dict[str, float]:\n",
    "    em  = exact_match(answer, ideal_answer)\n",
    "    sim = 0.0\n",
    "    if ideal_answer.strip():\n",
    "        v = _embed_norm([answer or \"\", ideal_answer])[0:2]\n",
    "        sim = _cos_sim(v[0], v[1])\n",
    "    cited = set(cited_ids or [])\n",
    "    cov   = (len(cited & rel_ids)/len(cited)) if cited else 0.0\n",
    "    hallu = 1.0 if (cited and len(cited & rel_ids) == 0) else 0.0\n",
    "    faith = context_support_score(answer, ctx_docs or [])\n",
    "    return {\"EM\": em, \"SemanticSim\": sim, \"Faithfulness\": faith,\n",
    "            \"CitationCoverage\": cov, \"Hallucination\": hallu}\n",
    "\n",
    "# ==================== Recuperación (para armar contexto) ====================\n",
    "def retrieve_topk(query: str, topk: int = 200) -> pd.DataFrame:\n",
    "    df = search_full_scopus(query, topk=topk).reset_index(drop=True)  # <- tu función\n",
    "    return ensure_chunkuid(df)\n",
    "\n",
    "def rerank_ce(query: str, df_topk: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_r = rerank_with_cross_encoder(\n",
    "        query_text=query, df_topk=df_topk, text_cols=TEXT_COLS,\n",
    "        score_dense_col=\"score\", fuse_with_dense=True\n",
    "    )\n",
    "    return ensure_chunkuid(df_r)\n",
    "\n",
    "# ==================== Runner: AUGMENTATION mejorado ====================\n",
    "def run_augmentation_only() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    gt = load_gt(GT_PATH)\n",
    "    rows = []\n",
    "    for q, info in gt.items():\n",
    "        rel  = info[\"relevant_ids\"]\n",
    "        gold = info[\"ideal_answer\"]\n",
    "\n",
    "        # LLM-only (sin contexto)\n",
    "        out0 = gen_llm_only(q)\n",
    "        m0   = gen_metrics(out0[\"answer\"], gold, out0.get(\"cited_ids\",[]), rel, ctx_docs=[])\n",
    "\n",
    "        # RAG-básico (pre-rerank)\n",
    "        df_b  = retrieve_topk(q, topk=200)\n",
    "        ctx_b = build_context(df_b, TEXT_COLS, k_ctx=CTX_K_BASIC)\n",
    "        out1  = gen_rag_basic(q, ctx_b)\n",
    "        m1    = gen_metrics(out1[\"answer\"], gold, out1.get(\"cited_ids\",[]), rel, ctx_docs=ctx_b)\n",
    "\n",
    "        # RAG-full (post-rerank + builder)\n",
    "        df_f  = rerank_ce(q, df_b)\n",
    "        ctx_f = build_context(df_f, TEXT_COLS, k_ctx=CTX_K_FULL)\n",
    "        out2  = gen_rag_full(q, ctx_f)\n",
    "        m2    = gen_metrics(out2[\"answer\"], gold, out2.get(\"cited_ids\",[]), rel, ctx_docs=ctx_f)\n",
    "\n",
    "        rows += [\n",
    "            {\"cond\":\"LLM-only\", \"consulta\":q, **m0},\n",
    "            {\"cond\":\"RAG-basic\",\"consulta\":q, **m1},\n",
    "            {\"cond\":\"RAG-full\", \"consulta\":q, **m2},\n",
    "        ]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Macro por condición (incluye Faithfulness)\n",
    "    macro = (df.groupby(\"cond\")\n",
    "             .agg({\"EM\":\"mean\",\"SemanticSim\":\"mean\",\"Faithfulness\":\"mean\",\n",
    "                   \"CitationCoverage\":\"mean\",\"Hallucination\":\"mean\"})\n",
    "             .reset_index())\n",
    "\n",
    "    # Δ vs LLM-only (ganancia de augmentation)\n",
    "    base = macro.set_index(\"cond\").loc[\"LLM-only\"]\n",
    "    for cond in [\"RAG-basic\",\"RAG-full\"]:\n",
    "        for m in [\"EM\",\"SemanticSim\",\"Faithfulness\",\"CitationCoverage\",\"Hallucination\"]:\n",
    "            macro.loc[macro[\"cond\"]==cond, f\"Δ_{m}_vs_LLM\"] = macro.set_index(\"cond\").loc[cond, m] - base[m]\n",
    "\n",
    "    print(\"== AUGMENTATION (MEJORADO): MACRO POR CONDICIÓN ==\")\n",
    "    print(macro.round(3).to_string(index=False))\n",
    "\n",
    "    os.makedirs(\"runs_eval\", exist_ok=True)\n",
    "    df.to_csv(\"runs_eval/augmentation_per_query.csv\", index=False)\n",
    "    macro.to_csv(\"runs_eval/augmentation_macro.csv\", index=False)\n",
    "    return df, macro\n",
    "\n",
    "# ==================== Ejecutar ====================\n",
    "df_aug_per_query, df_aug_macro = run_augmentation_only()\n"
   ],
   "id": "d2d7fc886171f667",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Embedder: intfloat/multilingual-e5-small\n",
      "== AUGMENTATION (MEJORADO): MACRO POR CONDICIÓN ==\n",
      "     cond  EM  SemanticSim  Faithfulness  CitationCoverage  Hallucination  Δ_EM_vs_LLM  Δ_SemanticSim_vs_LLM  Δ_Faithfulness_vs_LLM  Δ_CitationCoverage_vs_LLM  Δ_Hallucination_vs_LLM\n",
      " LLM-only 0.0        0.893         0.000               0.0            0.0          NaN                   NaN                    NaN                        NaN                     NaN\n",
      "RAG-basic 0.0        0.807         0.778               0.0            0.0          0.0                -0.087                  0.778                        0.0                     0.0\n",
      " RAG-full 0.0        0.818         0.834               0.0            0.0          0.0                -0.075                  0.834                        0.0                     0.0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T18:38:52.515679Z",
     "start_time": "2025-11-05T18:38:30.661873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==================== AUGMENTATION: EVALUACIÓN SOLO DE CONTEXTO (usa tu recuperador) ====================\n",
    "# Métricas: Context Recall / Precision / Noise / Compression Ratio / Context Length\n",
    "# Comparación: RAG-basic (concat top-k pre-rerank) vs RAG-full (builder sobre rerank)\n",
    "# SIN usar la respuesta del LLM (solo contextos)\n",
    "# Usa los CHUNKUID desde tu recuperador (meta_min + scopus CSV)\n",
    "\n",
    "import os, re, csv, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Sequence, Tuple, Iterable\n",
    "\n",
    "# ========= CONFIG =========\n",
    "GT_PATH       = os.environ.get(\"GT_PATH\", \"groundtruth.csv\")   # ⚠️ TSV por tabs aunque sea .csv\n",
    "ID_COL        = os.environ.get(\"ID_COL\", \"chunkuid\")\n",
    "TEXT_COLS     = [c.strip() for c in os.environ.get(\"TEXT_COLS\", \"title,abstract\").split(\",\")]\n",
    "CTX_K_BASIC   = int(os.environ.get(\"CTX_K_BASIC\", \"6\"))\n",
    "CTX_K_FULL    = int(os.environ.get(\"CTX_K_FULL\", \"6\"))\n",
    "SIM_EMB_MODEL = os.environ.get(\"EMB_MODEL\", \"intfloat/multilingual-e5-small\")\n",
    "SIM_TAU       = float(os.environ.get(\"CTX_SIM_TAU\", \"0.80\"))\n",
    "\n",
    "# ========= FUNCIONES QUE YA DEBES TENER EN TU ENTORNO =========\n",
    "# - load_pkl_and_model() -> (model, meta_min)   [meta_min con columnas: chunk_uid, scopus_id, ...]\n",
    "# - load_scopus_csv()     -> DataFrame del CSV   [con columnas: scopus_id, title, abstract, ...]\n",
    "# - search_full_scopus(query, topk)\n",
    "# - rerank_with_cross_encoder(query_text, df_topk, text_cols, score_dense_col, fuse_with_dense)\n",
    "# - first_text(row, TEXT_COLS)\n",
    "# - ensure_chunkuid(df)   (renombra chunk_uid -> chunkuid si aplica)\n",
    "# - build_context(df, TEXT_COLS, k_ctx) -> [{'id','text'}, ...]\n",
    "\n",
    "# ========= GT LOADER =========\n",
    "def load_gt_context(path: str = GT_PATH, sep: str = \"\\t\", id_sep: str = \"|\") -> Dict[str, Dict]:\n",
    "    df = pd.read_csv(path, sep=sep, engine=\"python\", quoting=csv.QUOTE_MINIMAL,\n",
    "                     encoding=\"utf-8\", keep_default_na=False, on_bad_lines=\"error\")\n",
    "    req = {\"consulta\",\"fragmentos\"}\n",
    "    miss = req - set(df.columns)\n",
    "    if miss:\n",
    "        raise KeyError(f\"Faltan columnas {miss}. Presentes: {list(df.columns)}\")\n",
    "    def split_ids(s: str) -> List[str]:\n",
    "        return [t.strip() for t in (s or \"\").split(id_sep) if t.strip()]\n",
    "    out = {}\n",
    "    for _, r in df.iterrows():\n",
    "        q   = str(r[\"consulta\"]).strip()\n",
    "        rel = set(split_ids(str(r[\"fragmentos\"]).strip()))  # chunkuid\n",
    "        out[q] = {\"relevant_ids\": rel}\n",
    "    return out\n",
    "\n",
    "# ========= MAPA chunkuid -> texto (vía tu recuperador) =========\n",
    "_meta_min_cache: pd.DataFrame | None = None\n",
    "_scopus_df_cache: pd.DataFrame | None = None\n",
    "_chunkuid_to_text: Dict[str, str] | None = None\n",
    "\n",
    "def _warm_retriever_caches():\n",
    "    \"\"\"Precarga meta_min (PKL) y scopus CSV usando tus loaders, sin tocar el FAISS.\"\"\"\n",
    "    global _meta_min_cache, _scopus_df_cache\n",
    "    if _meta_min_cache is not None and _scopus_df_cache is not None:\n",
    "        return\n",
    "    _, meta_min = load_pkl_and_model()          # tu función: devuelve (model, meta_min)\n",
    "    sc = load_scopus_csv()                      # tu función: devuelve df del CSV\n",
    "    _meta_min_cache = meta_min.copy()\n",
    "    _scopus_df_cache = sc.copy()\n",
    "\n",
    "def _build_chunkuid_text_map():\n",
    "    \"\"\"Construye un dict {chunkuid -> texto} uniendo meta_min (chunk_uid, scopus_id) con scopus CSV (TEXT_COLS).\"\"\"\n",
    "    global _chunkuid_to_text\n",
    "    if _chunkuid_to_text is not None:\n",
    "        return\n",
    "    _warm_retriever_caches()\n",
    "    mm = _meta_min_cache\n",
    "    sc = _scopus_df_cache\n",
    "\n",
    "    # Asegurar columnas canónicas\n",
    "    if \"chunk_uid\" not in mm.columns:\n",
    "        raise KeyError(\"meta_min no tiene columna 'chunk_uid'\")\n",
    "    if \"scopus_id\" not in mm.columns:\n",
    "        raise KeyError(\"meta_min no tiene columna 'scopus_id'\")\n",
    "    if \"scopus_id\" not in sc.columns:\n",
    "        raise KeyError(\"scopus CSV no tiene columna 'scopus_id'\")\n",
    "\n",
    "    # Unir chunk_uid -> scopus_id -> textos\n",
    "    mm_small = mm[[\"chunk_uid\",\"scopus_id\"]].drop_duplicates()\n",
    "    sc_small = sc[[\"scopus_id\"] + [c for c in TEXT_COLS if c in sc.columns]].copy()\n",
    "    merged = mm_small.merge(sc_small, how=\"left\", on=\"scopus_id\")\n",
    "\n",
    "    # Construir texto por prioridad de TEXT_COLS\n",
    "    def _row_text(row: pd.Series) -> str:\n",
    "        for c in TEXT_COLS:\n",
    "            if c in row and isinstance(row[c], str) and row[c].strip():\n",
    "                return row[c]\n",
    "        # fallback: concatenar cualquier columna de texto que exista\n",
    "        parts = []\n",
    "        for c in row.index:\n",
    "            v = row[c]\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                parts.append(v.strip())\n",
    "        return \" \".join(parts)\n",
    "    merged[\"__txt__\"] = merged.apply(_row_text, axis=1)\n",
    "    _chunkuid_to_text = dict(zip(merged[\"chunk_uid\"].astype(str), merged[\"__txt__\"].astype(str)))\n",
    "\n",
    "def text_for_chunkuid(cuid: str) -> str:\n",
    "    \"\"\"Devuelve el texto (p.ej. title/abstract) asociado al chunkuid usando tu recuperador.\"\"\"\n",
    "    _build_chunkuid_text_map()\n",
    "    return _chunkuid_to_text.get(str(cuid), \"\")\n",
    "\n",
    "# ========= Utilidades: oraciones y tokens =========\n",
    "_SENT_RE = re.compile(r'(?<=[\\.\\?\\!；;:])\\s+|\\n+')\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = [p.strip() for p in _SENT_RE.split(text) if p and p.strip()]\n",
    "    return [p for p in parts if len(p.split()) >= 3]\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    try:\n",
    "        import tiktoken\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(enc.encode(text))\n",
    "    except Exception:\n",
    "        return len(text.split())\n",
    "\n",
    "def total_tokens(texts: Iterable[str]) -> int:\n",
    "    return sum(count_tokens(t) for t in texts)\n",
    "\n",
    "# ========= Embeddings y similitud =========\n",
    "from sentence_transformers import SentenceTransformer\n",
    "_emb_model: SentenceTransformer | None = None\n",
    "\n",
    "def get_emb():\n",
    "    global _emb_model\n",
    "    if _emb_model is None:\n",
    "        _emb_model = SentenceTransformer(SIM_EMB_MODEL, device=\"cpu\")\n",
    "        print(f\"[CTX] Loaded embedder: {SIM_EMB_MODEL}\")\n",
    "    return _emb_model\n",
    "\n",
    "def embed_sentences(sents: List[str]) -> np.ndarray:\n",
    "    if not sents:\n",
    "        return np.zeros((0, 384), dtype=float)\n",
    "    emb = get_emb()\n",
    "    return emb.encode(sents, normalize_embeddings=True)\n",
    "\n",
    "def cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    if A.size == 0 or B.size == 0:\n",
    "        return np.zeros((A.shape[0], B.shape[0]))\n",
    "    return np.clip(A @ B.T, -1.0, 1.0)\n",
    "\n",
    "# ========= Construcción de contextos (BASIC vs FULL) =========\n",
    "def build_basic_context_text(df_topk: pd.DataFrame, k_ctx: int) -> str:\n",
    "    ctx = build_context(df_topk, TEXT_COLS, k_ctx=k_ctx)  # [{'id','text'},...]\n",
    "    return \"\\n\\n\".join(f\"[{c['id']}]\\n{c['text']}\" for c in ctx if c.get(\"text\"))\n",
    "\n",
    "def build_full_context_text(query: str, df_topk: pd.DataFrame, k_ctx: int) -> str:\n",
    "    df_r  = rerank_with_cross_encoder(\n",
    "        query_text=query, df_topk=df_topk, text_cols=TEXT_COLS,\n",
    "        score_dense_col=\"score\", fuse_with_dense=True\n",
    "    )\n",
    "    df_r  = ensure_chunkuid(df_r)\n",
    "    ctx   = build_context(df_r, TEXT_COLS, k_ctx=k_ctx)\n",
    "    # Heurístico simple de “builder”: 2 oraciones por chunk\n",
    "    joined = []\n",
    "    for c in ctx:\n",
    "        sents = split_sentences(c.get(\"text\",\"\"))\n",
    "        joined.append(f\"[{c['id']}]\\n\" + \" \".join(sents[:2]))\n",
    "    return \"\\n\\n\".join(joined)\n",
    "\n",
    "# ========= Métricas de contexto =========\n",
    "def context_metrics_for(query: str, rel_ids: set, df_topk: pd.DataFrame,\n",
    "                        mode: str, k_ctx: int, tau: float = SIM_TAU) -> Dict:\n",
    "    \"\"\"\n",
    "    mode: 'basic' (pre-rerank, concat top-k)  |  'full' (post-rerank + builder)\n",
    "    \"\"\"\n",
    "    df_topk = ensure_chunkuid(df_topk)\n",
    "\n",
    "    if mode == \"basic\":\n",
    "        ctx_text = build_basic_context_text(df_topk, k_ctx)\n",
    "    elif mode == \"full\":\n",
    "        ctx_text = build_full_context_text(query, df_topk, k_ctx)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'basic' or 'full'\")\n",
    "\n",
    "    # Oraciones del contexto (C)\n",
    "    C_sents = split_sentences(ctx_text)\n",
    "    C_tokens = total_tokens(C_sents)\n",
    "\n",
    "    # Oraciones de evidencia (E) desde GT (usando mapa chunkuid->texto del recuperador)\n",
    "    E_texts = [text_for_chunkuid(cid) for cid in rel_ids]\n",
    "    E_sents = []\n",
    "    for t in E_texts:\n",
    "        E_sents.extend(split_sentences(t))\n",
    "    E_tokens = total_tokens(E_sents)\n",
    "\n",
    "    # Si no hay evidencia GT para la consulta\n",
    "    if len(E_sents) == 0:\n",
    "        return {\n",
    "            \"cond\": f\"RAG-{mode}\",\n",
    "            \"consulta\": query,\n",
    "            \"E_sents\": 0, \"C_sents\": len(C_sents),\n",
    "            \"context_recall\": np.nan,\n",
    "            \"context_precision\": np.nan,\n",
    "            \"context_noise\": np.nan,\n",
    "            \"context_tokens\": C_tokens,\n",
    "            \"evidence_tokens\": E_tokens,\n",
    "            \"matched_E\": 0, \"matched_C\": 0,\n",
    "            \"matched_tokens\": 0,\n",
    "            \"compression_ratio\": 0.0,\n",
    "            \"tau\": tau,\n",
    "        }\n",
    "\n",
    "    # Similitud C×E\n",
    "    C_emb = embed_sentences(C_sents)   # |C| x D\n",
    "    E_emb = embed_sentences(E_sents)   # |E| x D\n",
    "    S = cosine_sim_matrix(C_emb, E_emb)\n",
    "\n",
    "    # Matches por umbral\n",
    "    C_match_any = (S >= tau).any(axis=1) if S.size else np.zeros(len(C_sents), dtype=bool)\n",
    "    E_match_any = (S >= tau).any(axis=0) if S.size else np.zeros(len(E_sents), dtype=bool)\n",
    "\n",
    "    matched_C = int(C_match_any.sum())\n",
    "    matched_E = int(E_match_any.sum())\n",
    "\n",
    "    # Métricas\n",
    "    context_recall    = matched_E / len(E_sents)\n",
    "    context_precision = (matched_C / len(C_sents)) if C_sents else np.nan\n",
    "    context_noise     = 1.0 - (context_precision if not math.isnan(context_precision) else 0.0)\n",
    "\n",
    "    # Compresión: tokens de oraciones de C que matchean / tokens totales de C\n",
    "    matched_tokens = sum(count_tokens(s) for i, s in enumerate(C_sents) if C_match_any[i])\n",
    "    compression_ratio = (matched_tokens / C_tokens) if C_tokens > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"cond\": f\"RAG-{mode}\",\n",
    "        \"consulta\": query,\n",
    "        \"E_sents\": len(E_sents), \"C_sents\": len(C_sents),\n",
    "        \"context_recall\": context_recall,\n",
    "        \"context_precision\": context_precision,\n",
    "        \"context_noise\": context_noise,\n",
    "        \"context_tokens\": C_tokens,\n",
    "        \"evidence_tokens\": E_tokens,\n",
    "        \"matched_E\": matched_E, \"matched_C\": matched_C,\n",
    "        \"matched_tokens\": matched_tokens,\n",
    "        \"compression_ratio\": compression_ratio,\n",
    "        \"tau\": tau,\n",
    "    }\n",
    "\n",
    "# ========= Runner =========\n",
    "def run_augmentation_context_only() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    # calentar caches del recuperador para mapa chunkuid->texto\n",
    "    _warm_retriever_caches()\n",
    "    _build_chunkuid_text_map()\n",
    "\n",
    "    gt = load_gt_context(GT_PATH)\n",
    "    all_rows = []\n",
    "    for q, info in gt.items():\n",
    "        rel = info[\"relevant_ids\"]  # set de chunkuid (evidencia GT)\n",
    "\n",
    "        # top-k candidatos del recuperador\n",
    "        df_topk = search_full_scopus(q, topk=max(CTX_K_BASIC, CTX_K_FULL, 50)).reset_index(drop=True)\n",
    "        df_topk = ensure_chunkuid(df_topk)\n",
    "\n",
    "        # métricas por condición\n",
    "        mb = context_metrics_for(q, rel, df_topk, mode=\"basic\", k_ctx=CTX_K_BASIC, tau=SIM_TAU)\n",
    "        mf = context_metrics_for(q, rel, df_topk, mode=\"full\",  k_ctx=CTX_K_FULL,  tau=SIM_TAU)\n",
    "        all_rows += [mb, mf]\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # Macro por condición\n",
    "    agg_cols = [\"context_recall\",\"context_precision\",\"context_noise\",\"compression_ratio\",\"context_tokens\"]\n",
    "    macro = (df.groupby(\"cond\")[agg_cols].mean(numeric_only=True).reset_index())\n",
    "\n",
    "    # Δ (RAG-full − RAG-basic)\n",
    "    m_basic = macro.set_index(\"cond\").loc[\"RAG-basic\"]\n",
    "    m_full  = macro.set_index(\"cond\").loc[\"RAG-full\"]\n",
    "    deltas = {\n",
    "        \"Δ_context_recall\":    m_full[\"context_recall\"]    - m_basic[\"context_recall\"],\n",
    "        \"Δ_context_precision\": m_full[\"context_precision\"] - m_basic[\"context_precision\"],\n",
    "        \"Δ_context_noise\":     m_full[\"context_noise\"]     - m_basic[\"context_noise\"],\n",
    "        \"Δ_compression_ratio\": m_full[\"compression_ratio\"] - m_basic[\"compression_ratio\"],\n",
    "        \"Δ_context_tokens\":    m_full[\"context_tokens\"]    - m_basic[\"context_tokens\"],\n",
    "    }\n",
    "    delta_df = pd.DataFrame([deltas])\n",
    "\n",
    "    print(\"== AUGMENTATION (CONTEXT-ONLY, vía recuperador): MACRO ==\")\n",
    "    print(macro.round(3).to_string(index=False))\n",
    "    print(\"\\n== Δ (RAG-full − RAG-basic) ==\")\n",
    "    print(delta_df.round(3).to_string(index=False))\n",
    "\n",
    "    os.makedirs(\"runs_eval\", exist_ok=True)\n",
    "    df.to_csv(\"runs_eval/context_eval_per_query.csv\", index=False)\n",
    "    macro.to_csv(\"runs_eval/context_eval_macro.csv\", index=False)\n",
    "    delta_df.to_csv(\"runs_eval/context_eval_deltas_full_minus_basic.csv\", index=False)\n",
    "    return df, macro, delta_df\n",
    "\n",
    "# ========== Ejecutar ==========\n",
    "df_ctx_per_q, df_ctx_macro, df_ctx_deltas = run_augmentation_context_only()\n"
   ],
   "id": "2c5749193402a2dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modelo: intfloat/multilingual-e5-small | max_seq_length=300\n",
      "[INFO] meta_min columnas: ['vec_id', 'chunk_uid', 'doc_id', 'chunk_id', 'start_token', 'end_token', 'scopus_id'] | filas=21005\n",
      "[CTX] Loaded embedder: intfloat/multilingual-e5-small\n",
      "== AUGMENTATION (CONTEXT-ONLY, vía recuperador): MACRO ==\n",
      "     cond  context_recall  context_precision  context_noise  compression_ratio  context_tokens\n",
      "RAG-basic             1.0              0.975          0.025              0.326          99.200\n",
      " RAG-full             1.0              0.978          0.022              0.330          90.067\n",
      "\n",
      "== Δ (RAG-full − RAG-basic) ==\n",
      " Δ_context_recall  Δ_context_precision  Δ_context_noise  Δ_compression_ratio  Δ_context_tokens\n",
      "              0.0                0.003           -0.003                0.004            -9.133\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## generacion",
   "id": "6b45a0002f344d6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T19:53:28.880553Z",
     "start_time": "2025-11-13T19:53:15.313115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os, pickle\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---- Rutas (ajústalas o usa variables de entorno) ----\n",
    "PKL_MIN_PATH = os.environ.get(\"PKL_MIN_PATH\", \"embeddings_meta_min.pkl\")\n",
    "FAISS_PATH   = os.environ.get(\"FAISS_PATH\", \"faiss_index_ip.bin\")\n",
    "SCOPUS_CSV   = os.environ.get(\"SCOPUS_CSV\", \"scopusdata.csv\")\n",
    "SCOPUS_SEP   = os.environ.get(\"SCOPUS_SEP\", \"|\")\n",
    "\n",
    "# ---- Config LLM (Ollama + Gemma3) ----\n",
    "OLLAMA_HOST   = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL  = os.environ.get(\"OLLAMA_MODEL\", \"gemma3:4b\")\n",
    "_OLLAMA_CHAT_URL = f\"{OLLAMA_HOST.rstrip('/')}/api/chat\"\n",
    "\n",
    "MAX_CTX_CHARS = int(os.environ.get(\"RAG_MAX_CTX_CHARS\", \"8000\"))\n",
    "\n",
    "# ---- Caches simples ----\n",
    "_model_cache = None\n",
    "_meta_min_cache = None\n",
    "_index_cache = None\n",
    "_scopus_cache = None\n",
    "\n",
    "def load_pkl_and_model(emb_max_seq_len=300):\n",
    "    global _model_cache, _meta_min_cache\n",
    "    if _model_cache is not None and _meta_min_cache is not None:\n",
    "        return _model_cache, _meta_min_cache\n",
    "    with open(PKL_MIN_PATH, \"rb\") as f:\n",
    "        pkl = pickle.load(f)\n",
    "\n",
    "    meta_min = pkl[\"meta_min\"].copy()  # DataFrame: vec_id, chunk_uid, doc_id, chunk_id, (scopus_id), start/end\n",
    "    _meta_min_cache = meta_min\n",
    "\n",
    "    model_name = pkl.get(\"model\", \"intfloat/multilingual-e5-large\")\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.max_seq_length = min(int(emb_max_seq_len), 512)\n",
    "    _model_cache = model\n",
    "\n",
    "    print(f\"[INFO] Modelo: {model_name} | max_seq_length={model.max_seq_length}\")\n",
    "    print(f\"[INFO] meta_min columnas: {list(meta_min.columns)} | filas={len(meta_min)}\")\n",
    "    return _model_cache, _meta_min_cache\n",
    "\n",
    "def load_faiss():\n",
    "    global _index_cache\n",
    "    if _index_cache is None:\n",
    "        _index_cache = faiss.read_index(FAISS_PATH)\n",
    "        print(f\"[INFO] Índice FAISS cargado: ntotal={_index_cache.ntotal}\")\n",
    "    return _index_cache\n",
    "\n",
    "def load_scopus_csv():\n",
    "    global _scopus_cache\n",
    "    if _scopus_cache is None:\n",
    "        df = pd.read_csv(SCOPUS_CSV, sep=SCOPUS_SEP)\n",
    "        if \"scopus_id\" not in df.columns:\n",
    "            raise ValueError(f\"{SCOPUS_CSV} no tiene columna 'scopus_id'\")\n",
    "        df[\"scopus_id\"] = df[\"scopus_id\"].astype(str)\n",
    "        _scopus_cache = df\n",
    "        print(f\"[INFO] scoupusdata.csv: filas={len(df)} | cols={len(df.columns)}\")\n",
    "    return _scopus_cache\n",
    "\n",
    "def e5_encode_query(model, query_text: str):\n",
    "    return model.encode(\n",
    "        [f\"query: {query_text}\"],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "def search_min(query_text: str, topk: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve SOLO el meta mínimo del PKL (sin CSV):\n",
    "    vec_id, score, chunk_uid, doc_id, chunk_id, (scopus_id si existe), start/end\n",
    "    \"\"\"\n",
    "    model, meta_min = load_pkl_and_model()\n",
    "    index = load_faiss()\n",
    "\n",
    "    q = e5_encode_query(model, query_text)\n",
    "    D, I = index.search(q, topk)\n",
    "    vec_ids = I[0].tolist()\n",
    "\n",
    "    hits = meta_min.set_index(\"vec_id\").loc[vec_ids].reset_index()\n",
    "    hits.insert(1, \"score\", D[0])\n",
    "\n",
    "    cols_front = [c for c in [\"vec_id\",\"score\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"scopus_id\",\"start_token\",\"end_token\"] if c in hits.columns]\n",
    "    rest = [c for c in hits.columns if c not in cols_front]\n",
    "    return hits[cols_front + rest].reset_index(drop=True)\n",
    "\n",
    "def search_full_scopus(query_text: str, topk: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Une el TOP-K con TODAS las columnas de scoupusdata.csv por scopus_id.\n",
    "    \"\"\"\n",
    "    model, meta_min = load_pkl_and_model()\n",
    "    index = load_faiss()\n",
    "    sc = load_scopus_csv()\n",
    "\n",
    "    q = e5_encode_query(model, query_text)\n",
    "    D, I = index.search(q, topk)\n",
    "    vec_ids = I[0].tolist()\n",
    "\n",
    "    hits = meta_min.set_index(\"vec_id\").loc[vec_ids].reset_index()\n",
    "    hits.insert(1, \"score\", D[0])\n",
    "\n",
    "    if \"scopus_id\" not in hits.columns:\n",
    "        raise ValueError(\"meta_min en PKL no contiene 'scopus_id'; no puedo unir con el CSV.\")\n",
    "\n",
    "    out = hits.merge(sc, how=\"left\", on=\"scopus_id\")\n",
    "\n",
    "    # Orden: primero claves/score/offsets, luego TODO el CSV\n",
    "    front = [c for c in [\"vec_id\",\"score\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"scopus_id\",\"start_token\",\"end_token\"] if c in out.columns]\n",
    "    csv_cols = [c for c in sc.columns if c not in front]\n",
    "    return out[front + csv_cols].reset_index(drop=True)\n",
    "\n",
    "# ========= RAG BÁSICO SOLO CON EL RECUPERADOR (k=15) =========\n",
    "\n",
    "def build_context_from_hits(hits: pd.DataFrame, max_chars: int = MAX_CTX_CHARS) -> str:\n",
    "    \"\"\"\n",
    "    Construye un contexto concatenando título + abstract (u otras columnas).\n",
    "    Corta a max_chars para no pasar de contexto.\n",
    "    \"\"\"\n",
    "    title_col = \"title\" if \"title\" in hits.columns else None\n",
    "    abs_col   = \"abstract\" if \"abstract\" in hits.columns else None\n",
    "\n",
    "    chunks = []\n",
    "    for _, row in hits.iterrows():\n",
    "        parts = []\n",
    "        if title_col:\n",
    "            parts.append(f\"Título: {row.get(title_col, '')}\")\n",
    "        if abs_col:\n",
    "            parts.append(f\"Resumen: {row.get(abs_col, '')}\")\n",
    "        txt = \"\\n\".join(parts).strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        chunks.append(txt)\n",
    "\n",
    "    full_ctx = \"\\n\\n---\\n\\n\".join(chunks)\n",
    "    if len(full_ctx) > max_chars:\n",
    "        full_ctx = full_ctx[:max_chars]\n",
    "    return full_ctx\n",
    "\n",
    "def answer_with_llm(query_text: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Llama al LLM local (Ollama + Gemma3) usando solo el contexto recuperado.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"Eres un asistente experto en literatura científica. \"\n",
    "        \"Responde a la pregunta del usuario usando EXCLUSIVAMENTE la información del contexto. \"\n",
    "        \"Si el contexto no contiene suficiente evidencia, di explícitamente que no hay información suficiente.\"\n",
    "    )\n",
    "\n",
    "    user_content = f\"\"\"Pregunta del usuario:\n",
    "{query_text}\n",
    "\n",
    "Contexto recuperado (fragmentos de artículos científicos):\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_content},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(_OLLAMA_CHAT_URL, json=payload, timeout=300)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    # Formato típico de Ollama: {\"message\": {\"role\": \"assistant\", \"content\": \"...\"}}\n",
    "    msg = data.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    if not msg:\n",
    "        raise RuntimeError(f\"Respuesta vacía de Ollama: {data}\")\n",
    "    return msg\n",
    "\n",
    "def rag_only_retriever(query_text: str, topk: int = 15) -> dict:\n",
    "    \"\"\"\n",
    "    Pipeline RAG mínimo:\n",
    "    1) Recupera top-k (k=15 por defecto).\n",
    "    2) Construye contexto.\n",
    "    3) Llama al LLM (Gemma3 en Ollama).\n",
    "    Devuelve dict con respuesta + dataframe de hits + contexto.\n",
    "    \"\"\"\n",
    "    hits = search_full_scopus(query_text, topk=topk)\n",
    "    ctx = build_context_from_hits(hits, max_chars=MAX_CTX_CHARS)\n",
    "    answer = answer_with_llm(query_text, ctx)\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"hits\": hits,\n",
    "        \"context\": ctx,\n",
    "    }\n",
    "\n",
    "# ====== DEMO RÁPIDA ======\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"¿Cuántos milímetros  de lluvia cayeron en cada barrio del país en cada día de 2024?\"\n",
    "\n",
    "    print(\"\\n=== RAG solo con recuperador (k=15) ===\")\n",
    "    result = rag_only_retriever(query, topk=15)\n",
    "\n",
    "    print(\"\\n--- RESPUESTA LLM (Gemma3/Ollama) ---\\n\")\n",
    "    print(result[\"answer\"])\n",
    "\n",
    "    # Si quieres revisar los docs usados:\n",
    "    df = result[\"hits\"]\n",
    "    df.to_csv(\"ground.csv\", sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "    print(\"\\nPrimeros 5 artículos recuperados:\")\n",
    "    print(df.head(5)[[\"scopus_id\",\"title\",\"abstract\"]])\n"
   ],
   "id": "5e994343daa9ace0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG solo con recuperador (k=15) ===\n",
      "[INFO] Modelo: intfloat/multilingual-e5-small | max_seq_length=300\n",
      "[INFO] meta_min columnas: ['vec_id', 'chunk_uid', 'doc_id', 'chunk_id', 'start_token', 'end_token', 'scopus_id'] | filas=21005\n",
      "[INFO] Índice FAISS cargado: ntotal=21005\n",
      "[INFO] scoupusdata.csv: filas=19233 | cols=9\n",
      "\n",
      "--- RESPUESTA LLM (Gemma3/Ollama) ---\n",
      "\n",
      "No hay información suficiente en el contexto para responder a la pregunta. El texto describe la creación y funcionamiento de redes de parcelas de largo plazo para estudiar bosques tropicales, pero no proporciona datos sobre la cantidad de lluvia que ha caído en cada barrio del país en cada día de 2024.\n",
      "\n",
      "Primeros 5 artículos recuperados:\n",
      "     scopus_id                                              title  \\\n",
      "0  85106283218  Taking the pulse of Earth's tropical forests u...   \n",
      "1  85114871125  Author Correction: The effect of national prot...   \n",
      "2  85148222225  Water, Sanitation and Socioeconomic Situation ...   \n",
      "3  85139503798  Una nueva especie de Pristimantis (Anura: Stra...   \n",
      "4  85117710376  Spatial variability of diurnal to seasonal cyc...   \n",
      "\n",
      "                                            abstract  \n",
      "0  Tropical forests are the most diverse and prod...  \n",
      "1  In the original version of this Article Rasa Z...  \n",
      "2  The current research was carried out under a d...  \n",
      "3  RESUMEN: En este estudio, basándonos en eviden...  \n",
      "4  Study region: The upper part of the Guayllabam...  \n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T20:22:00.153708Z",
     "start_time": "2025-11-13T20:21:09.822088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, math, pickle, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "from typing import List, Sequence, Optional, Tuple\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# ============================================================\n",
    "#                CONFIG GLOBAL\n",
    "# ============================================================\n",
    "\n",
    "# ---- Rutas (ajústalas o usa variables de entorno) ----\n",
    "PKL_MIN_PATH = os.environ.get(\"PKL_MIN_PATH\", \"embeddings_meta_min.pkl\")\n",
    "FAISS_PATH   = os.environ.get(\"FAISS_PATH\", \"faiss_index_ip.bin\")\n",
    "SCOPUS_CSV   = os.environ.get(\"SCOPUS_CSV\", \"scopusdata.csv\")\n",
    "SCOPUS_SEP   = os.environ.get(\"SCOPUS_SEP\", \"|\")\n",
    "\n",
    "# ---- Config LLM (Ollama + Gemma3) ----\n",
    "OLLAMA_HOST   = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL  = os.environ.get(\"OLLAMA_MODEL\", \"gemma3:4b\")\n",
    "_OLLAMA_CHAT_URL = f\"{OLLAMA_HOST.rstrip('/')}/api/chat\"\n",
    "\n",
    "# ---- Config CrossEncoder (Reranking) ----\n",
    "CROSS_ENCODER_MODEL = os.environ.get(\n",
    "    \"CROSS_ENCODER_MODEL\",\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ")\n",
    "CE_BATCH_SIZE = int(os.environ.get(\"CE_BATCH_SIZE\", \"64\"))\n",
    "W_CE, W_DENSE = float(os.environ.get(\"W_CE\", \"0.7\")), float(os.environ.get(\"W_DENSE\", \"0.3\"))\n",
    "\n",
    "# ---- RAG general ----\n",
    "MAX_CTX_CHARS = int(os.environ.get(\"RAG_MAX_CTX_CHARS\", \"8000\"))\n",
    "\n",
    "# columnas de texto (orden de prioridad)\n",
    "TEXT_COLS = [\"title\", \"abstract\"]  # <- ajustado a tu CSV\n",
    "\n",
    "# ---- Caches simples ----\n",
    "_model_cache = None\n",
    "_meta_min_cache = None\n",
    "_index_cache = None\n",
    "_scopus_cache = None\n",
    "_ce_cache: Optional[CrossEncoder] = None\n",
    "\n",
    "# ============================================================\n",
    "#                LOADERs: E5 + FAISS + SCOPUS\n",
    "# ============================================================\n",
    "\n",
    "def load_pkl_and_model(emb_max_seq_len=300):\n",
    "    global _model_cache, _meta_min_cache\n",
    "    if _model_cache is not None and _meta_min_cache is not None:\n",
    "        return _model_cache, _meta_min_cache\n",
    "\n",
    "    with open(PKL_MIN_PATH, \"rb\") as f:\n",
    "        pkl = pickle.load(f)\n",
    "\n",
    "    meta_min = pkl[\"meta_min\"].copy()\n",
    "    _meta_min_cache = meta_min\n",
    "\n",
    "    model_name = pkl.get(\"model\", \"intfloat/multilingual-e5-large\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    model.max_seq_length = min(int(emb_max_seq_len), 512)\n",
    "    _model_cache = model\n",
    "\n",
    "    print(f\"[INFO] Modelo bi-encoder: {model_name} | max_seq_length={model.max_seq_length} | device={device}\")\n",
    "    print(f\"[INFO] meta_min columnas: {list(meta_min.columns)} | filas={len(meta_min)}\")\n",
    "    return _model_cache, _meta_min_cache\n",
    "\n",
    "def load_faiss():\n",
    "    global _index_cache\n",
    "    if _index_cache is None:\n",
    "        _index_cache = faiss.read_index(FAISS_PATH)\n",
    "        print(f\"[INFO] Índice FAISS cargado: ntotal={_index_cache.ntotal}\")\n",
    "    return _index_cache\n",
    "\n",
    "def load_scopus_csv():\n",
    "    global _scopus_cache\n",
    "    if _scopus_cache is None:\n",
    "        df = pd.read_csv(SCOPUS_CSV, sep=SCOPUS_SEP)\n",
    "        if \"scopus_id\" not in df.columns:\n",
    "            raise ValueError(f\"{SCOPUS_CSV} no tiene columna 'scopus_id'\")\n",
    "        df[\"scopus_id\"] = df[\"scopus_id\"].astype(str)\n",
    "        _scopus_cache = df\n",
    "        print(f\"[INFO] scopusdata.csv: filas={len(df)} | cols={len(df.columns)}\")\n",
    "    return _scopus_cache\n",
    "\n",
    "def e5_encode_query(model, query_text: str):\n",
    "    return model.encode(\n",
    "        [f\"query: {query_text}\"],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "def search_full_scopus(query_text: str, topk: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recuperación densa (FAISS) + join con CSV.\n",
    "    Devuelve topk candidatos con TODAS las columnas de scopusdata.csv.\n",
    "    \"\"\"\n",
    "    model, meta_min = load_pkl_and_model()\n",
    "    index = load_faiss()\n",
    "    sc = load_scopus_csv()\n",
    "\n",
    "    q = e5_encode_query(model, query_text)\n",
    "    D, I = index.search(q, topk)\n",
    "    vec_ids = I[0].tolist()\n",
    "\n",
    "    hits = meta_min.set_index(\"vec_id\").loc[vec_ids].reset_index()\n",
    "    hits.insert(1, \"score\", D[0])\n",
    "\n",
    "    if \"scopus_id\" not in hits.columns:\n",
    "        raise ValueError(\"meta_min en PKL no contiene 'scopus_id'; no puedo unir con el CSV.\")\n",
    "\n",
    "    out = hits.merge(sc, how=\"left\", on=\"scopus_id\")\n",
    "\n",
    "    # Orden: primero claves/score/offsets, luego TODO el CSV\n",
    "    front = [c for c in [\"vec_id\",\"score\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"scopus_id\",\"start_token\",\"end_token\"] if c in out.columns]\n",
    "    csv_cols = [c for c in sc.columns if c not in front]\n",
    "    return out[front + csv_cols].reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "#                RERANKING: CROSS-ENCODER\n",
    "# ============================================================\n",
    "\n",
    "def get_cross_encoder(model_name: str = CROSS_ENCODER_MODEL) -> CrossEncoder:\n",
    "    global _ce_cache\n",
    "    if _ce_cache is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        _ce_cache = CrossEncoder(model_name, device=device)\n",
    "        print(f\"[INFO] Cross-Encoder cargado: {model_name} | device={device}\")\n",
    "    return _ce_cache\n",
    "\n",
    "def _first_nonempty(row: pd.Series, cols: Sequence[str]) -> str:\n",
    "    \"\"\"Devuelve el primer texto no vacío según prioridad en 'cols'.\n",
    "       Si no encuentra, intenta concatenar campos semánticos.\"\"\"\n",
    "    for c in cols:\n",
    "        if c in row and isinstance(row[c], str) and row[c].strip():\n",
    "            return row[c]\n",
    "    parts = []\n",
    "    for c in row.index:\n",
    "        name = c.lower()\n",
    "        if any(tok in name for tok in (\"title\",\"abstract\",\"summary\",\"keywords\",\"chunk\",\"desc\")):\n",
    "            v = row[c]\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                parts.append(v.strip())\n",
    "    return \" \".join(parts)[:4096]\n",
    "\n",
    "def _build_pairs(query_text: str, df_topk: pd.DataFrame, text_cols: Optional[List[str]]) -> Tuple[List[Tuple[str,str]], List[int]]:\n",
    "    cols = text_cols or TEXT_COLS\n",
    "    pairs, idx_map = [], []\n",
    "    for i, row in df_topk.iterrows():\n",
    "        txt = _first_nonempty(row, cols)\n",
    "        pairs.append((query_text, txt if isinstance(txt, str) else \"\"))\n",
    "        idx_map.append(i)\n",
    "    return pairs, idx_map\n",
    "\n",
    "def _minmax(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    mn, mx = float(np.nanmin(x)), float(np.nanmax(x))\n",
    "    if not np.isfinite(mn) or not np.isfinite(mx) or (mx - mn) <= 1e-12:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - mn) / (mx - mn + 1e-12)\n",
    "\n",
    "def rerank_with_cross_encoder(query_text: str,\n",
    "                              df_topk: pd.DataFrame,\n",
    "                              text_cols: Optional[List[str]] = None,\n",
    "                              score_dense_col: str = \"score\",\n",
    "                              fuse_with_dense: bool = True,\n",
    "                              batch_size: int = CE_BATCH_SIZE,\n",
    "                              model_name: str = CROSS_ENCODER_MODEL) -> pd.DataFrame:\n",
    "    \"\"\"Reordena df_topk usando un Cross-Encoder y devuelve un nuevo DataFrame ordenado.\"\"\"\n",
    "    if df_topk is None or len(df_topk) == 0:\n",
    "        raise ValueError(\"df_topk está vacío; ejecuta primero search_full_scopus(query, topk=N).\")\n",
    "\n",
    "    ce = get_cross_encoder(model_name)\n",
    "    pairs, idx_map = _build_pairs(query_text, df_topk, text_cols)\n",
    "\n",
    "    # Predicción por lotes\n",
    "    scores_ce = []\n",
    "    for start in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[start:start+batch_size]\n",
    "        s = ce.predict(batch)\n",
    "        scores_ce.append(np.asarray(s, dtype=np.float32))\n",
    "    scores_ce = np.concatenate(scores_ce, axis=0) if scores_ce else np.zeros(len(df_topk), dtype=np.float32)\n",
    "\n",
    "    out = df_topk.copy()\n",
    "    out.loc[idx_map, \"score_ce\"] = scores_ce\n",
    "    ce_norm = _minmax(out[\"score_ce\"].values)\n",
    "\n",
    "    if fuse_with_dense and score_dense_col in out.columns:\n",
    "        dense_norm = _minmax(out[score_dense_col].values)\n",
    "        out[\"score_dense_norm\"] = dense_norm\n",
    "        out[\"score_final\"] = W_CE * ce_norm + W_DENSE * dense_norm\n",
    "        order_col = \"score_final\"\n",
    "    else:\n",
    "        out[\"score_final\"] = ce_norm\n",
    "        order_col = \"score_final\"\n",
    "\n",
    "    out = out.sort_values(order_col, ascending=False).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def search_full_scopus_reranked(query_text: str,\n",
    "                                topk_dense: int = 100,\n",
    "                                topk_final: int = 15,\n",
    "                                text_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Recupera topk_dense con FAISS.\n",
    "    2) Hace reranking con Cross-Encoder.\n",
    "    3) Devuelve SOLO los topk_final re-rankeados.\n",
    "    \"\"\"\n",
    "    df_topk = search_full_scopus(query_text, topk=topk_dense)\n",
    "    reranked = rerank_with_cross_encoder(\n",
    "        query_text=query_text,\n",
    "        df_topk=df_topk,\n",
    "        text_cols=text_cols,\n",
    "        score_dense_col=\"score\",\n",
    "        fuse_with_dense=True\n",
    "    )\n",
    "    return reranked.head(topk_final).reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "#                CONTEXT BUILDER\n",
    "# ============================================================\n",
    "\n",
    "def build_context_from_hits(hits: pd.DataFrame, max_chars: int = MAX_CTX_CHARS) -> str:\n",
    "    \"\"\"\n",
    "    Construye un contexto concatenando título + abstract (u otras columnas).\n",
    "    Corta a max_chars para no pasarse de contexto.\n",
    "    \"\"\"\n",
    "    title_col = \"title\" if \"title\" in hits.columns else None\n",
    "    abs_col   = \"abstract\" if \"abstract\" in hits.columns else None\n",
    "\n",
    "    chunks = []\n",
    "    for _, row in hits.iterrows():\n",
    "        parts = []\n",
    "        if title_col:\n",
    "            t = str(row.get(title_col, \"\") or \"\").strip()\n",
    "            if t:\n",
    "                parts.append(f\"Título: {t}\")\n",
    "        if abs_col:\n",
    "            a = str(row.get(abs_col, \"\") or \"\").strip()\n",
    "            if a:\n",
    "                parts.append(f\"Resumen: {a}\")\n",
    "        txt = \"\\n\".join(parts).strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        chunks.append(txt)\n",
    "\n",
    "    full_ctx = \"\\n\\n---\\n\\n\".join(chunks)\n",
    "    if len(full_ctx) > max_chars:\n",
    "        full_ctx = full_ctx[:max_chars]\n",
    "    return full_ctx\n",
    "\n",
    "# ============================================================\n",
    "#                LLM LOCAL (GEMMA3 / OLLAMA)\n",
    "# ============================================================\n",
    "\n",
    "def answer_with_llm(query_text: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Llama al LLM local (Ollama + Gemma3) usando solo el contexto recuperado.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"Eres un asistente experto en literatura científica. \"\n",
    "        \"Responde a la pregunta del usuario usando EXCLUSIVAMENTE la información del contexto. \"\n",
    "        \"Si el contexto no contiene suficiente evidencia, di explícitamente que no hay información suficiente.\"\n",
    "    )\n",
    "\n",
    "    user_content = f\"\"\"Pregunta del usuario:\n",
    "{query_text}\n",
    "\n",
    "Contexto recuperado (fragmentos de artículos científicos):\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_content},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(_OLLAMA_CHAT_URL, json=payload, timeout=300)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    msg = data.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    if not msg:\n",
    "        raise RuntimeError(f\"Respuesta vacía de Ollama: {data}\")\n",
    "    return msg\n",
    "\n",
    "# ============================================================\n",
    "#                PIPELINE RAG COMPLETO (RE-RANK + CONTEXTO)\n",
    "# ============================================================\n",
    "\n",
    "def rag_with_rerank(query_text: str,\n",
    "                    topk_dense: int = 100,\n",
    "                    topk_ctx: int = 15,\n",
    "                    text_cols: Optional[List[str]] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Pipeline RAG:\n",
    "    1) Recuperación densa topk_dense.\n",
    "    2) Reranking con CrossEncoder.\n",
    "    3) Selección de topk_ctx para contexto.\n",
    "    4) Construcción de contexto (Context Builder).\n",
    "    5) Llamada a Gemma3 en Ollama.\n",
    "    \"\"\"\n",
    "    hits_reranked = search_full_scopus_reranked(\n",
    "        query_text=query_text,\n",
    "        topk_dense=topk_dense,\n",
    "        topk_final=topk_ctx,\n",
    "        text_cols=text_cols\n",
    "    )\n",
    "    ctx = build_context_from_hits(hits_reranked, max_chars=MAX_CTX_CHARS)\n",
    "    answer = answer_with_llm(query_text, ctx)\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"hits\": hits_reranked,\n",
    "        \"context\": ctx,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "#                DEMO RÁPIDA\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"¿Qué especie de manglar domina en estuarios ecuatorianos y qué servicio ecosistémico crítico se cuantifica ante escenarios de aumento del nivel del mar?\"\n",
    "    result = rag_with_rerank(\n",
    "        query_text=query,\n",
    "        topk_dense=100,  # candidatos que entran al reranker\n",
    "        topk_ctx=15,     # context size final para Gemma3\n",
    "        text_cols=None   # o por ejemplo [\"title\",\"abstract\"] explícito\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== RESPUESTA LLM (Gemma3/Ollama, con reranking) ===\\n\")\n",
    "    print(result[\"answer\"])\n",
    "\n",
    "    # Guardar los docs usados\n",
    "    df = result[\"hits\"]\n",
    "    df.to_csv(\"ground_reranked.csv\", sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "    print(\"\\n[INFO] Primeros 5 artículos re-rankeados:\")\n",
    "    cols_show = [c for c in [\"vec_id\",\"score\",\"score_ce\",\"score_final\",\"scopus_id\",\"title\",\"abstract\"] if c in df.columns]\n",
    "    print(df.head(5)[cols_show])\n"
   ],
   "id": "a7cfbedda56ae431",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modelo bi-encoder: intfloat/multilingual-e5-small | max_seq_length=300 | device=cuda\n",
      "[INFO] meta_min columnas: ['vec_id', 'chunk_uid', 'doc_id', 'chunk_id', 'start_token', 'end_token', 'scopus_id'] | filas=21005\n",
      "[INFO] Índice FAISS cargado: ntotal=21005\n",
      "[INFO] scopusdata.csv: filas=19233 | cols=9\n",
      "[INFO] Cross-Encoder cargado: cross-encoder/ms-marco-MiniLM-L-6-v2 | device=cuda\n",
      "\n",
      "=== RESPUESTA LLM (Gemma3/Ollama, con reranking) ===\n",
      "\n",
      "Basándonos en la información proporcionada, no hay información suficiente para determinar qué especie de manglar domina en los estuarios ecuatorianos ni qué servicio ecosistémico crítico se cuantifica ante escenarios de aumento del nivel del mar.\n",
      "\n",
      "Los resúmenes proporcionan información sobre la biodiversidad de manglares en Ecuador (nuevas especies de Pristimantis), la gestión de áreas protegidas de manglares (AUSCEM), la importancia de los manglares para las comunidades locales y la necesidad de más investigación sobre los servicios ecosistémicos de los manglares, especialmente en relación con el impacto del cambio climático y la gestión de los recursos hídricos. Sin embargo, no hay una descripción específica de la especie dominante en los estuarios ni un enfoque particular en un servicio ecosistémico específico para la proyección de escenarios de aumento del nivel del mar.\n",
      "\n",
      "[INFO] Primeros 5 artículos re-rankeados:\n",
      "   vec_id     score   score_ce  score_final    scopus_id  \\\n",
      "0    6966  0.817981  -8.047054     0.786648  85139503798   \n",
      "1   12244  0.844780 -10.449600     0.501628  85165115887   \n",
      "2    1266  0.827236 -10.476370     0.356411  85111703107   \n",
      "3   11341  0.834713 -11.057104     0.295470  85160716020   \n",
      "4    2726  0.830537 -10.912597     0.292201  85121217104   \n",
      "\n",
      "                                               title  \\\n",
      "0  Una nueva especie de Pristimantis (Anura: Stra...   \n",
      "1  Assessing Management Effectiveness: Manglares ...   \n",
      "2  Community-university partnership in water educ...   \n",
      "3                               Mangroves of Ecuador   \n",
      "4  Priorities of action and research for the prot...   \n",
      "\n",
      "                                            abstract  \n",
      "0  RESUMEN: En este estudio, basándonos en eviden...  \n",
      "1  The Manglares El Salado Fauna Production Reser...  \n",
      "2  Universities have the mission to serve society...  \n",
      "3  Between the 1970s and 1990s, Ecuador lost 27.6...  \n",
      "4  Ecuador belongs to the megadiverse countries o...  \n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
