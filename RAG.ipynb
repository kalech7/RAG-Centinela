{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad318955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import csv\n",
    "import pandas as pd, unicodedata, regex, json\n",
    "from pathlib import Path\n",
    "from ftfy import fix_text\n",
    "from pathlib import Path\n",
    "import pysbd\n",
    "import regex as re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750cfcf",
   "metadata": {},
   "source": [
    "# Ingesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406ac341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura tus credenciales y URL de conexión\n",
    "URI = \"bolt://localhost:7687\" \n",
    "USER = \"neo4j\"\n",
    "PASSWORD = \"password\"\n",
    "\n",
    "# Crear driver\n",
    "driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43a54a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doi', 'author_count', 'publication_date', 'abstract', 'title', 'scopus_id', 'neo4jImportId', 'affiliation_count', 'pk', 'name', 'country', 'city', 'auth_name', 'citation_count', 'initials', 'current_affiliation', 'first_name', 'last_name', 'updated', 'cursor', 'next_url']\n"
     ]
    }
   ],
   "source": [
    "node_properties_query = \"\"\"\n",
    "MATCH (n)\n",
    "UNWIND keys(n) AS prop\n",
    "RETURN DISTINCT prop AS property_name\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(node_properties_query)\n",
    "    columns = [record[\"property_name\"] for record in result]\n",
    "\n",
    "print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df91f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\n    MATCH (a:Article)\\n    WHERE a.scopus_id IS NOT NULL\\n      AND a.title IS NOT NULL AND a.title <> \"\"\\n      AND a.abstract IS NOT NULL AND a.abstract <> \"\"\\n      AND a.doi IS NOT NULL AND a.doi <> \"\"\\n    MATCH (au:Author)-[:WROTE]->(a)\\n    WITH a, collect(DISTINCT au.first_name + \" \" + au.last_name) AS authors\\n    WHERE size(authors) > 0\\n    OPTIONAL MATCH (a)-[:BELONGS_TO]->(af:Affiliation)\\n    WITH a, authors,\\n         collect(DISTINCT af.name)    AS affiliations,\\n         collect(DISTINCT af.city)    AS affiliation_cities,\\n         collect(DISTINCT af.country) AS affiliation_countries\\n    RETURN\\n      a.scopus_id                    AS scopus_id,\\n      a.title                        AS title,\\n      a.abstract                     AS abstract,\\n      a.doi                          AS doi,\\n      authors                        AS authors,\\n      affiliations                   AS affiliations,\\n      affiliation_cities             AS affiliation_cities,\\n      affiliation_countries          AS affiliation_countries,\\n      coalesce(a.citation_count, 0)  AS citation_count\\n    ORDER BY scopus_id\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportación completada: scopusdata.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def export_articles_to_csv():\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Article)\n",
    "    WHERE a.scopus_id IS NOT NULL\n",
    "      AND a.title IS NOT NULL AND a.title <> \"\"\n",
    "      AND a.abstract IS NOT NULL AND a.abstract <> \"\"\n",
    "      AND a.doi IS NOT NULL AND a.doi <> \"\"\n",
    "    MATCH (au:Author)-[:WROTE]->(a)\n",
    "    WITH a, collect(DISTINCT au.first_name + \" \" + au.last_name) AS authors\n",
    "    WHERE size(authors) > 0\n",
    "    OPTIONAL MATCH (a)-[:BELONGS_TO]->(af:Affiliation)\n",
    "    WITH a, authors,\n",
    "         collect(DISTINCT af.name)    AS affiliations,\n",
    "         collect(DISTINCT af.city)    AS affiliation_cities,\n",
    "         collect(DISTINCT af.country) AS affiliation_countries\n",
    "    RETURN\n",
    "      a.scopus_id                    AS scopus_id,\n",
    "      a.title                        AS title,\n",
    "      a.abstract                     AS abstract,\n",
    "      a.doi                          AS doi,\n",
    "      authors                        AS authors,\n",
    "      affiliations                   AS affiliations,\n",
    "      affiliation_cities             AS affiliation_cities,\n",
    "      affiliation_countries          AS affiliation_countries,\n",
    "      coalesce(a.citation_count, 0)  AS citation_count\n",
    "    ORDER BY scopus_id\n",
    "    \"\"\"\n",
    "\n",
    "    # Ejecuta consulta y arma DataFrame\n",
    "    with driver.session() as session:\n",
    "        rows = [dict(r) for r in session.run(query)]\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Une listas con '; ' (evita introducir comas que confundan a quien lo lea a mano)\n",
    "    def join_list(x):\n",
    "        return \"; \".join(str(v) for v in x if v) if isinstance(x, list) else x\n",
    "\n",
    "    for col in [\"authors\", \"affiliations\", \"affiliation_cities\", \"affiliation_countries\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(join_list)\n",
    "\n",
    "    # Orden de columnas\n",
    "    df = df[[\n",
    "        \"title\", \"abstract\", \"doi\", \"authors\",\n",
    "        \"affiliations\", \"affiliation_cities\", \"affiliation_countries\",\n",
    "        \"citation_count\", \"scopus_id\"\n",
    "    ]]\n",
    "\n",
    "    # Exporta con separador '|'\n",
    "    # - quoting=QUOTE_MINIMAL: si algún campo contiene el separador '|', Pandas lo pondrá entre comillas.\n",
    "    # - lineterminator=\"\\n\": EOL consistente.\n",
    "    df.to_csv(\n",
    "        \"scopusdata.csv\",\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "        sep=\"|\",\n",
    "        quoting=csv.QUOTE_MINIMAL,\n",
    "        lineterminator=\"\\n\",\n",
    "    )\n",
    "    print(\"Exportación completada: scopusdata.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_articles_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c781b1",
   "metadata": {},
   "source": [
    "pseudocodigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf779b",
   "metadata": {},
   "source": [
    "# normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788aa3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo → /run/media/alech/backup/Github/tesis/processed.parquet\n",
      "                                          title_norm  \\\n",
      "0  thou shalt not die in this place : an ethnomet...   \n",
      "1  use of learning frames in climate change commu...   \n",
      "2  free access to public ecuadorian universities:...   \n",
      "\n",
      "                                       abstract_norm  \n",
      "0  ecuador, located in south america, has a popul...  \n",
      "1  differences in climate change learning frames ...  \n",
      "2  a free higher education policy was implemented...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- Configuración --------\n",
    "INPUT_CSV   = \"scopusdata.csv\"      # archivo con separador '|'\n",
    "OUTPUT_PATH = \"processed.parquet\"   # salida recomendada (parquet)\n",
    "REMOVE_ISOLATED_NUMBERS = False     # True si quieres quitar números sueltos\n",
    "\n",
    "# -------- Funciones --------\n",
    "def normalize_unicode_and_case(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = fix_text(s)                        # corrige codificación/caracteres raros\n",
    "    s = s.replace(\"\\u00A0\", \" \")           # NBSP -> espacio normal\n",
    "    s = unicodedata.normalize(\"NFC\", s)    # Unicode canónica\n",
    "    s = s.lower()                          # minúsculas\n",
    "    return s\n",
    "\n",
    "def strip_non_informative(s: str, remove_numbers: bool = False) -> str:\n",
    "    # Conserva letras/números/espacios y signos básicos de textos científicos\n",
    "    s = regex.sub(r\"[^\\p{L}\\p{N}\\s\\-\\.,;:()\\[\\]/%]\", \" \", s)\n",
    "    if remove_numbers:\n",
    "        # Elimina números aislados; conserva casos como \"co2\", \"iso-9001\"\n",
    "        s = regex.sub(r\"\\b\\d+\\b\", \" \", s)\n",
    "    s = regex.sub(r\"\\s+\", \" \", s).strip()  # espacios\n",
    "    return s\n",
    "\n",
    "def normalize_text(s: str, remove_numbers: bool = False) -> str:\n",
    "    s = normalize_unicode_and_case(s)\n",
    "    s = strip_non_informative(s, remove_numbers=remove_numbers)\n",
    "    return s\n",
    "\n",
    "def safe_convert_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convierte a dtypes 'seguros' sin depender de opciones globales.\"\"\"\n",
    "    try:\n",
    "        return df.convert_dtypes(dtype_backend=\"numpy_nullable\")  # pandas nuevos\n",
    "    except TypeError:\n",
    "        return df.convert_dtypes()  # pandas más viejos\n",
    "\n",
    "def sanitize_objects(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convierte objetos no escalares (listas/dicts) a JSON string.\"\"\"\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            df[c] = df[c].map(\n",
    "                lambda x: x if isinstance(x, (str, int, float, bool, type(None)))\n",
    "                else json.dumps(x, ensure_ascii=False)\n",
    "            )\n",
    "    return df\n",
    "\n",
    "def try_save_parquet(df: pd.DataFrame, path: str) -> bool:\n",
    "    \"\"\"Intenta guardar con fastparquet, luego pyarrow. Devuelve True si logra parquet.\"\"\"\n",
    "    # 1) fastparquet\n",
    "    try:\n",
    "        import fastparquet  # noqa: F401\n",
    "        df.to_parquet(path, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) pyarrow\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        df.to_parquet(path, index=False, engine=\"pyarrow\")  # compresión por defecto\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# -------- Proceso --------\n",
    "# Lee CSV con separador pipe. Si tu exportación puso comillas cuando había '|',\n",
    "# pandas las respeta automáticamente.\n",
    "df = pd.read_csv(INPUT_CSV, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "# Asegura presencia de columnas requeridas\n",
    "for col in [\"title\", \"abstract\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "# Normalización SOLO sobre texto analizable\n",
    "df[\"title_norm\"]    = df[\"title\"].fillna(\"\").map(lambda x: normalize_text(x, REMOVE_ISOLATED_NUMBERS))\n",
    "df[\"abstract_norm\"] = df[\"abstract\"].fillna(\"\").map(lambda x: normalize_text(x, REMOVE_ISOLATED_NUMBERS))\n",
    "\n",
    "# Guardar salida con metadatos originales + columnas normalizadas\n",
    "cols_out = list(df.columns)\n",
    "for c in [\"title_norm\", \"abstract_norm\"]:\n",
    "    if c not in cols_out:\n",
    "        cols_out.append(c)\n",
    "\n",
    "# Copia de salida + saneo de tipos\n",
    "out = df[cols_out].copy()\n",
    "out = safe_convert_dtypes(out)\n",
    "\n",
    "# Asegura strings planos en columnas de texto clave\n",
    "for c in [\"title\", \"abstract\", \"title_norm\", \"abstract_norm\"]:\n",
    "    if c in out.columns:\n",
    "        out[c] = out[c].astype(str)\n",
    "\n",
    "# Serializa objetos complejos a JSON para evitar fallos de parquet\n",
    "out = sanitize_objects(out)\n",
    "\n",
    "# -------- Guardado robusto --------\n",
    "parquet_ok = try_save_parquet(out, OUTPUT_PATH)\n",
    "\n",
    "if parquet_ok:\n",
    "    print(f\"Listo → {Path(OUTPUT_PATH).resolve()}\")\n",
    "else:\n",
    "    # Fallback a CSV para no perder progreso\n",
    "    fallback = Path(OUTPUT_PATH).with_suffix(\".csv\")\n",
    "    out.to_csv(fallback, index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "    print(\"No se pudo escribir Parquet con fastparquet ni pyarrow. \"\n",
    "          f\"Se guardó CSV en → {fallback.resolve()}\")\n",
    "\n",
    "# Vista rápida\n",
    "print(out[[\"title_norm\", \"abstract_norm\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d8ac3",
   "metadata": {},
   "source": [
    "### deteccion de idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63916c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lang                                      abstract_norm\n",
      "0   en  ecuador, located in south america, has a popul...\n",
      "1   en  differences in climate change learning frames ...\n",
      "2   en  a free higher education policy was implemented...\n",
      "3   en  this study explored the influence of each fami...\n",
      "4   en  the rapid adoption and the diversification of ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "PATH = \"processed.parquet\"  \n",
    "OUT  = \"processed_lbl.parquet\"   \n",
    "\n",
    "DetectorFactory.seed = 0  # resultados más estables\n",
    "\n",
    "def detect_lang_safe(text: str) -> str:\n",
    "    t = (text or \"\").strip()\n",
    "    if not t:\n",
    "        return \"und\"  # indeterminado\n",
    "    try:\n",
    "        return detect(t)\n",
    "    except Exception:\n",
    "        return \"und\"\n",
    "\n",
    "# --- cargar parquet ---\n",
    "# intenta fastparquet y luego pyarrow\n",
    "try:\n",
    "    df = pd.read_parquet(PATH, engine=\"fastparquet\")\n",
    "except Exception:\n",
    "    df = pd.read_parquet(PATH, engine=\"pyarrow\")\n",
    "\n",
    "# --- elegir fuente para detección\n",
    "source_col = \"abstract_norm\" \n",
    "if source_col not in df.columns:\n",
    "    # si no existe ninguna, crea vacía para no romper\n",
    "    df[source_col] = \"\"\n",
    "\n",
    "# --- detectar idioma ---\n",
    "df[\"lang\"] = df[source_col].map(detect_lang_safe)\n",
    "\n",
    "# --- guardar ---\n",
    "try:\n",
    "    df.to_parquet(OUT, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        df.to_parquet(OUT, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        # último recurso: CSV para no perder el trabajo\n",
    "        Path(OUT).with_suffix(\".csv\")\n",
    "        df.to_csv(Path(OUT).with_suffix(\".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(df[[\"lang\", source_col]].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0979f1",
   "metadata": {},
   "source": [
    "## segmentar oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63baabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo:\n",
      "   row_id_original  sentence_idx  \\\n",
      "0                0             0   \n",
      "1                0             1   \n",
      "2                0             2   \n",
      "3                0             3   \n",
      "4                0             4   \n",
      "5                0             5   \n",
      "6                0             6   \n",
      "7                0             7   \n",
      "8                0             8   \n",
      "9                0             9   \n",
      "\n",
      "                                            sentence  \n",
      "0  ecuador, located in south america, has a popul...  \n",
      "1  according to the national institution of stati...  \n",
      "2  palliative care and hospice are relatively new...  \n",
      "3  in ecuador people usually die at home, in hosp...  \n",
      "4  in 2012, the first ecuadorian hospice was crea...  \n",
      "5  according to symbolic interactionism theory, r...  \n",
      "6  symbolic interactionism proposes that human be...  \n",
      "7  through an ethnomethodological approach, the f...  \n",
      "8  results emerge from the introspection of real ...  \n",
      "9  based on a thematic analysis, the following st...  \n",
      "\n",
      "Guardado → /run/media/alech/backup/Github/tesis/processed_sentences.parquet\n"
     ]
    }
   ],
   "source": [
    "# -------- Configuración --------\n",
    "INPUT_PARQUET  = \"processed_lbl.parquet\"      # entrada\n",
    "OUTPUT_PARQUET = \"processed_sentences.parquet\"  # salida\n",
    "SOURCE_COL     = \"abstract_norm\"               # columna a segmentar\n",
    "\n",
    "# -------- Carga robusta --------\n",
    "def read_parquet_any(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_parquet(path, engine=\"fastparquet\")\n",
    "    except Exception:\n",
    "        return pd.read_parquet(path, engine=\"pyarrow\")\n",
    "\n",
    "# -------- Segmentadores (ES / EN) --------\n",
    "seg_es = pysbd.Segmenter(language=\"es\", clean=False)\n",
    "seg_en = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "\n",
    "def split_by_lang(text: str, lang: str = \"es\") -> list[str]:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    lang = (lang or \"es\").lower()\n",
    "    seg = seg_en if lang.startswith(\"en\") else seg_es\n",
    "    try:\n",
    "        return seg.segment(text.strip())\n",
    "    except Exception:\n",
    "        # fallback simple si falla pysbd\n",
    "        return [text.strip()]\n",
    "\n",
    "# -------- Proceso --------\n",
    "df = read_parquet_any(INPUT_PARQUET)\n",
    "\n",
    "if SOURCE_COL not in df.columns:\n",
    "    df[SOURCE_COL] = \"\"\n",
    "\n",
    "# si tienes columna de idioma, úsala; si no, asume \"es\"\n",
    "lang_series = df[\"lang\"] if \"lang\" in df.columns else [\"es\"] * len(df)\n",
    "\n",
    "# segmentar\n",
    "df[\"sentences\"] = [\n",
    "    split_by_lang(text, lang)\n",
    "    for text, lang in zip(df[SOURCE_COL], lang_series)\n",
    "]\n",
    "\n",
    "# una oración por fila\n",
    "out = df.explode(\"sentences\", ignore_index=False)\n",
    "out = out.rename(columns={\"sentences\": \"sentence\"})\n",
    "out = out.reset_index(names=\"row_id_original\")\n",
    "out[\"sentence_idx\"] = out.groupby(\"row_id_original\").cumcount()\n",
    "\n",
    "# columnas finales\n",
    "keep = []\n",
    "for c in [\"scopus_id\", \"title\", \"abstract\", \"abstract_norm\", \"lang\"]:\n",
    "    if c in out.columns:\n",
    "        keep.append(c)\n",
    "keep += [\"row_id_original\", \"sentence_idx\", \"sentence\"]\n",
    "out = out[keep]\n",
    "\n",
    "# -------- Guardar --------\n",
    "try:\n",
    "    out.to_parquet(OUTPUT_PARQUET, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        out.to_parquet(OUTPUT_PARQUET, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        out.to_csv(Path(OUTPUT_PARQUET).with_suffix(\".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Ejemplo:\")\n",
    "print(out[[\"row_id_original\", \"sentence_idx\", \"sentence\"]].head(10))\n",
    "print(f\"\\nGuardado → {Path(OUTPUT_PARQUET).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbe64b",
   "metadata": {},
   "source": [
    "## tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae16b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0  ecuador, located in south america, has a popul...   \n",
      "1  according to the national institution of stati...   \n",
      "2  palliative care and hospice are relatively new...   \n",
      "3  in ecuador people usually die at home, in hosp...   \n",
      "4  in 2012, the first ecuadorian hospice was crea...   \n",
      "5  according to symbolic interactionism theory, r...   \n",
      "6  symbolic interactionism proposes that human be...   \n",
      "7  through an ethnomethodological approach, the f...   \n",
      "\n",
      "                                          tokens_csv  \n",
      "0  ecuador,located,in south america,has,a,populat...  \n",
      "1  according to the,national,institution,of,stati...  \n",
      "2  palliative care,and,hospice,are,relatively new...  \n",
      "3  in,ecuador,people,usually,die,at home,in,hospi...  \n",
      "4   in,2012,the,first,ecuadorian,hospice,was created  \n",
      "5  according to,symbolic,interactionism,theory,re...  \n",
      "6  symbolic,interactionism,proposes,that,human be...  \n",
      "7  through,an,ethnomethodological,approach,the,fo...  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "INPUT_PARQUET  = \"processed_sentences.parquet\"\n",
    "OUTPUT_PARQUET = \"corpus_token\"\n",
    "SENT_COL = \"sentence\"\n",
    "\n",
    "TOKEN_RE = re.compile(r\"(?:[^\\W_]+(?:[-_][^\\W_]+)+|\\d+(?:\\.\\d+)+|[^\\W_]+)\", re.VERBOSE | re.IGNORECASE | re.UNICODE)\n",
    "def simple_tokenize(s:str):\n",
    "    if not isinstance(s, str): return []\n",
    "    return TOKEN_RE.findall(re.sub(r\"\\s+\", \" \", s.strip()))\n",
    "\n",
    "# 1) Carga y tokeniza todas las oraciones\n",
    "def read_parquet_any(p):\n",
    "    try: return pd.read_parquet(p, engine=\"fastparquet\")\n",
    "    except Exception: return pd.read_parquet(p, engine=\"pyarrow\")\n",
    "\n",
    "df = read_parquet_any(INPUT_PARQUET)\n",
    "df[\"tokens_base\"] = df[SENT_COL].map(simple_tokenize)\n",
    "\n",
    "# 2) Entrena bigramas y (opcional) trigramas\n",
    "sentences = df[\"tokens_base\"].tolist()\n",
    "\n",
    "# ⚠️ delimiter must be str if tokens are str\n",
    "bigram = Phrases(sentences, min_count=5, threshold=10.0, delimiter=\" \")\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "trigram = Phrases(bigram_phraser[sentences], min_count=5, threshold=10.0, delimiter=\" \")\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# 3) Aplica: pega frases automáticamente (p.ej., aprendizaje_automático)\n",
    "df[\"tokens\"] = [trigram_phraser[bigram_phraser[toks]] for toks in df[\"tokens_base\"]]\n",
    "df[\"tokens_csv\"] = df[\"tokens\"].map(lambda xs: \",\".join(xs))\n",
    "df[\"n_tokens\"] = df[\"tokens\"].map(len)\n",
    "\n",
    "# 4) Guarda\n",
    "try:\n",
    "    df.to_parquet(OUTPUT_PARQUET, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try: df.to_parquet(OUTPUT_PARQUET, index=False, engine=\"pyarrow\")\n",
    "    except Exception: df.to_csv(Path(OUTPUT_PARQUET).with_suffix(\".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(df[[SENT_COL, \"tokens_csv\"]].head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0786c93",
   "metadata": {},
   "source": [
    "### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f7801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0  ecuador, located in south america, has a popul...   \n",
      "1  according to the national institution of stati...   \n",
      "2  palliative care and hospice are relatively new...   \n",
      "3  in ecuador people usually die at home, in hosp...   \n",
      "4  in 2012, the first ecuadorian hospice was crea...   \n",
      "5  according to symbolic interactionism theory, r...   \n",
      "6  symbolic interactionism proposes that human be...   \n",
      "7  through an ethnomethodological approach, the f...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [ecuador, located, in south america, has, a, p...   \n",
      "1  [according to the, national, institution, of, ...   \n",
      "2  [palliative care, and, hospice, are, relativel...   \n",
      "3  [in, ecuador, people, usually, die, at home, i...   \n",
      "4  [in, 2012, the, first, ecuadorian, hospice, wa...   \n",
      "5  [according to, symbolic, interactionism, theor...   \n",
      "6  [symbolic, interactionism, proposes, that, hum...   \n",
      "7  [through, an, ethnomethodological, approach, t...   \n",
      "\n",
      "                                       tokens_nostop  \\\n",
      "0  [ecuador, located, in south america, populatio...   \n",
      "1  [according to the, national, institution, stat...   \n",
      "2  [palliative care, hospice, relatively new, con...   \n",
      "3  [ecuador, people, usually, die, at home, hospi...   \n",
      "4          [first, ecuadorian, hospice, was created]   \n",
      "5  [according to, symbolic, interactionism, theor...   \n",
      "6  [symbolic, interactionism, proposes, human bei...   \n",
      "7  [ethnomethodological, approach, following, res...   \n",
      "\n",
      "                                      text_for_embed  \n",
      "0  ecuador located in south america population mi...  \n",
      "1  according to the national institution statisti...  \n",
      "2  palliative care hospice relatively new concept...  \n",
      "3  ecuador people usually die at home hospitals n...  \n",
      "4               first ecuadorian hospice was created  \n",
      "5  according to symbolic interactionism theory re...  \n",
      "6  symbolic interactionism proposes human beings ...  \n",
      "7  ethnomethodological approach following researc...  \n",
      "Guardado en: corpus_token_nostop.parquet\n"
     ]
    }
   ],
   "source": [
    "# === Quitar stopwords sobre df[\"tokens\"] con n-gramas separados por espacio ===\n",
    "import re, unicodedata, os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stop\n",
    "\n",
    "# Asegura recurso stopwords NLTK\n",
    "try:\n",
    "    _ = nltk_stop.words(\"spanish\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "# Idiomas a filtrar (ajusta a [\"spanish\"] si quieres solo ES)\n",
    "LANGS = [\"spanish\", \"english\"]\n",
    "\n",
    "# Construye set de stopwords\n",
    "STOPSET = set()\n",
    "for lang in LANGS:\n",
    "    try:\n",
    "        STOPSET |= set(nltk_stop.words(lang))\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "STOPSET_NORM = {_norm(w) for w in STOPSET}\n",
    "\n",
    "# Frases que no se filtran nunca (escribe aquí con ESPACIOS)\n",
    "PROTECT_PHRASES = {\n",
    "    \"in south america\",\n",
    "    # añade más si quieres: \"public health\", \"quality of life\", ...\n",
    "}\n",
    "\n",
    "def is_stop(tok: str) -> bool:\n",
    "    \"\"\"\n",
    "    Mantén n-gramas con contenido: elimina solo si TODAS las partes\n",
    "    (separadas por espacio o guion) son stopwords; protege frases explícitas.\n",
    "    \"\"\"\n",
    "    if not isinstance(tok, str) or not tok:\n",
    "        return True  # vacío o no-string -> descartar\n",
    "\n",
    "    t = _norm(tok).strip()\n",
    "\n",
    "    # Protección explícita\n",
    "    if t in PROTECT_PHRASES:\n",
    "        return False\n",
    "\n",
    "    # Token simple (sin espacios ni guiones)\n",
    "    if (\" \" not in t) and (\"-\" not in t):\n",
    "        return t in STOPSET_NORM\n",
    "\n",
    "    # Token compuesto: separa por espacios o guiones (uno o más)\n",
    "    parts = [p for p in re.split(r\"[ \\-]+\", t) if p]\n",
    "    if not parts:\n",
    "        return True\n",
    "\n",
    "    # Elimina SOLO si *todas* las partes son stopwords\n",
    "    return all(p in STOPSET_NORM for p in parts)\n",
    "\n",
    "def filter_tokens(tokens, min_len=2, drop_numeric=True):\n",
    "    out = []\n",
    "    if not isinstance(tokens, (list, tuple)):\n",
    "        return out\n",
    "    for t in tokens:\n",
    "        if not isinstance(t, str) or not t:\n",
    "            continue\n",
    "        if drop_numeric and t.isnumeric():\n",
    "            continue\n",
    "        if len(t) < min_len:\n",
    "            continue\n",
    "        if is_stop(t):\n",
    "            continue\n",
    "        out.append(t)\n",
    "    return out\n",
    "\n",
    "# --- Aplicar al DF (requiere df[\"tokens\"] como lista de strings) ---\n",
    "if \"tokens\" not in df.columns:\n",
    "    raise KeyError(\"Se esperaba df['tokens']. Asegúrate de haber generado los n-gramas antes.\")\n",
    "\n",
    "df[\"tokens_nostop\"] = df[\"tokens\"].map(filter_tokens)\n",
    "\n",
    "# (Opcional) Texto para embeddings (bi-encoder): tokens unidos por espacio\n",
    "df[\"text_for_embed\"] = df[\"tokens_nostop\"].map(lambda xs: \" \".join(xs))\n",
    "\n",
    "# Vistazo rápido (muestra si existen)\n",
    "cols_show = [c for c in [\"sentence\", \"tokens\", \"tokens_nostop\", \"text_for_embed\"] if c in df.columns]\n",
    "print(df[cols_show].head(8))\n",
    "\n",
    "# --- Guardar a nuevo archivo para no sobrescribir el original ---\n",
    "OUT_BASE = \"corpus_token_nostop\"\n",
    "parquet_path = f\"{OUT_BASE}.parquet\"\n",
    "try:\n",
    "    df.to_parquet(parquet_path, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        df.to_parquet(parquet_path, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        df.to_csv(f\"{OUT_BASE}.csv\", index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Guardado en:\",\n",
    "      parquet_path if os.path.exists(parquet_path) else f\"{OUT_BASE}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263b065",
   "metadata": {},
   "source": [
    "## lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef675ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      text_for_embed  \\\n",
      "0  ecuador located in south america population mi...   \n",
      "1  according to the national institution statisti...   \n",
      "2  palliative care hospice relatively new concept...   \n",
      "3  ecuador people usually die at home hospitals n...   \n",
      "4               first ecuadorian hospice was created   \n",
      "5  according to symbolic interactionism theory re...   \n",
      "6  symbolic interactionism proposes human beings ...   \n",
      "7  ethnomethodological approach following researc...   \n",
      "\n",
      "                                          text_lemma  \n",
      "0  ecuador locate in south america population mil...  \n",
      "1  accord to the national institution statistic e...  \n",
      "2  palliative care hospice relatively new concept...  \n",
      "3  ecuador people usually die at home hospital nu...  \n",
      "4                 first ecuadorian hospice be create  \n",
      "5  accord to symbolic interactionism theory resea...  \n",
      "6  symbolic interactionism propose human being ca...  \n",
      "7  ethnomethodological approach follow research a...  \n",
      "Guardado en: corpus_token_nostop_lemma.parquet\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZE text_for_embed (ES/EN) con spaCy en batch\n",
    "import re, unicodedata, os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"corpus_token_nostop.parquet\")\n",
    "\n",
    "assert \"text_for_embed\" in df.columns, \"Falta la columna 'text_for_embed'.\"\n",
    "\n",
    "# Carga modelos spaCy (puedes usar *_md/_lg si los tienes)\n",
    "nlp_es = spacy.load(\"es_core_news_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "nlp_en = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "\n",
    "# Heurística ligera para detectar español\n",
    "SPANISH_CUES = {\"de\",\"la\",\"el\",\"los\",\"las\",\"y\",\"en\",\"para\",\"con\",\"por\",\"del\",\"al\",\"un\",\"una\",\"unos\",\"unas\",\"se\",\"su\",\"sus\"}\n",
    "ACCENTS_RE = re.compile(r\"[áéíóúñüÁÉÍÓÚÑÜ]\")\n",
    "\n",
    "def _is_spanish_like(text: str) -> bool:\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    if ACCENTS_RE.search(text):\n",
    "        return True\n",
    "    words = [w.lower() for w in re.split(r\"\\s+\", text.strip()) if w]\n",
    "    # si al menos una palabra típica española aparece -> ES\n",
    "    return any(w in SPANISH_CUES for w in words)\n",
    "\n",
    "def _lemma_doc(doc):\n",
    "    # cuida pronombres (algunos modelos antiguos devuelven \"-PRON-\")\n",
    "    toks = []\n",
    "    for t in doc:\n",
    "        lem = t.lemma_ if t.lemma_ and t.lemma_ != \"-PRON-\" else t.text\n",
    "        toks.append(lem.lower())\n",
    "    # une con espacios (mismo formato que text_for_embed)\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# Divide índices por idioma con la heurística\n",
    "idx_es = df.index[df[\"text_for_embed\"].apply(_is_spanish_like)].tolist()\n",
    "idx_en = df.index.difference(idx_es).tolist()\n",
    "\n",
    "# Lematiza en batch por idioma (más rápido que fila a fila)\n",
    "text_lemma = pd.Series(index=df.index, dtype=object)\n",
    "\n",
    "if idx_es:\n",
    "    docs_es = nlp_es.pipe(df.loc[idx_es, \"text_for_embed\"].fillna(\"\"), batch_size=512, n_process=1)\n",
    "    for i, doc in zip(idx_es, docs_es):\n",
    "        text_lemma.loc[i] = _lemma_doc(doc)\n",
    "\n",
    "if idx_en:\n",
    "    docs_en = nlp_en.pipe(df.loc[idx_en, \"text_for_embed\"].fillna(\"\"), batch_size=512, n_process=1)\n",
    "    for i, doc in zip(idx_en, docs_en):\n",
    "        text_lemma.loc[i] = _lemma_doc(doc)\n",
    "\n",
    "# Asigna columna nueva\n",
    "df[\"text_lemma\"] = text_lemma.fillna(\"\")\n",
    "\n",
    "# Vista rápida\n",
    "print(df[[\"text_for_embed\", \"text_lemma\"]].head(8))\n",
    "\n",
    "# Guardar (nuevo archivo para no pisar el anterior)\n",
    "OUT = \"corpus_token_nostop_lemma.parquet\"\n",
    "try:\n",
    "    df.to_parquet(OUT, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        df.to_parquet(OUT, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        df.to_csv(\"corpus_token_nostop_lemma.csv\", index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Guardado en:\", OUT if os.path.exists(OUT) else \"corpus_token_nostop_lemma.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788917fb",
   "metadata": {},
   "source": [
    "## chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07593bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc_id  chunk_id chunk_uid    scopus_id  start_token  end_token  \\\n",
      "0       0         0       0-0  85059061481            0        156   \n",
      "1       1         0       1-0  85061967853            0         94   \n",
      "2       2         0       2-0  85067792389            0         60   \n",
      "3       3         0       3-0  85068192726            0        255   \n",
      "4       4         0       4-0  85069901345            0        173   \n",
      "5       5         0       5-0  85070472925            0        205   \n",
      "6       6         0       6-0  85071977997            0        195   \n",
      "7       7         0       7-0  85072017885            0        169   \n",
      "\n",
      "   token_count                                         text_chunk  \n",
      "0          156  ecuador locate in south america population mil...  \n",
      "1           94  difference climate change learning frame pedag...  \n",
      "2           60  free high education policy be implement ecuado...  \n",
      "3          255  this study explore influence family member lif...  \n",
      "4          173  rapid adoption diversification cloud compute t...  \n",
      "5          205  traditional pretest prove homoscedasticity e g...  \n",
      "6          195  polymer electrolyte fuel cell pefc produce ele...  \n",
      "7          169  traffic prediction high accuracy have become v...  \n",
      "N docs (únicos scopus_id): 19233 | N chunks: 21005\n",
      "Guardado en: corpus_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "# ====== CHUNKING *SIEMPRE* DESDE text_lemma (agrupado por scopus_id) ======\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "IN_PARQUET  = os.environ.get(\"IN_PARQUET\", \"corpus_token_nostop_lemma.parquet\")\n",
    "OUT_CHUNKS  = os.environ.get(\"OUT_CHUNKS\", \"corpus_chunks.parquet\")\n",
    "\n",
    "# --- Parámetros de chunking ---\n",
    "MAX_TOKENS      = int(os.environ.get(\"MAX_TOKENS\", \"300\"))      # 200–400 recomendado\n",
    "OVERLAP_RATIO   = float(os.environ.get(\"OVERLAP_RATIO\", \"0.2\"))  # 15–30% recomendado\n",
    "OVERLAP_TOKENS  = int(MAX_TOKENS * OVERLAP_RATIO)\n",
    "STRIDE          = max(1, MAX_TOKENS - OVERLAP_TOKENS)\n",
    "\n",
    "# Tokenizer E5 (coherente con embeddings e5*)\n",
    "TOKENIZER_NAME = \"intfloat/multilingual-e5-base\"\n",
    "tok = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "# ---------- 0) Carga y validación ----------\n",
    "df = pd.read_parquet(IN_PARQUET)\n",
    "\n",
    "assert \"scopus_id\" in df.columns, \"Falta columna 'scopus_id' en el parquet.\"\n",
    "assert \"text_lemma\" in df.columns, \"Falta columna 'text_lemma' (se usa siempre).\"\n",
    "\n",
    "# Normaliza tipos / limpieza básica\n",
    "df[\"scopus_id\"]  = df[\"scopus_id\"].astype(str)\n",
    "df[\"text_lemma\"] = (\n",
    "    df[\"text_lemma\"].fillna(\"\").astype(str)\n",
    "      .str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    ")\n",
    "\n",
    "# ---------- 1) Construir texto lematizado por documento ----------\n",
    "# Orden preferido: sentence_idx > row_id_original > orden actual\n",
    "sort_keys = [\"scopus_id\"]\n",
    "if \"sentence_idx\" in df.columns:\n",
    "    sort_keys += [\"sentence_idx\"]\n",
    "elif \"row_id_original\" in df.columns:\n",
    "    sort_keys += [\"row_id_original\"]\n",
    "\n",
    "df = df.sort_values(sort_keys, kind=\"mergesort\")\n",
    "\n",
    "# Texto lematizado consolidado por scopus_id\n",
    "agg_text = df.groupby(\"scopus_id\")[\"text_lemma\"].apply(\n",
    "    lambda s: \" \".join([t for t in s.astype(str) if t])\n",
    ").rename(\"text_for_chunk\")\n",
    "\n",
    "# Solo scopus_id + texto consolidado\n",
    "doc_df = agg_text.to_frame().reset_index()\n",
    "doc_df[\"doc_id\"] = np.arange(len(doc_df), dtype=\"int64\")\n",
    "\n",
    "# Limpieza final del texto\n",
    "doc_df[\"text_for_chunk\"] = (\n",
    "    doc_df[\"text_for_chunk\"].fillna(\"\").astype(str)\n",
    "       .str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    ")\n",
    "doc_df = doc_df[doc_df[\"text_for_chunk\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# ---------- 2) Chunker por tokens ----------\n",
    "def chunk_text_by_tokens(text: str, max_tokens: int = MAX_TOKENS, stride: int = STRIDE):\n",
    "    ids = tok.encode(text, add_special_tokens=False)\n",
    "    n = len(ids)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = min(start + max_tokens, n)\n",
    "        sl = ids[start:end]\n",
    "        chunk_txt = tok.decode(sl, skip_special_tokens=True).strip()\n",
    "        if chunk_txt:\n",
    "            chunks.append({\n",
    "                \"start_token\": start,\n",
    "                \"end_token\": end,\n",
    "                \"token_count\": end - start,\n",
    "                \"text_chunk\": chunk_txt\n",
    "            })\n",
    "        if end == n:\n",
    "            break\n",
    "        start += stride\n",
    "    return chunks\n",
    "\n",
    "# ---------- 3) Generar filas de chunks (solo campos mínimos + scopus_id) ----------\n",
    "rows = []\n",
    "for _, r in doc_df.iterrows():\n",
    "    doc_id = int(r[\"doc_id\"])\n",
    "    scid   = str(r[\"scopus_id\"])\n",
    "    text   = r[\"text_for_chunk\"]\n",
    "    for j, ch in enumerate(chunk_text_by_tokens(text, MAX_TOKENS, STRIDE)):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": j,\n",
    "            \"chunk_uid\": f\"{doc_id}-{j}\",\n",
    "            \"scopus_id\": scid,\n",
    "            \"start_token\": ch[\"start_token\"],\n",
    "            \"end_token\": ch[\"end_token\"],\n",
    "            \"token_count\": ch[\"token_count\"],\n",
    "            \"text_chunk\": ch[\"text_chunk\"],\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(rows)\n",
    "\n",
    "print(chunks_df.head(8))\n",
    "print(\"N docs (únicos scopus_id):\", doc_df.shape[0], \"| N chunks:\", chunks_df.shape[0])\n",
    "\n",
    "# ---------- 4) Guardar (solo mínimos) ----------\n",
    "save_cols = [\n",
    "    \"doc_id\",\"chunk_id\",\"chunk_uid\",\"scopus_id\",\n",
    "    \"start_token\",\"end_token\",\"token_count\",\"text_chunk\"\n",
    "]\n",
    "chunks_out = chunks_df[save_cols].copy()\n",
    "\n",
    "# Parquet (pyarrow preferente)\n",
    "try:\n",
    "    chunks_out.to_parquet(OUT_CHUNKS, index=False, engine=\"pyarrow\")\n",
    "except Exception:\n",
    "    try:\n",
    "        chunks_out.to_parquet(OUT_CHUNKS, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "    except Exception:\n",
    "        chunks_out.to_csv(OUT_CHUNKS.replace(\".parquet\", \".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Guardado en:\", OUT_CHUNKS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afbb74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device encode: cpu; model=intfloat/multilingual-e5-small\n",
      "[INFO] model.max_seq_length = 300\n",
      "[INFO] FAISS-CPU\n",
      "[PROG] 32/21005 (0.2%) - bs=32\n",
      "[PROG] 64/21005 (0.3%) - bs=32\n",
      "[PROG] 96/21005 (0.5%) - bs=32\n",
      "[PROG] 128/21005 (0.6%) - bs=32\n",
      "[PROG] 160/21005 (0.8%) - bs=32\n",
      "[PROG] 192/21005 (0.9%) - bs=32\n",
      "[PROG] 224/21005 (1.1%) - bs=32\n",
      "[PROG] 256/21005 (1.2%) - bs=32\n",
      "[PROG] 288/21005 (1.4%) - bs=32\n",
      "[PROG] 320/21005 (1.5%) - bs=32\n",
      "[PROG] 352/21005 (1.7%) - bs=32\n",
      "[PROG] 384/21005 (1.8%) - bs=32\n",
      "[PROG] 416/21005 (2.0%) - bs=32\n",
      "[PROG] 448/21005 (2.1%) - bs=32\n",
      "[PROG] 480/21005 (2.3%) - bs=32\n",
      "[PROG] 512/21005 (2.4%) - bs=32\n",
      "[PROG] 544/21005 (2.6%) - bs=32\n",
      "[PROG] 576/21005 (2.7%) - bs=32\n",
      "[PROG] 608/21005 (2.9%) - bs=32\n",
      "[PROG] 640/21005 (3.0%) - bs=32\n",
      "[PROG] 672/21005 (3.2%) - bs=32\n",
      "[PROG] 704/21005 (3.4%) - bs=32\n",
      "[PROG] 736/21005 (3.5%) - bs=32\n",
      "[PROG] 768/21005 (3.7%) - bs=32\n",
      "[PROG] 800/21005 (3.8%) - bs=32\n",
      "[PROG] 832/21005 (4.0%) - bs=32\n",
      "[PROG] 864/21005 (4.1%) - bs=32\n",
      "[PROG] 896/21005 (4.3%) - bs=32\n",
      "[PROG] 928/21005 (4.4%) - bs=32\n",
      "[PROG] 960/21005 (4.6%) - bs=32\n",
      "[PROG] 992/21005 (4.7%) - bs=32\n",
      "[PROG] 1024/21005 (4.9%) - bs=32\n",
      "[PROG] 1056/21005 (5.0%) - bs=32\n",
      "[PROG] 1088/21005 (5.2%) - bs=32\n",
      "[PROG] 1120/21005 (5.3%) - bs=32\n",
      "[PROG] 1152/21005 (5.5%) - bs=32\n",
      "[PROG] 1184/21005 (5.6%) - bs=32\n",
      "[PROG] 1216/21005 (5.8%) - bs=32\n",
      "[PROG] 1248/21005 (5.9%) - bs=32\n",
      "[PROG] 1280/21005 (6.1%) - bs=32\n",
      "[PROG] 1312/21005 (6.2%) - bs=32\n",
      "[PROG] 1344/21005 (6.4%) - bs=32\n",
      "[PROG] 1376/21005 (6.6%) - bs=32\n",
      "[PROG] 1408/21005 (6.7%) - bs=32\n",
      "[PROG] 1440/21005 (6.9%) - bs=32\n",
      "[PROG] 1472/21005 (7.0%) - bs=32\n",
      "[PROG] 1504/21005 (7.2%) - bs=32\n",
      "[PROG] 1536/21005 (7.3%) - bs=32\n",
      "[PROG] 1568/21005 (7.5%) - bs=32\n",
      "[PROG] 1600/21005 (7.6%) - bs=32\n",
      "[PROG] 1632/21005 (7.8%) - bs=32\n",
      "[PROG] 1664/21005 (7.9%) - bs=32\n",
      "[PROG] 1696/21005 (8.1%) - bs=32\n",
      "[PROG] 1728/21005 (8.2%) - bs=32\n",
      "[PROG] 1760/21005 (8.4%) - bs=32\n",
      "[PROG] 1792/21005 (8.5%) - bs=32\n",
      "[PROG] 1824/21005 (8.7%) - bs=32\n",
      "[PROG] 1856/21005 (8.8%) - bs=32\n",
      "[PROG] 1888/21005 (9.0%) - bs=32\n",
      "[PROG] 1920/21005 (9.1%) - bs=32\n",
      "[PROG] 1952/21005 (9.3%) - bs=32\n",
      "[PROG] 1984/21005 (9.4%) - bs=32\n",
      "[PROG] 2016/21005 (9.6%) - bs=32\n",
      "[PROG] 2048/21005 (9.8%) - bs=32\n",
      "[PROG] 2080/21005 (9.9%) - bs=32\n",
      "[PROG] 2112/21005 (10.1%) - bs=32\n",
      "[PROG] 2144/21005 (10.2%) - bs=32\n",
      "[PROG] 2176/21005 (10.4%) - bs=32\n",
      "[PROG] 2208/21005 (10.5%) - bs=32\n",
      "[PROG] 2240/21005 (10.7%) - bs=32\n",
      "[PROG] 2272/21005 (10.8%) - bs=32\n",
      "[PROG] 2304/21005 (11.0%) - bs=32\n",
      "[PROG] 2336/21005 (11.1%) - bs=32\n",
      "[PROG] 2368/21005 (11.3%) - bs=32\n",
      "[PROG] 2400/21005 (11.4%) - bs=32\n",
      "[PROG] 2432/21005 (11.6%) - bs=32\n",
      "[PROG] 2464/21005 (11.7%) - bs=32\n",
      "[PROG] 2496/21005 (11.9%) - bs=32\n",
      "[PROG] 2528/21005 (12.0%) - bs=32\n",
      "[PROG] 2560/21005 (12.2%) - bs=32\n",
      "[PROG] 2592/21005 (12.3%) - bs=32\n",
      "[PROG] 2624/21005 (12.5%) - bs=32\n",
      "[PROG] 2656/21005 (12.6%) - bs=32\n",
      "[PROG] 2688/21005 (12.8%) - bs=32\n",
      "[PROG] 2720/21005 (12.9%) - bs=32\n",
      "[PROG] 2752/21005 (13.1%) - bs=32\n",
      "[PROG] 2784/21005 (13.3%) - bs=32\n",
      "[PROG] 2816/21005 (13.4%) - bs=32\n",
      "[PROG] 2848/21005 (13.6%) - bs=32\n",
      "[PROG] 2880/21005 (13.7%) - bs=32\n",
      "[PROG] 2912/21005 (13.9%) - bs=32\n",
      "[PROG] 2944/21005 (14.0%) - bs=32\n",
      "[PROG] 2976/21005 (14.2%) - bs=32\n",
      "[PROG] 3008/21005 (14.3%) - bs=32\n",
      "[PROG] 3040/21005 (14.5%) - bs=32\n",
      "[PROG] 3072/21005 (14.6%) - bs=32\n",
      "[PROG] 3104/21005 (14.8%) - bs=32\n",
      "[PROG] 3136/21005 (14.9%) - bs=32\n",
      "[PROG] 3168/21005 (15.1%) - bs=32\n",
      "[PROG] 3200/21005 (15.2%) - bs=32\n",
      "[PROG] 3232/21005 (15.4%) - bs=32\n",
      "[PROG] 3264/21005 (15.5%) - bs=32\n",
      "[PROG] 3296/21005 (15.7%) - bs=32\n",
      "[PROG] 3328/21005 (15.8%) - bs=32\n",
      "[PROG] 3360/21005 (16.0%) - bs=32\n",
      "[PROG] 3392/21005 (16.1%) - bs=32\n",
      "[PROG] 3424/21005 (16.3%) - bs=32\n",
      "[PROG] 3456/21005 (16.5%) - bs=32\n",
      "[PROG] 3488/21005 (16.6%) - bs=32\n",
      "[PROG] 3520/21005 (16.8%) - bs=32\n",
      "[PROG] 3552/21005 (16.9%) - bs=32\n",
      "[PROG] 3584/21005 (17.1%) - bs=32\n",
      "[PROG] 3616/21005 (17.2%) - bs=32\n",
      "[PROG] 3648/21005 (17.4%) - bs=32\n",
      "[PROG] 3680/21005 (17.5%) - bs=32\n",
      "[PROG] 3712/21005 (17.7%) - bs=32\n",
      "[PROG] 3744/21005 (17.8%) - bs=32\n",
      "[PROG] 3776/21005 (18.0%) - bs=32\n",
      "[PROG] 3808/21005 (18.1%) - bs=32\n",
      "[PROG] 3840/21005 (18.3%) - bs=32\n",
      "[PROG] 3872/21005 (18.4%) - bs=32\n",
      "[PROG] 3904/21005 (18.6%) - bs=32\n",
      "[PROG] 3936/21005 (18.7%) - bs=32\n",
      "[PROG] 3968/21005 (18.9%) - bs=32\n",
      "[PROG] 4000/21005 (19.0%) - bs=32\n",
      "[PROG] 4032/21005 (19.2%) - bs=32\n",
      "[PROG] 4064/21005 (19.3%) - bs=32\n",
      "[PROG] 4096/21005 (19.5%) - bs=32\n",
      "[PROG] 4128/21005 (19.7%) - bs=32\n",
      "[PROG] 4160/21005 (19.8%) - bs=32\n",
      "[PROG] 4192/21005 (20.0%) - bs=32\n",
      "[PROG] 4224/21005 (20.1%) - bs=32\n",
      "[PROG] 4256/21005 (20.3%) - bs=32\n",
      "[PROG] 4288/21005 (20.4%) - bs=32\n",
      "[PROG] 4320/21005 (20.6%) - bs=32\n",
      "[PROG] 4352/21005 (20.7%) - bs=32\n",
      "[PROG] 4384/21005 (20.9%) - bs=32\n",
      "[PROG] 4416/21005 (21.0%) - bs=32\n",
      "[PROG] 4448/21005 (21.2%) - bs=32\n",
      "[PROG] 4480/21005 (21.3%) - bs=32\n",
      "[PROG] 4512/21005 (21.5%) - bs=32\n",
      "[PROG] 4544/21005 (21.6%) - bs=32\n",
      "[PROG] 4576/21005 (21.8%) - bs=32\n",
      "[PROG] 4608/21005 (21.9%) - bs=32\n",
      "[PROG] 4640/21005 (22.1%) - bs=32\n",
      "[PROG] 4672/21005 (22.2%) - bs=32\n",
      "[PROG] 4704/21005 (22.4%) - bs=32\n",
      "[PROG] 4736/21005 (22.5%) - bs=32\n",
      "[PROG] 4768/21005 (22.7%) - bs=32\n",
      "[PROG] 4800/21005 (22.9%) - bs=32\n",
      "[PROG] 4832/21005 (23.0%) - bs=32\n",
      "[PROG] 4864/21005 (23.2%) - bs=32\n",
      "[PROG] 4896/21005 (23.3%) - bs=32\n",
      "[PROG] 4928/21005 (23.5%) - bs=32\n",
      "[PROG] 4960/21005 (23.6%) - bs=32\n",
      "[PROG] 4992/21005 (23.8%) - bs=32\n",
      "[PROG] 5024/21005 (23.9%) - bs=32\n",
      "[PROG] 5056/21005 (24.1%) - bs=32\n",
      "[PROG] 5088/21005 (24.2%) - bs=32\n",
      "[PROG] 5120/21005 (24.4%) - bs=32\n",
      "[PROG] 5152/21005 (24.5%) - bs=32\n",
      "[PROG] 5184/21005 (24.7%) - bs=32\n",
      "[PROG] 5216/21005 (24.8%) - bs=32\n",
      "[PROG] 5248/21005 (25.0%) - bs=32\n",
      "[PROG] 5280/21005 (25.1%) - bs=32\n",
      "[PROG] 5312/21005 (25.3%) - bs=32\n",
      "[PROG] 5344/21005 (25.4%) - bs=32\n",
      "[PROG] 5376/21005 (25.6%) - bs=32\n",
      "[PROG] 5408/21005 (25.7%) - bs=32\n",
      "[PROG] 5440/21005 (25.9%) - bs=32\n",
      "[PROG] 5472/21005 (26.1%) - bs=32\n",
      "[PROG] 5504/21005 (26.2%) - bs=32\n",
      "[PROG] 5536/21005 (26.4%) - bs=32\n",
      "[PROG] 5568/21005 (26.5%) - bs=32\n",
      "[PROG] 5600/21005 (26.7%) - bs=32\n",
      "[PROG] 5632/21005 (26.8%) - bs=32\n",
      "[PROG] 5664/21005 (27.0%) - bs=32\n",
      "[PROG] 5696/21005 (27.1%) - bs=32\n",
      "[PROG] 5728/21005 (27.3%) - bs=32\n",
      "[PROG] 5760/21005 (27.4%) - bs=32\n",
      "[PROG] 5792/21005 (27.6%) - bs=32\n",
      "[PROG] 5824/21005 (27.7%) - bs=32\n",
      "[PROG] 5856/21005 (27.9%) - bs=32\n",
      "[PROG] 5888/21005 (28.0%) - bs=32\n",
      "[PROG] 5920/21005 (28.2%) - bs=32\n",
      "[PROG] 5952/21005 (28.3%) - bs=32\n",
      "[PROG] 5984/21005 (28.5%) - bs=32\n",
      "[PROG] 6016/21005 (28.6%) - bs=32\n",
      "[PROG] 6048/21005 (28.8%) - bs=32\n",
      "[PROG] 6080/21005 (28.9%) - bs=32\n",
      "[PROG] 6112/21005 (29.1%) - bs=32\n",
      "[PROG] 6144/21005 (29.3%) - bs=32\n",
      "[PROG] 6176/21005 (29.4%) - bs=32\n",
      "[PROG] 6208/21005 (29.6%) - bs=32\n",
      "[PROG] 6240/21005 (29.7%) - bs=32\n",
      "[PROG] 6272/21005 (29.9%) - bs=32\n",
      "[PROG] 6304/21005 (30.0%) - bs=32\n",
      "[PROG] 6336/21005 (30.2%) - bs=32\n",
      "[PROG] 6368/21005 (30.3%) - bs=32\n",
      "[PROG] 6400/21005 (30.5%) - bs=32\n",
      "[PROG] 6432/21005 (30.6%) - bs=32\n",
      "[PROG] 6464/21005 (30.8%) - bs=32\n",
      "[PROG] 6496/21005 (30.9%) - bs=32\n",
      "[PROG] 6528/21005 (31.1%) - bs=32\n",
      "[PROG] 6560/21005 (31.2%) - bs=32\n",
      "[PROG] 6592/21005 (31.4%) - bs=32\n",
      "[PROG] 6624/21005 (31.5%) - bs=32\n",
      "[PROG] 6656/21005 (31.7%) - bs=32\n",
      "[PROG] 6688/21005 (31.8%) - bs=32\n",
      "[PROG] 6720/21005 (32.0%) - bs=32\n",
      "[PROG] 6752/21005 (32.1%) - bs=32\n",
      "[PROG] 6784/21005 (32.3%) - bs=32\n",
      "[PROG] 6816/21005 (32.4%) - bs=32\n",
      "[PROG] 6848/21005 (32.6%) - bs=32\n",
      "[PROG] 6880/21005 (32.8%) - bs=32\n",
      "[PROG] 6912/21005 (32.9%) - bs=32\n",
      "[PROG] 6944/21005 (33.1%) - bs=32\n",
      "[PROG] 6976/21005 (33.2%) - bs=32\n",
      "[PROG] 7008/21005 (33.4%) - bs=32\n",
      "[PROG] 7040/21005 (33.5%) - bs=32\n",
      "[PROG] 7072/21005 (33.7%) - bs=32\n",
      "[PROG] 7104/21005 (33.8%) - bs=32\n",
      "[PROG] 7136/21005 (34.0%) - bs=32\n",
      "[PROG] 7168/21005 (34.1%) - bs=32\n",
      "[PROG] 7200/21005 (34.3%) - bs=32\n",
      "[PROG] 7232/21005 (34.4%) - bs=32\n",
      "[PROG] 7264/21005 (34.6%) - bs=32\n",
      "[PROG] 7296/21005 (34.7%) - bs=32\n",
      "[PROG] 7328/21005 (34.9%) - bs=32\n",
      "[PROG] 7360/21005 (35.0%) - bs=32\n",
      "[PROG] 7392/21005 (35.2%) - bs=32\n",
      "[PROG] 7424/21005 (35.3%) - bs=32\n",
      "[PROG] 7456/21005 (35.5%) - bs=32\n",
      "[PROG] 7488/21005 (35.6%) - bs=32\n",
      "[PROG] 7520/21005 (35.8%) - bs=32\n",
      "[PROG] 7552/21005 (36.0%) - bs=32\n",
      "[PROG] 7584/21005 (36.1%) - bs=32\n",
      "[PROG] 7616/21005 (36.3%) - bs=32\n",
      "[PROG] 7648/21005 (36.4%) - bs=32\n",
      "[PROG] 7680/21005 (36.6%) - bs=32\n",
      "[PROG] 7712/21005 (36.7%) - bs=32\n",
      "[PROG] 7744/21005 (36.9%) - bs=32\n",
      "[PROG] 7776/21005 (37.0%) - bs=32\n",
      "[PROG] 7808/21005 (37.2%) - bs=32\n",
      "[PROG] 7840/21005 (37.3%) - bs=32\n",
      "[PROG] 7872/21005 (37.5%) - bs=32\n",
      "[PROG] 7904/21005 (37.6%) - bs=32\n",
      "[PROG] 7936/21005 (37.8%) - bs=32\n",
      "[PROG] 7968/21005 (37.9%) - bs=32\n",
      "[PROG] 8000/21005 (38.1%) - bs=32\n",
      "[PROG] 8032/21005 (38.2%) - bs=32\n",
      "[PROG] 8064/21005 (38.4%) - bs=32\n",
      "[PROG] 8096/21005 (38.5%) - bs=32\n",
      "[PROG] 8128/21005 (38.7%) - bs=32\n",
      "[PROG] 8160/21005 (38.8%) - bs=32\n",
      "[PROG] 8192/21005 (39.0%) - bs=32\n",
      "[PROG] 8224/21005 (39.2%) - bs=32\n",
      "[PROG] 8256/21005 (39.3%) - bs=32\n",
      "[PROG] 8288/21005 (39.5%) - bs=32\n",
      "[PROG] 8320/21005 (39.6%) - bs=32\n",
      "[PROG] 8352/21005 (39.8%) - bs=32\n",
      "[PROG] 8384/21005 (39.9%) - bs=32\n",
      "[PROG] 8416/21005 (40.1%) - bs=32\n",
      "[PROG] 8448/21005 (40.2%) - bs=32\n",
      "[PROG] 8480/21005 (40.4%) - bs=32\n",
      "[PROG] 8512/21005 (40.5%) - bs=32\n",
      "[PROG] 8544/21005 (40.7%) - bs=32\n",
      "[PROG] 8576/21005 (40.8%) - bs=32\n",
      "[PROG] 8608/21005 (41.0%) - bs=32\n",
      "[PROG] 8640/21005 (41.1%) - bs=32\n",
      "[PROG] 8672/21005 (41.3%) - bs=32\n",
      "[PROG] 8704/21005 (41.4%) - bs=32\n",
      "[PROG] 8736/21005 (41.6%) - bs=32\n",
      "[PROG] 8768/21005 (41.7%) - bs=32\n",
      "[PROG] 8800/21005 (41.9%) - bs=32\n",
      "[PROG] 8832/21005 (42.0%) - bs=32\n",
      "[PROG] 8864/21005 (42.2%) - bs=32\n",
      "[PROG] 8896/21005 (42.4%) - bs=32\n",
      "[PROG] 8928/21005 (42.5%) - bs=32\n",
      "[PROG] 8960/21005 (42.7%) - bs=32\n",
      "[PROG] 8992/21005 (42.8%) - bs=32\n",
      "[PROG] 9024/21005 (43.0%) - bs=32\n",
      "[PROG] 9056/21005 (43.1%) - bs=32\n",
      "[PROG] 9088/21005 (43.3%) - bs=32\n",
      "[PROG] 9120/21005 (43.4%) - bs=32\n",
      "[PROG] 9152/21005 (43.6%) - bs=32\n",
      "[PROG] 9184/21005 (43.7%) - bs=32\n",
      "[PROG] 9216/21005 (43.9%) - bs=32\n",
      "[PROG] 9248/21005 (44.0%) - bs=32\n",
      "[PROG] 9280/21005 (44.2%) - bs=32\n",
      "[PROG] 9312/21005 (44.3%) - bs=32\n",
      "[PROG] 9344/21005 (44.5%) - bs=32\n",
      "[PROG] 9376/21005 (44.6%) - bs=32\n",
      "[PROG] 9408/21005 (44.8%) - bs=32\n",
      "[PROG] 9440/21005 (44.9%) - bs=32\n",
      "[PROG] 9472/21005 (45.1%) - bs=32\n",
      "[PROG] 9504/21005 (45.2%) - bs=32\n",
      "[PROG] 9536/21005 (45.4%) - bs=32\n",
      "[PROG] 9568/21005 (45.6%) - bs=32\n",
      "[PROG] 9600/21005 (45.7%) - bs=32\n",
      "[PROG] 9632/21005 (45.9%) - bs=32\n",
      "[PROG] 9664/21005 (46.0%) - bs=32\n",
      "[PROG] 9696/21005 (46.2%) - bs=32\n",
      "[PROG] 9728/21005 (46.3%) - bs=32\n",
      "[PROG] 9760/21005 (46.5%) - bs=32\n",
      "[PROG] 9792/21005 (46.6%) - bs=32\n",
      "[PROG] 9824/21005 (46.8%) - bs=32\n",
      "[PROG] 9856/21005 (46.9%) - bs=32\n",
      "[PROG] 9888/21005 (47.1%) - bs=32\n",
      "[PROG] 9920/21005 (47.2%) - bs=32\n",
      "[PROG] 9952/21005 (47.4%) - bs=32\n",
      "[PROG] 9984/21005 (47.5%) - bs=32\n",
      "[PROG] 10016/21005 (47.7%) - bs=32\n",
      "[PROG] 10048/21005 (47.8%) - bs=32\n",
      "[PROG] 10080/21005 (48.0%) - bs=32\n",
      "[PROG] 10112/21005 (48.1%) - bs=32\n",
      "[PROG] 10144/21005 (48.3%) - bs=32\n",
      "[PROG] 10176/21005 (48.4%) - bs=32\n",
      "[PROG] 10208/21005 (48.6%) - bs=32\n",
      "[PROG] 10240/21005 (48.8%) - bs=32\n",
      "[PROG] 10272/21005 (48.9%) - bs=32\n",
      "[PROG] 10304/21005 (49.1%) - bs=32\n",
      "[PROG] 10336/21005 (49.2%) - bs=32\n",
      "[PROG] 10368/21005 (49.4%) - bs=32\n",
      "[PROG] 10400/21005 (49.5%) - bs=32\n",
      "[PROG] 10432/21005 (49.7%) - bs=32\n",
      "[PROG] 10464/21005 (49.8%) - bs=32\n",
      "[PROG] 10496/21005 (50.0%) - bs=32\n",
      "[PROG] 10528/21005 (50.1%) - bs=32\n",
      "[PROG] 10560/21005 (50.3%) - bs=32\n",
      "[PROG] 10592/21005 (50.4%) - bs=32\n",
      "[PROG] 10624/21005 (50.6%) - bs=32\n",
      "[PROG] 10656/21005 (50.7%) - bs=32\n",
      "[PROG] 10688/21005 (50.9%) - bs=32\n",
      "[PROG] 10720/21005 (51.0%) - bs=32\n",
      "[PROG] 10752/21005 (51.2%) - bs=32\n",
      "[PROG] 10784/21005 (51.3%) - bs=32\n",
      "[PROG] 10816/21005 (51.5%) - bs=32\n",
      "[PROG] 10848/21005 (51.6%) - bs=32\n",
      "[PROG] 10880/21005 (51.8%) - bs=32\n",
      "[PROG] 10912/21005 (51.9%) - bs=32\n",
      "[PROG] 10944/21005 (52.1%) - bs=32\n",
      "[PROG] 10976/21005 (52.3%) - bs=32\n",
      "[PROG] 11008/21005 (52.4%) - bs=32\n",
      "[PROG] 11040/21005 (52.6%) - bs=32\n",
      "[PROG] 11072/21005 (52.7%) - bs=32\n",
      "[PROG] 11104/21005 (52.9%) - bs=32\n",
      "[PROG] 11136/21005 (53.0%) - bs=32\n",
      "[PROG] 11168/21005 (53.2%) - bs=32\n",
      "[PROG] 11200/21005 (53.3%) - bs=32\n",
      "[PROG] 11232/21005 (53.5%) - bs=32\n",
      "[PROG] 11264/21005 (53.6%) - bs=32\n",
      "[PROG] 11296/21005 (53.8%) - bs=32\n",
      "[PROG] 11328/21005 (53.9%) - bs=32\n",
      "[PROG] 11360/21005 (54.1%) - bs=32\n",
      "[PROG] 11392/21005 (54.2%) - bs=32\n",
      "[PROG] 11424/21005 (54.4%) - bs=32\n",
      "[PROG] 11456/21005 (54.5%) - bs=32\n",
      "[PROG] 11488/21005 (54.7%) - bs=32\n",
      "[PROG] 11520/21005 (54.8%) - bs=32\n",
      "[PROG] 11552/21005 (55.0%) - bs=32\n",
      "[PROG] 11584/21005 (55.1%) - bs=32\n",
      "[PROG] 11616/21005 (55.3%) - bs=32\n",
      "[PROG] 11648/21005 (55.5%) - bs=32\n",
      "[PROG] 11680/21005 (55.6%) - bs=32\n",
      "[PROG] 11712/21005 (55.8%) - bs=32\n",
      "[PROG] 11744/21005 (55.9%) - bs=32\n",
      "[PROG] 11776/21005 (56.1%) - bs=32\n",
      "[PROG] 11808/21005 (56.2%) - bs=32\n",
      "[PROG] 11840/21005 (56.4%) - bs=32\n",
      "[PROG] 11872/21005 (56.5%) - bs=32\n",
      "[PROG] 11904/21005 (56.7%) - bs=32\n",
      "[PROG] 11936/21005 (56.8%) - bs=32\n",
      "[PROG] 11968/21005 (57.0%) - bs=32\n",
      "[PROG] 12000/21005 (57.1%) - bs=32\n",
      "[PROG] 12032/21005 (57.3%) - bs=32\n",
      "[PROG] 12064/21005 (57.4%) - bs=32\n",
      "[PROG] 12096/21005 (57.6%) - bs=32\n",
      "[PROG] 12128/21005 (57.7%) - bs=32\n",
      "[PROG] 12160/21005 (57.9%) - bs=32\n",
      "[PROG] 12192/21005 (58.0%) - bs=32\n",
      "[PROG] 12224/21005 (58.2%) - bs=32\n",
      "[PROG] 12256/21005 (58.3%) - bs=32\n",
      "[PROG] 12288/21005 (58.5%) - bs=32\n",
      "[PROG] 12320/21005 (58.7%) - bs=32\n",
      "[PROG] 12352/21005 (58.8%) - bs=32\n",
      "[PROG] 12384/21005 (59.0%) - bs=32\n",
      "[PROG] 12416/21005 (59.1%) - bs=32\n",
      "[PROG] 12448/21005 (59.3%) - bs=32\n",
      "[PROG] 12480/21005 (59.4%) - bs=32\n",
      "[PROG] 12512/21005 (59.6%) - bs=32\n",
      "[PROG] 12544/21005 (59.7%) - bs=32\n",
      "[PROG] 12576/21005 (59.9%) - bs=32\n",
      "[PROG] 12608/21005 (60.0%) - bs=32\n",
      "[PROG] 12640/21005 (60.2%) - bs=32\n",
      "[PROG] 12672/21005 (60.3%) - bs=32\n",
      "[PROG] 12704/21005 (60.5%) - bs=32\n",
      "[PROG] 12736/21005 (60.6%) - bs=32\n",
      "[PROG] 12768/21005 (60.8%) - bs=32\n",
      "[PROG] 12800/21005 (60.9%) - bs=32\n",
      "[PROG] 12832/21005 (61.1%) - bs=32\n",
      "[PROG] 12864/21005 (61.2%) - bs=32\n",
      "[PROG] 12896/21005 (61.4%) - bs=32\n",
      "[PROG] 12928/21005 (61.5%) - bs=32\n",
      "[PROG] 12960/21005 (61.7%) - bs=32\n",
      "[PROG] 12992/21005 (61.9%) - bs=32\n",
      "[PROG] 13024/21005 (62.0%) - bs=32\n",
      "[PROG] 13056/21005 (62.2%) - bs=32\n",
      "[PROG] 13088/21005 (62.3%) - bs=32\n",
      "[PROG] 13120/21005 (62.5%) - bs=32\n",
      "[PROG] 13152/21005 (62.6%) - bs=32\n",
      "[PROG] 13184/21005 (62.8%) - bs=32\n",
      "[PROG] 13216/21005 (62.9%) - bs=32\n",
      "[PROG] 13248/21005 (63.1%) - bs=32\n",
      "[PROG] 13280/21005 (63.2%) - bs=32\n",
      "[PROG] 13312/21005 (63.4%) - bs=32\n",
      "[PROG] 13344/21005 (63.5%) - bs=32\n",
      "[PROG] 13376/21005 (63.7%) - bs=32\n",
      "[PROG] 13408/21005 (63.8%) - bs=32\n",
      "[PROG] 13440/21005 (64.0%) - bs=32\n",
      "[PROG] 13472/21005 (64.1%) - bs=32\n",
      "[PROG] 13504/21005 (64.3%) - bs=32\n",
      "[PROG] 13536/21005 (64.4%) - bs=32\n",
      "[PROG] 13568/21005 (64.6%) - bs=32\n",
      "[PROG] 13600/21005 (64.7%) - bs=32\n",
      "[PROG] 13632/21005 (64.9%) - bs=32\n",
      "[PROG] 13664/21005 (65.1%) - bs=32\n",
      "[PROG] 13696/21005 (65.2%) - bs=32\n",
      "[PROG] 13728/21005 (65.4%) - bs=32\n",
      "[PROG] 13760/21005 (65.5%) - bs=32\n",
      "[PROG] 13792/21005 (65.7%) - bs=32\n",
      "[PROG] 13824/21005 (65.8%) - bs=32\n",
      "[PROG] 13856/21005 (66.0%) - bs=32\n",
      "[PROG] 13888/21005 (66.1%) - bs=32\n",
      "[PROG] 13920/21005 (66.3%) - bs=32\n",
      "[PROG] 13952/21005 (66.4%) - bs=32\n",
      "[PROG] 13984/21005 (66.6%) - bs=32\n",
      "[PROG] 14016/21005 (66.7%) - bs=32\n",
      "[PROG] 14048/21005 (66.9%) - bs=32\n",
      "[PROG] 14080/21005 (67.0%) - bs=32\n",
      "[PROG] 14112/21005 (67.2%) - bs=32\n",
      "[PROG] 14144/21005 (67.3%) - bs=32\n",
      "[PROG] 14176/21005 (67.5%) - bs=32\n",
      "[PROG] 14208/21005 (67.6%) - bs=32\n",
      "[PROG] 14240/21005 (67.8%) - bs=32\n",
      "[PROG] 14272/21005 (67.9%) - bs=32\n",
      "[PROG] 14304/21005 (68.1%) - bs=32\n",
      "[PROG] 14336/21005 (68.3%) - bs=32\n",
      "[PROG] 14368/21005 (68.4%) - bs=32\n",
      "[PROG] 14400/21005 (68.6%) - bs=32\n",
      "[PROG] 14432/21005 (68.7%) - bs=32\n",
      "[PROG] 14464/21005 (68.9%) - bs=32\n",
      "[PROG] 14496/21005 (69.0%) - bs=32\n",
      "[PROG] 14528/21005 (69.2%) - bs=32\n",
      "[PROG] 14560/21005 (69.3%) - bs=32\n",
      "[PROG] 14592/21005 (69.5%) - bs=32\n",
      "[PROG] 14624/21005 (69.6%) - bs=32\n",
      "[PROG] 14656/21005 (69.8%) - bs=32\n",
      "[PROG] 14688/21005 (69.9%) - bs=32\n",
      "[PROG] 14720/21005 (70.1%) - bs=32\n",
      "[PROG] 14752/21005 (70.2%) - bs=32\n",
      "[PROG] 14784/21005 (70.4%) - bs=32\n",
      "[PROG] 14816/21005 (70.5%) - bs=32\n",
      "[PROG] 14848/21005 (70.7%) - bs=32\n",
      "[PROG] 14880/21005 (70.8%) - bs=32\n",
      "[PROG] 14912/21005 (71.0%) - bs=32\n",
      "[PROG] 14944/21005 (71.1%) - bs=32\n",
      "[PROG] 14976/21005 (71.3%) - bs=32\n",
      "[PROG] 15008/21005 (71.4%) - bs=32\n",
      "[PROG] 15040/21005 (71.6%) - bs=32\n",
      "[PROG] 15072/21005 (71.8%) - bs=32\n",
      "[PROG] 15104/21005 (71.9%) - bs=32\n",
      "[PROG] 15136/21005 (72.1%) - bs=32\n",
      "[PROG] 15168/21005 (72.2%) - bs=32\n",
      "[PROG] 15200/21005 (72.4%) - bs=32\n",
      "[PROG] 15232/21005 (72.5%) - bs=32\n",
      "[PROG] 15264/21005 (72.7%) - bs=32\n",
      "[PROG] 15296/21005 (72.8%) - bs=32\n",
      "[PROG] 15328/21005 (73.0%) - bs=32\n",
      "[PROG] 15360/21005 (73.1%) - bs=32\n",
      "[PROG] 15392/21005 (73.3%) - bs=32\n",
      "[PROG] 15424/21005 (73.4%) - bs=32\n",
      "[PROG] 15456/21005 (73.6%) - bs=32\n",
      "[PROG] 15488/21005 (73.7%) - bs=32\n",
      "[PROG] 15520/21005 (73.9%) - bs=32\n",
      "[PROG] 15552/21005 (74.0%) - bs=32\n",
      "[PROG] 15584/21005 (74.2%) - bs=32\n",
      "[PROG] 15616/21005 (74.3%) - bs=32\n",
      "[PROG] 15648/21005 (74.5%) - bs=32\n",
      "[PROG] 15680/21005 (74.6%) - bs=32\n",
      "[PROG] 15712/21005 (74.8%) - bs=32\n",
      "[PROG] 15744/21005 (75.0%) - bs=32\n",
      "[PROG] 15776/21005 (75.1%) - bs=32\n",
      "[PROG] 15808/21005 (75.3%) - bs=32\n",
      "[PROG] 15840/21005 (75.4%) - bs=32\n",
      "[PROG] 15872/21005 (75.6%) - bs=32\n",
      "[PROG] 15904/21005 (75.7%) - bs=32\n",
      "[PROG] 15936/21005 (75.9%) - bs=32\n",
      "[PROG] 15968/21005 (76.0%) - bs=32\n",
      "[PROG] 16000/21005 (76.2%) - bs=32\n",
      "[PROG] 16032/21005 (76.3%) - bs=32\n",
      "[PROG] 16064/21005 (76.5%) - bs=32\n",
      "[PROG] 16096/21005 (76.6%) - bs=32\n",
      "[PROG] 16128/21005 (76.8%) - bs=32\n",
      "[PROG] 16160/21005 (76.9%) - bs=32\n",
      "[PROG] 16192/21005 (77.1%) - bs=32\n",
      "[PROG] 16224/21005 (77.2%) - bs=32\n",
      "[PROG] 16256/21005 (77.4%) - bs=32\n",
      "[PROG] 16288/21005 (77.5%) - bs=32\n",
      "[PROG] 16320/21005 (77.7%) - bs=32\n",
      "[PROG] 16352/21005 (77.8%) - bs=32\n",
      "[PROG] 16384/21005 (78.0%) - bs=32\n",
      "[PROG] 16416/21005 (78.2%) - bs=32\n",
      "[PROG] 16448/21005 (78.3%) - bs=32\n",
      "[PROG] 16480/21005 (78.5%) - bs=32\n",
      "[PROG] 16512/21005 (78.6%) - bs=32\n",
      "[PROG] 16544/21005 (78.8%) - bs=32\n",
      "[PROG] 16576/21005 (78.9%) - bs=32\n",
      "[PROG] 16608/21005 (79.1%) - bs=32\n",
      "[PROG] 16640/21005 (79.2%) - bs=32\n",
      "[PROG] 16672/21005 (79.4%) - bs=32\n",
      "[PROG] 16704/21005 (79.5%) - bs=32\n",
      "[PROG] 16736/21005 (79.7%) - bs=32\n",
      "[PROG] 16768/21005 (79.8%) - bs=32\n",
      "[PROG] 16800/21005 (80.0%) - bs=32\n",
      "[PROG] 16832/21005 (80.1%) - bs=32\n",
      "[PROG] 16864/21005 (80.3%) - bs=32\n",
      "[PROG] 16896/21005 (80.4%) - bs=32\n",
      "[PROG] 16928/21005 (80.6%) - bs=32\n",
      "[PROG] 16960/21005 (80.7%) - bs=32\n",
      "[PROG] 16992/21005 (80.9%) - bs=32\n",
      "[PROG] 17024/21005 (81.0%) - bs=32\n",
      "[PROG] 17056/21005 (81.2%) - bs=32\n",
      "[PROG] 17088/21005 (81.4%) - bs=32\n",
      "[PROG] 17120/21005 (81.5%) - bs=32\n",
      "[PROG] 17152/21005 (81.7%) - bs=32\n",
      "[PROG] 17184/21005 (81.8%) - bs=32\n",
      "[PROG] 17216/21005 (82.0%) - bs=32\n",
      "[PROG] 17248/21005 (82.1%) - bs=32\n",
      "[PROG] 17280/21005 (82.3%) - bs=32\n",
      "[PROG] 17312/21005 (82.4%) - bs=32\n",
      "[PROG] 17344/21005 (82.6%) - bs=32\n",
      "[PROG] 17376/21005 (82.7%) - bs=32\n",
      "[PROG] 17408/21005 (82.9%) - bs=32\n",
      "[PROG] 17440/21005 (83.0%) - bs=32\n",
      "[PROG] 17472/21005 (83.2%) - bs=32\n",
      "[PROG] 17504/21005 (83.3%) - bs=32\n",
      "[PROG] 17536/21005 (83.5%) - bs=32\n",
      "[PROG] 17568/21005 (83.6%) - bs=32\n",
      "[PROG] 17600/21005 (83.8%) - bs=32\n",
      "[PROG] 17632/21005 (83.9%) - bs=32\n",
      "[PROG] 17664/21005 (84.1%) - bs=32\n",
      "[PROG] 17696/21005 (84.2%) - bs=32\n",
      "[PROG] 17728/21005 (84.4%) - bs=32\n",
      "[PROG] 17760/21005 (84.6%) - bs=32\n",
      "[PROG] 17792/21005 (84.7%) - bs=32\n",
      "[PROG] 17824/21005 (84.9%) - bs=32\n",
      "[PROG] 17856/21005 (85.0%) - bs=32\n",
      "[PROG] 17888/21005 (85.2%) - bs=32\n",
      "[PROG] 17920/21005 (85.3%) - bs=32\n",
      "[PROG] 17952/21005 (85.5%) - bs=32\n",
      "[PROG] 17984/21005 (85.6%) - bs=32\n",
      "[PROG] 18016/21005 (85.8%) - bs=32\n",
      "[PROG] 18048/21005 (85.9%) - bs=32\n",
      "[PROG] 18080/21005 (86.1%) - bs=32\n",
      "[PROG] 18112/21005 (86.2%) - bs=32\n",
      "[PROG] 18144/21005 (86.4%) - bs=32\n",
      "[PROG] 18176/21005 (86.5%) - bs=32\n",
      "[PROG] 18208/21005 (86.7%) - bs=32\n",
      "[PROG] 18240/21005 (86.8%) - bs=32\n",
      "[PROG] 18272/21005 (87.0%) - bs=32\n",
      "[PROG] 18304/21005 (87.1%) - bs=32\n",
      "[PROG] 18336/21005 (87.3%) - bs=32\n",
      "[PROG] 18368/21005 (87.4%) - bs=32\n",
      "[PROG] 18400/21005 (87.6%) - bs=32\n",
      "[PROG] 18432/21005 (87.8%) - bs=32\n",
      "[PROG] 18464/21005 (87.9%) - bs=32\n",
      "[PROG] 18496/21005 (88.1%) - bs=32\n",
      "[PROG] 18528/21005 (88.2%) - bs=32\n",
      "[PROG] 18560/21005 (88.4%) - bs=32\n",
      "[PROG] 18592/21005 (88.5%) - bs=32\n",
      "[PROG] 18624/21005 (88.7%) - bs=32\n",
      "[PROG] 18656/21005 (88.8%) - bs=32\n",
      "[PROG] 18688/21005 (89.0%) - bs=32\n",
      "[PROG] 18720/21005 (89.1%) - bs=32\n",
      "[PROG] 18752/21005 (89.3%) - bs=32\n",
      "[PROG] 18784/21005 (89.4%) - bs=32\n",
      "[PROG] 18816/21005 (89.6%) - bs=32\n",
      "[PROG] 18848/21005 (89.7%) - bs=32\n",
      "[PROG] 18880/21005 (89.9%) - bs=32\n",
      "[PROG] 18912/21005 (90.0%) - bs=32\n",
      "[PROG] 18944/21005 (90.2%) - bs=32\n",
      "[PROG] 18976/21005 (90.3%) - bs=32\n",
      "[PROG] 19008/21005 (90.5%) - bs=32\n",
      "[PROG] 19040/21005 (90.6%) - bs=32\n",
      "[PROG] 19072/21005 (90.8%) - bs=32\n",
      "[PROG] 19104/21005 (90.9%) - bs=32\n",
      "[PROG] 19136/21005 (91.1%) - bs=32\n",
      "[PROG] 19168/21005 (91.3%) - bs=32\n",
      "[PROG] 19200/21005 (91.4%) - bs=32\n",
      "[PROG] 19232/21005 (91.6%) - bs=32\n",
      "[PROG] 19264/21005 (91.7%) - bs=32\n",
      "[PROG] 19296/21005 (91.9%) - bs=32\n",
      "[PROG] 19328/21005 (92.0%) - bs=32\n",
      "[PROG] 19360/21005 (92.2%) - bs=32\n",
      "[PROG] 19392/21005 (92.3%) - bs=32\n",
      "[PROG] 19424/21005 (92.5%) - bs=32\n",
      "[PROG] 19456/21005 (92.6%) - bs=32\n",
      "[PROG] 19488/21005 (92.8%) - bs=32\n",
      "[PROG] 19520/21005 (92.9%) - bs=32\n",
      "[PROG] 19552/21005 (93.1%) - bs=32\n",
      "[PROG] 19584/21005 (93.2%) - bs=32\n",
      "[PROG] 19616/21005 (93.4%) - bs=32\n",
      "[PROG] 19648/21005 (93.5%) - bs=32\n",
      "[PROG] 19680/21005 (93.7%) - bs=32\n",
      "[PROG] 19712/21005 (93.8%) - bs=32\n",
      "[PROG] 19744/21005 (94.0%) - bs=32\n",
      "[PROG] 19776/21005 (94.1%) - bs=32\n",
      "[PROG] 19808/21005 (94.3%) - bs=32\n",
      "[PROG] 19840/21005 (94.5%) - bs=32\n",
      "[PROG] 19872/21005 (94.6%) - bs=32\n",
      "[PROG] 19904/21005 (94.8%) - bs=32\n",
      "[PROG] 19936/21005 (94.9%) - bs=32\n",
      "[PROG] 19968/21005 (95.1%) - bs=32\n",
      "[PROG] 20000/21005 (95.2%) - bs=32\n",
      "[PROG] 20032/21005 (95.4%) - bs=32\n",
      "[PROG] 20064/21005 (95.5%) - bs=32\n",
      "[PROG] 20096/21005 (95.7%) - bs=32\n",
      "[PROG] 20128/21005 (95.8%) - bs=32\n",
      "[PROG] 20160/21005 (96.0%) - bs=32\n",
      "[PROG] 20192/21005 (96.1%) - bs=32\n",
      "[PROG] 20224/21005 (96.3%) - bs=32\n",
      "[PROG] 20256/21005 (96.4%) - bs=32\n",
      "[PROG] 20288/21005 (96.6%) - bs=32\n",
      "[PROG] 20320/21005 (96.7%) - bs=32\n",
      "[PROG] 20352/21005 (96.9%) - bs=32\n",
      "[PROG] 20384/21005 (97.0%) - bs=32\n",
      "[PROG] 20416/21005 (97.2%) - bs=32\n",
      "[PROG] 20448/21005 (97.3%) - bs=32\n",
      "[PROG] 20480/21005 (97.5%) - bs=32\n",
      "[PROG] 20512/21005 (97.7%) - bs=32\n",
      "[PROG] 20544/21005 (97.8%) - bs=32\n",
      "[PROG] 20576/21005 (98.0%) - bs=32\n",
      "[PROG] 20608/21005 (98.1%) - bs=32\n",
      "[PROG] 20640/21005 (98.3%) - bs=32\n",
      "[PROG] 20672/21005 (98.4%) - bs=32\n",
      "[PROG] 20704/21005 (98.6%) - bs=32\n",
      "[PROG] 20736/21005 (98.7%) - bs=32\n",
      "[PROG] 20768/21005 (98.9%) - bs=32\n",
      "[PROG] 20800/21005 (99.0%) - bs=32\n",
      "[PROG] 20832/21005 (99.2%) - bs=32\n",
      "[PROG] 20864/21005 (99.3%) - bs=32\n",
      "[PROG] 20896/21005 (99.5%) - bs=32\n",
      "[PROG] 20928/21005 (99.6%) - bs=32\n",
      "[PROG] 20960/21005 (99.8%) - bs=32\n",
      "[PROG] 20992/21005 (99.9%) - bs=32\n",
      "[PROG] 21005/21005 (100.0%) - bs=32\n",
      "[OK] FAISS guardado: faiss_index_ip.bin | ntotal=21005 | dim=384\n",
      "[OK] PKL (meta_min) guardado: embeddings_meta_min.pkl\n"
     ]
    }
   ],
   "source": [
    "import os, gc, pickle, numpy as np, pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import Optional\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "PARQUET_PATH   = os.environ.get(\"PARQUET_PATH\", \"corpus_chunks.parquet\")   # <-- solo contiene los chunks\n",
    "FAISS_PATH     = os.environ.get(\"FAISS_PATH\", \"faiss_index_ip.bin\")\n",
    "PKL_MIN_PATH   = os.environ.get(\"PKL_MIN_PATH\", \"embeddings_meta_min.pkl\")\n",
    "\n",
    "\n",
    "# Modelo recomendado para CPU\n",
    "EMB_MODEL        = os.environ.get(\"EMB_MODEL\", \"intfloat/multilingual-e5-small\")\n",
    "EMB_MAX_SEQ_LEN  = int(os.environ.get(\"EMB_MAX_SEQ_LEN\", \"300\"))   # <=512\n",
    "INIT_BATCH       = int(os.environ.get(\"BATCH_SIZE\", \"32\"))\n",
    "MIN_BATCH        = 1\n",
    "\n",
    "# -------------------- 0) Carga parquet de chunks + limpieza --------------------\n",
    "try:\n",
    "    chunks_df = pd.read_parquet(PARQUET_PATH, engine=\"pyarrow\")\n",
    "except Exception:\n",
    "    chunks_df = pd.read_parquet(PARQUET_PATH, engine=\"fastparquet\")\n",
    "\n",
    "required_cols = {\"doc_id\",\"chunk_id\",\"start_token\",\"end_token\",\"text_chunk\"}\n",
    "missing = required_cols - set(chunks_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas requeridas en {PARQUET_PATH}: {missing}\")\n",
    "\n",
    "# Normaliza tipos\n",
    "for c in [\"doc_id\",\"chunk_id\",\"start_token\",\"end_token\"]:\n",
    "    if chunks_df[c].dtype.kind not in \"iu\":\n",
    "        chunks_df[c] = pd.to_numeric(chunks_df[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "# Limpieza texto\n",
    "chunks_df[\"text_chunk\"] = chunks_df[\"text_chunk\"].astype(str).str.strip()\n",
    "chunks_df = chunks_df[chunks_df[\"text_chunk\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# chunk_uid y scopus_id opcional\n",
    "if \"chunk_uid\" not in chunks_df.columns:\n",
    "    chunks_df[\"chunk_uid\"] = chunks_df[\"doc_id\"].astype(str) + \"-\" + chunks_df[\"chunk_id\"].astype(str)\n",
    "if \"scopus_id\" in chunks_df.columns:\n",
    "    chunks_df[\"scopus_id\"] = chunks_df[\"scopus_id\"].astype(str)\n",
    "\n",
    "# IDs vectoriales alineados 0..N-1\n",
    "N = len(chunks_df)\n",
    "chunks_df[\"vec_id\"] = np.arange(N, dtype=\"int64\")\n",
    "chunks_df[\"embedding_model\"] = EMB_MODEL\n",
    "\n",
    "# -------------------- 1) Modelo (CPU) alineado con chunking --------------------\n",
    "print(f\"[INFO] device encode: cpu; model={EMB_MODEL}\")\n",
    "model = SentenceTransformer(EMB_MODEL, device=\"cpu\")\n",
    "model.max_seq_length = min(EMB_MAX_SEQ_LEN, 512)\n",
    "print(f\"[INFO] model.max_seq_length = {model.max_seq_length}\")\n",
    "\n",
    "# Prefijo E5\n",
    "passages = (\"passage: \" + chunks_df[\"text_chunk\"]).tolist()\n",
    "\n",
    "# -------------------- 2) FAISS (IP con embeddings normalizados -> coseno) --------------------\n",
    "def make_faiss_index(dim: int):\n",
    "    print(\"[INFO] FAISS-CPU\")\n",
    "    return faiss.IndexFlatIP(dim)\n",
    "\n",
    "def st_encode_cpu(texts, batch_size, normalize=True, to_numpy=True):\n",
    "    embs = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=False,\n",
    "        normalize_embeddings=normalize,\n",
    "        convert_to_numpy=to_numpy\n",
    "    )\n",
    "    return np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "# -------------------- 3) Bucle por lotes (CPU) --------------------\n",
    "def stream_encode_and_build(passages, init_bs=INIT_BATCH, min_bs=MIN_BATCH):\n",
    "    i, bs = 0, init_bs\n",
    "    index = None\n",
    "    dim = None\n",
    "\n",
    "    while i < N:\n",
    "        j = min(i + bs, N)\n",
    "        batch = passages[i:j]\n",
    "        try:\n",
    "            emb = st_encode_cpu(batch, batch_size=bs, normalize=True, to_numpy=True)\n",
    "\n",
    "            if dim is None:\n",
    "                dim = emb.shape[1]\n",
    "                index = make_faiss_index(dim)\n",
    "\n",
    "            index.add(emb)\n",
    "\n",
    "            i = j\n",
    "            print(f\"[PROG] {i}/{N} ({100.0*i/N:.1f}%) - bs={bs}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            prev_bs = bs\n",
    "            bs = max(min_bs, bs // 2)\n",
    "            gc.collect()\n",
    "            if prev_bs == bs and bs == min_bs:\n",
    "                raise RuntimeError(f\"Fallo persistente en CPU con batch={bs}: {e}\") from e\n",
    "            print(f\"[WARN] Error en i={i}. Bajo batch {prev_bs}->{bs} y reintento…\")\n",
    "            continue\n",
    "\n",
    "    return index, dim\n",
    "\n",
    "# -------------------- 4) Ejecutar pipeline --------------------\n",
    "index_cpu, dim = stream_encode_and_build(passages)\n",
    "\n",
    "# -------------------- 5) Guardar FAISS + PKL (mapa mínimo) --------------------\n",
    "faiss.write_index(index_cpu, FAISS_PATH)\n",
    "print(f\"[OK] FAISS guardado: {FAISS_PATH} | ntotal={index_cpu.ntotal} | dim={dim}\")\n",
    "\n",
    "# PKL: guardar meta_min con scopus_id si existe\n",
    "min_cols = [\"vec_id\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"start_token\",\"end_token\"]\n",
    "if \"scopus_id\" in chunks_df.columns:\n",
    "    min_cols.append(\"scopus_id\")\n",
    "\n",
    "with open(PKL_MIN_PATH, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"model\": EMB_MODEL,\n",
    "        \"device_used\": \"cpu\",\n",
    "        \"dim\": dim,\n",
    "        \"meta_min\": chunks_df[min_cols].copy()\n",
    "    }, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"[OK] PKL (meta_min) guardado: {PKL_MIN_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a9a44",
   "metadata": {},
   "source": [
    "## recuperacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "471578c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP-K (meta mínimo) ===\n",
      "[INFO] Modelo: intfloat/multilingual-e5-small | max_seq_length=300\n",
      "[INFO] meta_min columnas: ['vec_id', 'chunk_uid', 'doc_id', 'chunk_id', 'start_token', 'end_token', 'scopus_id'] | filas=21005\n",
      "[INFO] Índice FAISS cargado: ntotal=21005\n",
      "    vec_id     score chunk_uid  doc_id  chunk_id    scopus_id  start_token  \\\n",
      "0     5168  0.864254    4660-0    4660         0  85132240597            0   \n",
      "1     9580  0.860996    8747-0    8747         0  85148845556            0   \n",
      "2    14590  0.855215   13319-0   13319         0  85176328476            0   \n",
      "3     4581  0.849804    4128-0    4128         0  85129524720            0   \n",
      "4    10216  0.848282    9326-0    9326         0  85151140601            0   \n",
      "..     ...       ...       ...     ...       ...          ...          ...   \n",
      "95   20944  0.818524   19184-0   19184         0  85201458433            0   \n",
      "96    1846  0.818447    1631-0    1631         0  85116149453            0   \n",
      "97   11685  0.817932   10676-0   10676         0  85162230354            0   \n",
      "98   10574  0.817763    9656-0    9656         0  85152662053            0   \n",
      "99     262  0.817730     231-0     231         0  85096028578            0   \n",
      "\n",
      "    end_token  \n",
      "0         117  \n",
      "1         124  \n",
      "2         116  \n",
      "3         253  \n",
      "4         169  \n",
      "..        ...  \n",
      "95        237  \n",
      "96        295  \n",
      "97        178  \n",
      "98        300  \n",
      "99        253  \n",
      "\n",
      "[100 rows x 8 columns]\n",
      "\n",
      "=== TOP-K + TODA la metadata del CSV ===\n",
      "[INFO] scoupusdata.csv: filas=19233 | cols=9\n",
      "   vec_id     score chunk_uid  doc_id  chunk_id    scopus_id  start_token  \\\n",
      "0    5168  0.864254    4660-0    4660         0  85132240597            0   \n",
      "1    9580  0.860996    8747-0    8747         0  85148845556            0   \n",
      "2   14590  0.855215   13319-0   13319         0  85176328476            0   \n",
      "3    4581  0.849804    4128-0    4128         0  85129524720            0   \n",
      "4   10216  0.848282    9326-0    9326         0  85151140601            0   \n",
      "\n",
      "   end_token                                              title  \\\n",
      "0        117  “Rethink and reset” tourism in the Galapagos I...   \n",
      "1        124              Imperiled Ecosystems: Galápagos Scrub   \n",
      "2        116  A Bunch of Books, a Suitcase, and Many Trips b...   \n",
      "3        253  Fishing during the “new normality”: social and...   \n",
      "4        169  Residents, conservation, development and touri...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Tourism growth in biodiversity conservation ar...   \n",
      "1  The Galápagos Islands are an extraordinary loc...   \n",
      "2  The Galapagos Islands (Galapagos Province, Ecu...   \n",
      "3  The crisis caused by COVID-19 has profoundly a...   \n",
      "4  Tourism from its initial stage generates socio...   \n",
      "\n",
      "                                  doi  \\\n",
      "0        10.1016/j.annale.2022.100057   \n",
      "1  10.1016/B978-0-12-821139-7.00196-3   \n",
      "2                      10.1086/726324   \n",
      "3          10.1007/s40152-022-00268-z   \n",
      "4     10.4067/S0718-34022022000300201   \n",
      "\n",
      "                                             authors  \\\n",
      "0  Diego Quiroga Ferri; Diana V. Burbano; Thomas ...   \n",
      "1       Gonzalo Francisco Rivas-Torres; Nejem Raheem   \n",
      "2                                  Edgardo Civallero   \n",
      "3  Gabriela Rodríguez; Solange Andrade; Jorge Ram...   \n",
      "4          Paula Moya Mosquera; Andrea Muñoz Barriga   \n",
      "\n",
      "                                        affiliations  \\\n",
      "0  ABREC; Université McGill; Fundación Ecuatorian...   \n",
      "1  Emerson College; Universidad San Francisco de ...   \n",
      "2                          Charles Darwin Foundation   \n",
      "3  Charles Darwin Foundation Santa Cruz; Universi...   \n",
      "4        Pontificia Universidad Católica del Ecuador   \n",
      "\n",
      "            affiliation_cities   affiliation_countries  citation_count  \n",
      "0              Quito; Montreal         Ecuador; Canada               0  \n",
      "1                Boston; Quito  United States; Ecuador               0  \n",
      "2                        Quito                 Ecuador               0  \n",
      "3  Santa Cruz Island; Waterloo         Ecuador; Canada               0  \n",
      "4                        Quito                 Ecuador               0  \n"
     ]
    }
   ],
   "source": [
    "import os, pickle\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---- Rutas (ajústalas o usa variables de entorno) ----\n",
    "PKL_MIN_PATH = os.environ.get(\"PKL_MIN_PATH\", \"embeddings_meta_min.pkl\")\n",
    "FAISS_PATH   = os.environ.get(\"FAISS_PATH\", \"faiss_index_ip.bin\")\n",
    "SCOPUS_CSV   = os.environ.get(\"SCOPUS_CSV\", \"scopusdata.csv\")\n",
    "SCOPUS_SEP   = os.environ.get(\"SCOPUS_SEP\", \"|\") \n",
    "\n",
    "# ---- Caches simples ----\n",
    "_model_cache = None\n",
    "_meta_min_cache = None\n",
    "_index_cache = None\n",
    "_scopus_cache = None\n",
    "\n",
    "def load_pkl_and_model(emb_max_seq_len=300):\n",
    "    global _model_cache, _meta_min_cache\n",
    "    if _model_cache is not None and _meta_min_cache is not None:\n",
    "        return _model_cache, _meta_min_cache\n",
    "    with open(PKL_MIN_PATH, \"rb\") as f:\n",
    "        pkl = pickle.load(f)\n",
    "\n",
    "    meta_min = pkl[\"meta_min\"].copy()  # DataFrame: vec_id, chunk_uid, doc_id, chunk_id, (scopus_id), start/end\n",
    "    _meta_min_cache = meta_min\n",
    "\n",
    "    model_name = pkl.get(\"model\", \"intfloat/multilingual-e5-small\")\n",
    "    model = SentenceTransformer(model_name, device=\"cpu\")\n",
    "    model.max_seq_length = min(int(emb_max_seq_len), 512)\n",
    "    _model_cache = model\n",
    "\n",
    "    print(f\"[INFO] Modelo: {model_name} | max_seq_length={model.max_seq_length}\")\n",
    "    print(f\"[INFO] meta_min columnas: {list(meta_min.columns)} | filas={len(meta_min)}\")\n",
    "    return _model_cache, _meta_min_cache\n",
    "\n",
    "def load_faiss():\n",
    "    global _index_cache\n",
    "    if _index_cache is None:\n",
    "        _index_cache = faiss.read_index(FAISS_PATH)\n",
    "        print(f\"[INFO] Índice FAISS cargado: ntotal={_index_cache.ntotal}\")\n",
    "    return _index_cache\n",
    "\n",
    "def load_scopus_csv():\n",
    "    global _scopus_cache\n",
    "    if _scopus_cache is None:\n",
    "        df = pd.read_csv(SCOPUS_CSV, sep=SCOPUS_SEP)\n",
    "        if \"scopus_id\" not in df.columns:\n",
    "            raise ValueError(f\"{SCOPUS_CSV} no tiene columna 'scopus_id'\")\n",
    "        df[\"scopus_id\"] = df[\"scopus_id\"].astype(str)\n",
    "        _scopus_cache = df\n",
    "        print(f\"[INFO] scoupusdata.csv: filas={len(df)} | cols={len(df.columns)}\")\n",
    "    return _scopus_cache\n",
    "\n",
    "def e5_encode_query(model, query_text: str):\n",
    "    return model.encode([f\"query: {query_text}\"],\n",
    "                        normalize_embeddings=True,\n",
    "                        convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "def search_min(query_text: str, topk: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve SOLO el meta mínimo del PKL (sin CSV):\n",
    "    vec_id, score, chunk_uid, doc_id, chunk_id, (scopus_id si existe), start/end\n",
    "    \"\"\"\n",
    "    model, meta_min = load_pkl_and_model()\n",
    "    index = load_faiss()\n",
    "\n",
    "    q = e5_encode_query(model, query_text)\n",
    "    D, I = index.search(q, topk)\n",
    "    vec_ids = I[0].tolist()\n",
    "\n",
    "    hits = meta_min.set_index(\"vec_id\").loc[vec_ids].reset_index()\n",
    "    hits.insert(1, \"score\", D[0])\n",
    "\n",
    "    cols_front = [c for c in [\"vec_id\",\"score\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"scopus_id\",\"start_token\",\"end_token\"] if c in hits.columns]\n",
    "    rest = [c for c in hits.columns if c not in cols_front]\n",
    "    return hits[cols_front + rest].reset_index(drop=True)\n",
    "\n",
    "def search_full_scopus(query_text: str, topk: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Une el TOP-K con TODAS las columnas de scoupusdata.csv por scopus_id.\n",
    "    \"\"\"\n",
    "    model, meta_min = load_pkl_and_model()\n",
    "    index = load_faiss()\n",
    "    sc = load_scopus_csv()\n",
    "\n",
    "    q = e5_encode_query(model, query_text)\n",
    "    D, I = index.search(q, topk)\n",
    "    vec_ids = I[0].tolist()\n",
    "\n",
    "    hits = meta_min.set_index(\"vec_id\").loc[vec_ids].reset_index()\n",
    "    hits.insert(1, \"score\", D[0])\n",
    "\n",
    "    if \"scopus_id\" not in hits.columns:\n",
    "        raise ValueError(\"meta_min en PKL no contiene 'scopus_id'; no puedo unir con el CSV.\")\n",
    "\n",
    "    out = hits.merge(sc, how=\"left\", on=\"scopus_id\")\n",
    "\n",
    "    # Orden: primero claves/score/offsets, luego TODO el CSV\n",
    "    front = [c for c in [\"vec_id\",\"score\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"scopus_id\",\"start_token\",\"end_token\"] if c in out.columns]\n",
    "    csv_cols = [c for c in sc.columns if c not in front]\n",
    "    return out[front + csv_cols].reset_index(drop=True)\n",
    "\n",
    "# ====== DEMO RÁPIDA ======\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"turismo en galapagos\"   # cambia por tu consulta\n",
    "    print(\"\\n=== TOP-K (meta mínimo) ===\")\n",
    "    print(search_min(query, topk=100))\n",
    "\n",
    "    print(\"\\n=== TOP-K + TODA la metadata del CSV ===\")\n",
    "    df = search_full_scopus(query, topk=5)\n",
    "    # Si quieres guardar para revisar en Excel:\n",
    "    # df.to_csv(\"search_results_full_scopus.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653dda6c",
   "metadata": {},
   "source": [
    "# reranking cross encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9038908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cross-Encoder cargado: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[OK] Top 10 (ya re-rankeado):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec_id</th>\n",
       "      <th>score</th>\n",
       "      <th>score_ce</th>\n",
       "      <th>score_final</th>\n",
       "      <th>scopus_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5168</td>\n",
       "      <td>0.864254</td>\n",
       "      <td>-3.670709</td>\n",
       "      <td>0.823718</td>\n",
       "      <td>85132240597</td>\n",
       "      <td>“Rethink and reset” tourism in the Galapagos I...</td>\n",
       "      <td>Tourism growth in biodiversity conservation ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14203</td>\n",
       "      <td>0.841950</td>\n",
       "      <td>-2.713060</td>\n",
       "      <td>0.744398</td>\n",
       "      <td>85174965454</td>\n",
       "      <td>Agrobiodiversity in four Islands of the Galapa...</td>\n",
       "      <td>The Galapagos Islands, officially annexed to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10216</td>\n",
       "      <td>0.848282</td>\n",
       "      <td>-3.679054</td>\n",
       "      <td>0.720162</td>\n",
       "      <td>85151140601</td>\n",
       "      <td>Residents, conservation, development and touri...</td>\n",
       "      <td>Tourism from its initial stage generates socio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>775</td>\n",
       "      <td>0.820772</td>\n",
       "      <td>-1.053562</td>\n",
       "      <td>0.719618</td>\n",
       "      <td>85105310828</td>\n",
       "      <td>The Galápagos as penal colony: Exile, peonage,...</td>\n",
       "      <td>Transportation to remote islands has been a wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9580</td>\n",
       "      <td>0.860996</td>\n",
       "      <td>-4.914887</td>\n",
       "      <td>0.718906</td>\n",
       "      <td>85148845556</td>\n",
       "      <td>Imperiled Ecosystems: Galápagos Scrub</td>\n",
       "      <td>The Galápagos Islands are an extraordinary loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18344</td>\n",
       "      <td>0.844240</td>\n",
       "      <td>-3.452024</td>\n",
       "      <td>0.709393</td>\n",
       "      <td>85191855593</td>\n",
       "      <td>SUSTAINABLE MANAGEMENT APPLIED TO THE HOTEL SE...</td>\n",
       "      <td>This research analyzes the current situation o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5447</td>\n",
       "      <td>0.843322</td>\n",
       "      <td>-3.669117</td>\n",
       "      <td>0.688851</td>\n",
       "      <td>85133488337</td>\n",
       "      <td>Volcanic event management in the Galápagos Isl...</td>\n",
       "      <td>The volcanoes of Galápagos, Ecuador, are among...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14590</td>\n",
       "      <td>0.855215</td>\n",
       "      <td>-5.482558</td>\n",
       "      <td>0.643393</td>\n",
       "      <td>85176328476</td>\n",
       "      <td>A Bunch of Books, a Suitcase, and Many Trips b...</td>\n",
       "      <td>The Galapagos Islands (Galapagos Province, Ecu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15962</td>\n",
       "      <td>0.843961</td>\n",
       "      <td>-4.802355</td>\n",
       "      <td>0.616639</td>\n",
       "      <td>85182709765</td>\n",
       "      <td>The impact of the COVID-19 pandemic on the Gal...</td>\n",
       "      <td>The COVID-19 pandemic's early stages severely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11028</td>\n",
       "      <td>0.826736</td>\n",
       "      <td>-3.237062</td>\n",
       "      <td>0.611005</td>\n",
       "      <td>85159179257</td>\n",
       "      <td>Tourist planning of an emblematic destination....</td>\n",
       "      <td>The article through the analysis of the constr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vec_id     score  score_ce  score_final    scopus_id  \\\n",
       "0    5168  0.864254 -3.670709     0.823718  85132240597   \n",
       "1   14203  0.841950 -2.713060     0.744398  85174965454   \n",
       "2   10216  0.848282 -3.679054     0.720162  85151140601   \n",
       "3     775  0.820772 -1.053562     0.719618  85105310828   \n",
       "4    9580  0.860996 -4.914887     0.718906  85148845556   \n",
       "5   18344  0.844240 -3.452024     0.709393  85191855593   \n",
       "6    5447  0.843322 -3.669117     0.688851  85133488337   \n",
       "7   14590  0.855215 -5.482558     0.643393  85176328476   \n",
       "8   15962  0.843961 -4.802355     0.616639  85182709765   \n",
       "9   11028  0.826736 -3.237062     0.611005  85159179257   \n",
       "\n",
       "                                               title  \\\n",
       "0  “Rethink and reset” tourism in the Galapagos I...   \n",
       "1  Agrobiodiversity in four Islands of the Galapa...   \n",
       "2  Residents, conservation, development and touri...   \n",
       "3  The Galápagos as penal colony: Exile, peonage,...   \n",
       "4              Imperiled Ecosystems: Galápagos Scrub   \n",
       "5  SUSTAINABLE MANAGEMENT APPLIED TO THE HOTEL SE...   \n",
       "6  Volcanic event management in the Galápagos Isl...   \n",
       "7  A Bunch of Books, a Suitcase, and Many Trips b...   \n",
       "8  The impact of the COVID-19 pandemic on the Gal...   \n",
       "9  Tourist planning of an emblematic destination....   \n",
       "\n",
       "                                            abstract  \n",
       "0  Tourism growth in biodiversity conservation ar...  \n",
       "1  The Galapagos Islands, officially annexed to t...  \n",
       "2  Tourism from its initial stage generates socio...  \n",
       "3  Transportation to remote islands has been a wa...  \n",
       "4  The Galápagos Islands are an extraordinary loc...  \n",
       "5  This research analyzes the current situation o...  \n",
       "6  The volcanoes of Galápagos, Ecuador, are among...  \n",
       "7  The Galapagos Islands (Galapagos Province, Ecu...  \n",
       "8  The COVID-19 pandemic's early stages severely ...  \n",
       "9  The article through the analysis of the constr...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Sequence, Optional, Tuple\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# ---------- Configuración ----------\n",
    "CROSS_ENCODER_MODEL = os.environ.get(\"CROSS_ENCODER_MODEL\", \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "CE_BATCH_SIZE       = int(os.environ.get(\"CE_BATCH_SIZE\", \"64\"))\n",
    "W_CE, W_DENSE       = float(os.environ.get(\"W_CE\", \"0.7\")), float(os.environ.get(\"W_DENSE\", \"0.3\"))\n",
    "\n",
    "# columnas de texto (orden de prioridad)\n",
    "TEXT_COLS = [\"title\", \"abstract\"]  # <- ajustado a tu CSV\n",
    "\n",
    "# ---------- Utilidades ----------\n",
    "_ce_cache: Optional[CrossEncoder] = None\n",
    "\n",
    "def get_cross_encoder(model_name: str = CROSS_ENCODER_MODEL) -> CrossEncoder:\n",
    "    global _ce_cache\n",
    "    if _ce_cache is None:\n",
    "        _ce_cache = CrossEncoder(model_name, device=\"cpu\")\n",
    "        print(f\"[INFO] Cross-Encoder cargado: {model_name}\")\n",
    "    return _ce_cache\n",
    "\n",
    "def _first_nonempty(row: pd.Series, cols: Sequence[str]) -> str:\n",
    "    \"\"\"Devuelve el primer texto no vacío según prioridad en 'cols'.\n",
    "       Si no encuentra, intenta concatenar campos semánticos.\"\"\"\n",
    "    for c in cols:\n",
    "        if c in row and isinstance(row[c], str) and row[c].strip():\n",
    "            return row[c]\n",
    "    parts = []\n",
    "    for c in row.index:\n",
    "        name = c.lower()\n",
    "        if any(tok in name for tok in (\"title\",\"abstract\",\"summary\",\"keywords\",\"chunk\",\"desc\")):\n",
    "            v = row[c]\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                parts.append(v.strip())\n",
    "    return \" \".join(parts)[:4096]  # recorte defensivo\n",
    "\n",
    "def _build_pairs(query_text: str, df_topk: pd.DataFrame, text_cols: Optional[List[str]]) -> Tuple[List[Tuple[str,str]], List[int]]:\n",
    "    cols = text_cols or TEXT_COLS\n",
    "    pairs, idx_map = [], []\n",
    "    for i, row in df_topk.iterrows():\n",
    "        txt = _first_nonempty(row, cols)\n",
    "        pairs.append((query_text, txt if isinstance(txt, str) else \"\"))\n",
    "        idx_map.append(i)\n",
    "    return pairs, idx_map\n",
    "\n",
    "def _minmax(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    mn, mx = float(np.nanmin(x)), float(np.nanmax(x))\n",
    "    if not np.isfinite(mn) or not np.isfinite(mx) or (mx - mn) <= 1e-12:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - mn) / (mx - mn + 1e-12)\n",
    "\n",
    "def rerank_with_cross_encoder(query_text: str,\n",
    "                              df_topk: pd.DataFrame,\n",
    "                              text_cols: Optional[List[str]] = None,\n",
    "                              score_dense_col: str = \"score\",\n",
    "                              fuse_with_dense: bool = True,\n",
    "                              batch_size: int = CE_BATCH_SIZE,\n",
    "                              model_name: str = CROSS_ENCODER_MODEL) -> pd.DataFrame:\n",
    "    \"\"\"Reordena df_topk usando un Cross-Encoder y devuelve un nuevo DataFrame ordenado.\"\"\"\n",
    "    if df_topk is None or len(df_topk) == 0:\n",
    "        raise ValueError(\"df_topk está vacío; ejecuta primero search_full_scopus(query, topk=N).\")\n",
    "    ce = get_cross_encoder(model_name)\n",
    "    pairs, idx_map = _build_pairs(query_text, df_topk, text_cols)\n",
    "\n",
    "    # Predicción por lotes\n",
    "    scores_ce = []\n",
    "    for start in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[start:start+batch_size]\n",
    "        s = ce.predict(batch)  # lista/ndarray de floats\n",
    "        scores_ce.append(np.asarray(s, dtype=np.float32))\n",
    "    scores_ce = np.concatenate(scores_ce, axis=0) if scores_ce else np.zeros(len(df_topk), dtype=np.float32)\n",
    "\n",
    "    # Ensamble y orden\n",
    "    out = df_topk.copy()\n",
    "    out.loc[idx_map, \"score_ce\"] = scores_ce\n",
    "    ce_norm = _minmax(out[\"score_ce\"].values)\n",
    "\n",
    "    if fuse_with_dense and score_dense_col in out.columns:\n",
    "        dense_norm = _minmax(out[score_dense_col].values)\n",
    "        out[\"score_dense_norm\"] = dense_norm\n",
    "        out[\"score_final\"] = W_CE * ce_norm + W_DENSE * dense_norm\n",
    "        order_col = \"score_final\"\n",
    "    else:\n",
    "        out[\"score_final\"] = ce_norm\n",
    "        order_col = \"score_final\"\n",
    "\n",
    "    out = out.sort_values(order_col, ascending=False).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ---------- Ejemplo de uso: ejecuta después de definir tu 'query' ----------\n",
    "# 1) Recupera candidatos con tu función (puedes subir TOPK para un mejor re-ranking).\n",
    "query = os.environ.get(\"QUERY\", \"turismo en galapagos\")\n",
    "TOPK  = int(os.environ.get(\"TOPK\", \"100\"))\n",
    "\n",
    "try:\n",
    "    df_topk = search_full_scopus(query, topk=TOPK)   # usa tu función definida arriba\n",
    "except NameError:\n",
    "    raise RuntimeError(\"No encuentro search_full_scopus(). Ejecuta primero la celda donde la defines.\")\n",
    "\n",
    "# 2) Reranking con Cross-Encoder (ajusta columnas si conoces tus nombres exactos).\n",
    "#    Ejemplo: text_cols=[\"chunk_text\"] o [\"title\",\"abstract\",\"authkeywords\"]\n",
    "reranked = rerank_with_cross_encoder(\n",
    "    query_text=query,\n",
    "    df_topk=df_topk,\n",
    "    text_cols=None,            # autodetección robusta; cambia a lista explícita si la tienes\n",
    "    score_dense_col=\"score\",   # 'score' = puntaje FAISS de tu búsqueda\n",
    "    fuse_with_dense=True\n",
    ")\n",
    "\n",
    "# 3) Visualiza y/o guarda\n",
    "display_cols = [c for c in [\"vec_id\",\"score\",\"score_ce\",\"score_final\",\"scopus_id\",\"title\",\"abstract\",\"chunk_text\"] if c in reranked.columns]\n",
    "print(\"[OK] Top 10 (ya re-rankeado):\")\n",
    "display(reranked.head(10)[display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d1319",
   "metadata": {},
   "source": [
    "# Contruccion y Generación (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a18a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt (vista previa) ===\n",
      "Eres un asistente académico. Responde de forma clara, concisa y basada en evidencia, en español y estilo APA (7ª). Utiliza exclusivamente los fragmentos proporcionados. Cada párrafo debe incluir una mención autoral explícita (‘según <autor>’ o ‘de acuerdo con <autor>’) seguida de la cita en corchetes [n]. Si no hay autor disponible, usa ‘según la fuente [n]’. No imprimas tu propia sección de 'Fuentes'; solo escribe el cuerpo con citas [n]. Pregunta: el turismo en las islas galagos que tan bueno es? Fragmentos (con autor/año si disponible): [1] “Rethink and reset” tourism in the Galapagos Islands: Stakeholders' views on the sustainability of tourism development — Autor(es): Ferri et al. Tourism growth in biodiversity conservation areas presents both challenges and opportunities for sustainability. The COVID-19 pandemic brought both into focus in the Galapagos. This study engages with tourism service providers and regulators in Puerto Ayora, Santa Cruz Island, to explore how ...\n",
      "\n",
      "=== Respuesta (LLM - Ollama HTTP) ===\n",
      "\n",
      "El turismo en las Islas Galápagos presenta tanto desafíos como oportunidades para la sostenibilidad, según Ferri et al. [1].  La implementación del Modelo de Ecoturismo a una década de distancia ha generado debates sobre cómo se integra la sostenibilidad en las relaciones entre el turismo y la conservación, y los resultados sugieren que para lograr la sostenibilidad, se deben priorizar el compromiso de los actores locales en una visión compartida para el desarrollo turístico, abordar las preocupaciones de la comunidad, especialmente en lo referente a los servicios básicos, la salud y la educación, y evaluar y gestionar el equilibrio entre el turismo de alta gama regulado y el turismo informal de bajo costo.\n",
      "\n",
      "Según Allauca et al. [2], las Islas Galápagos, reconocidas como Patrimonio de la Humanidad, albergan una importante diversidad agrícola, aunque esta no ha sido completamente descrita. La presencia de especies agrícolas, medicinales y nativas en los cuatro islas habitadas (Isabela, Santa Cruz, Floreana y San Cristobal) es significativa, y se ven amenazadas por factores como el turismo, el cambio climático y las especies invasoras.\n",
      "\n",
      "Las percepciones de los residentes con respecto a la actividad turística en Puerto Ayora, Galápagos, son complejas, según Mosquera y Barriga [3].  Estos autores señalan que los impactos socioambientales del turismo, desde su inicio, generan cambios en las actitudes de los residentes, influenciados por factores ambientales, económicos, socioculturales y políticos, y que la planificación y el desarrollo turístico deben basarse en los tres pilares de la sostenibilidad.\n",
      "\n",
      "Jamieson y Astudillo [4] destacan que el uso de las islas como colonia penal, ejemplificado por la hacienda El Progreso entre 1878 y 1904, ilustra cómo las autoridades han abordado la criminalidad desde antes del surgimiento del estado moderno.  Esta operación demuestra que el concepto patriarcal de la hacienda continuó desempeñando un papel clave en la disciplina de la criminalidad percibida en América Latina a finales del siglo XIX, fuera de los roles de las fuerzas militares y policiales.\n",
      "\n",
      "La situación de las Islas Galápagos se considera crítica/en peligro por organizaciones como WWF y Conservation International [5], debido a la amenaza de especies invasoras, el cambio climático, el desarrollo económico y el turismo.  Estas amenazas, junto con la necesidad de controlar el turismo, impulsan iniciativas de biocontrol y otras medidas para proteger los ecosistemas terrestres, incluyendo manglares, matorral deciduous y bosques de montaña.\n",
      "\n",
      "Finalmente, Molina y Barriga [6] argumentan que, a pesar de los esfuerzos para promover el turismo sostenible, es difícil clasificar a las Islas Galápagos como un destino completamente sostenible, especialmente en términos de turismo basado en la comunidad.  La evaluación de las medidas adoptadas por los establecimientos hoteleros, incluyendo los instrumentos de gestión sostenible, revela que algunos se limitan a una estrategia de \"greenwashing\" para aparentar ser sostenibles, lo que subraya la necesidad de una planificación turística basada en los tres pilares de la sostenibilidad.\n",
      "\n",
      "=== Auditoría de citas usadas ===\n",
      "Citas usadas en el texto:\n",
      " - [1] Diego Quiroga Ferri; Diana V. Burbano; Thomas C. Meredith; Juan Carlos Valdivieso; Juan Carlos Izurieta; \"“Rethink and reset” tourism in the Galapagos Islands: Stakeholders' views on the sustainability of tourism development\"; https://doi.org/10.1016/j.annale.2022.100057\n",
      " - [2] Joanna Allauca; Luis O. Escudero; Angélica C. Zapata; Víctor Hugo Barrera; Álvaro R. Monteros-Altamirano; Marilú Valverde; \"Agrobiodiversity in four Islands of the Galapagos Archipelago, Ecuador\"; https://doi.org/10.1007/s10722-023-01759-6\n",
      " - [3] Paula Moya Mosquera; Andrea Muñoz Barriga; \"Residents, conservation, development and tourism in Galapagos\"; https://doi.org/10.4067/S0718-34022022000300201\n",
      " - [4] Ross Jamieson; Fernando Astudillo; \"The Galápagos as penal colony: Exile, peonage, and state control at the Hacienda El Progreso, 1878–1904\"; https://doi.org/10.1177/14624745211013100\n",
      " - [5] Gonzalo Francisco Rivas-Torres; Nejem Raheem; \"Imperiled Ecosystems: Galápagos Scrub\"; https://doi.org/10.1016/B978-0-12-821139-7.00196-3\n",
      " - [6] Samanta Victoria Torres Molina; Andrea Muñoz Barriga; \"SUSTAINABLE MANAGEMENT APPLIED TO THE HOTEL SECTOR IN GALÁPAGOS\"; https://doi.org/10.18601/01207555.n31.10\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, textwrap, requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# =================== Configuración ===================\n",
    "OLLAMA_HOST       = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL      = os.environ.get(\"OLLAMA_MODEL\", \"gemma3:4b\")\n",
    "TOP_CONTEXT       = int(os.environ.get(\"RAG_TOP_CONTEXT\", \"6\"))\n",
    "MAX_INPUT_CHARS   = int(os.environ.get(\"RAG_MAX_INPUT_CHARS\", \"7000\"))\n",
    "MAX_CHUNK_CHARS   = int(os.environ.get(\"RAG_MAX_CHUNK_CHARS\", \"900\"))\n",
    "TEMPERATURE       = float(os.environ.get(\"RAG_TEMPERATURE\", \"0.2\"))\n",
    "MAX_NEW_TOKENS    = int(os.environ.get(\"RAG_MAX_NEW_TOKENS\", \"768\"))  # ↑ un poco por seguridad\n",
    "HTTP_TIMEOUT_SECS = int(os.environ.get(\"RAG_HTTP_TIMEOUT_SECS\", \"300\"))\n",
    "DO_TRIM_ABSTRACT  = os.environ.get(\"RAG_TRIM_ABSTRACT\", \"1\") == \"1\"\n",
    "\n",
    "# =================== Utilidades generales ===================\n",
    "def _safe_str(x) -> str:\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, float) and x != x:  # NaN\n",
    "        return \"\"\n",
    "    try:\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _id_for_row(row: pd.Series) -> str:\n",
    "    for k in (\"doi\", \"scopus_id\", \"vec_id\"):\n",
    "        vs = _safe_str(row.get(k)).strip()\n",
    "        if vs:\n",
    "            return vs\n",
    "    return f\"row{row.name}\"\n",
    "\n",
    "def _first_nonempty(row: pd.Series, cols: List[str]) -> str:\n",
    "    for c in cols:\n",
    "        v = _safe_str(row.get(c, \"\"))\n",
    "        if v.strip():\n",
    "            return v.strip()\n",
    "    return \"\"\n",
    "\n",
    "def _shorten(txt, lim: int) -> str:\n",
    "    s = _safe_str(txt)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return (s[:lim-3] + \"...\") if len(s) > lim else s\n",
    "\n",
    "# ---------- Autores/Año para mención autoral ----------\n",
    "def _split_authors(raw: str) -> List[str]:\n",
    "    s = _safe_str(raw)\n",
    "    if not s.strip(): return []\n",
    "    s = s.replace(\"|\", \";\").replace(\" and \", \";\")\n",
    "    parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "    if len(parts) <= 1:\n",
    "        parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def _last_name(name: str) -> str:\n",
    "    n = _safe_str(name).strip()\n",
    "    if not n: return \"\"\n",
    "    if \",\" in n: return n.split(\",\")[0].strip()\n",
    "    tokens = n.split()\n",
    "    return tokens[-1].strip() if tokens else n\n",
    "\n",
    "def _format_authors_for_mention(raw: str, max_names: int = 2) -> Optional[str]:\n",
    "    authors = _split_authors(raw)\n",
    "    if not authors: return None\n",
    "    last_names = [_last_name(a) for a in authors if _last_name(a)]\n",
    "    if not last_names: return None\n",
    "    if len(last_names) == 1: return last_names[0]\n",
    "    if len(last_names) == 2: return f\"{last_names[0]} y {last_names[1]}\"\n",
    "    return f\"{last_names[0]} et al.\"\n",
    "\n",
    "def _extract_year(row: pd.Series) -> Optional[str]:\n",
    "    for c in [\"year\", \"publication_year\", \"cover_date\", \"date\"]:\n",
    "        val = _safe_str(row.get(c))\n",
    "        m = re.search(r\"(19|20)\\d{2}\", val)\n",
    "        if m: return m.group(0)\n",
    "    return None\n",
    "\n",
    "# =================== Construcción de bloques ===================\n",
    "def build_context_blocks(df_reranked: pd.DataFrame,\n",
    "                         top_k: int = TOP_CONTEXT,\n",
    "                         max_chunk_chars: int = MAX_CHUNK_CHARS) -> List[Dict]:\n",
    "    if df_reranked is None or len(df_reranked) == 0:\n",
    "        raise ValueError(\"df_reranked está vacío.\")\n",
    "    cols_title = [c for c in [\"title\",\"chunk_title\"] if c in df_reranked.columns] or [\"title\"]\n",
    "    cols_abs   = [c for c in [\"abstract\",\"chunk_text\",\"summary\"] if c in df_reranked.columns] or [\"abstract\"]\n",
    "    blocks = []\n",
    "    for i in range(min(top_k, len(df_reranked))):\n",
    "        row = df_reranked.iloc[i]\n",
    "        title  = _first_nonempty(row, cols_title) or \"Sin título\"\n",
    "        body   = _first_nonempty(row, cols_abs)\n",
    "        if DO_TRIM_ABSTRACT: body = _shorten(body, max_chunk_chars)\n",
    "        authors_raw = _safe_str(row.get(\"authors\", \"\"))\n",
    "        year  = _extract_year(row)\n",
    "        doi_raw = _safe_str(row.get(\"doi\", \"\"))\n",
    "        blocks.append({\n",
    "            \"cite_id\": _id_for_row(row),   # para [n]\n",
    "            \"title\": title,\n",
    "            \"text\": body,\n",
    "            \"authors_mention\": _format_authors_for_mention(authors_raw),\n",
    "            \"authors_raw\": authors_raw,    # para 'Fuentes'\n",
    "            \"year\": year,\n",
    "            \"doi_raw\": doi_raw             # para 'Fuentes'\n",
    "        })\n",
    "    return blocks\n",
    "\n",
    "# =================== Fuentes: DOI y autores ===================\n",
    "def _doi_url(doi_raw: str) -> str:\n",
    "    doi = _safe_str(doi_raw).strip()\n",
    "    if not doi: return \"s/d\"\n",
    "    return doi if doi.lower().startswith(\"http\") else f\"https://doi.org/{doi}\"\n",
    "\n",
    "def _authors_cite_line(raw_authors: str) -> str:\n",
    "    names = _split_authors(_safe_str(raw_authors))\n",
    "    if not names: return \"Autor(es) no disponibles\"\n",
    "    return \"; \".join([_safe_str(n).strip() for n in names if _safe_str(n).strip()])\n",
    "\n",
    "def render_fuentes_from_blocks(blocks: List[Dict]) -> str:\n",
    "    lines = []\n",
    "    for i, b in enumerate(blocks, start=1):\n",
    "        autores = _authors_cite_line(b.get(\"authors_raw\", \"\"))\n",
    "        titulo  = _safe_str(b.get(\"title\", \"Sin título\"))\n",
    "        doiurl  = _doi_url(b.get(\"doi_raw\", \"\"))\n",
    "        lines.append(f\"[{i}] {autores}; \\\"{titulo}\\\"; {doiurl}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# =================== Prompt con instrucción de mención autoral ===================\n",
    "def compose_prompt(query: str, blocks: List[Dict], max_chars: int = MAX_INPUT_CHARS) -> str:\n",
    "    header = (\n",
    "        \"Eres un asistente académico. Responde de forma clara, concisa y basada en evidencia, \"\n",
    "        \"en español y estilo APA (7ª). Utiliza exclusivamente los fragmentos proporcionados. \"\n",
    "        \"Cada párrafo debe incluir una mención autoral explícita (‘según <autor>’ o ‘de acuerdo con <autor>’) \"\n",
    "        \"seguida de la cita en corchetes [n]. Si no hay autor disponible, usa ‘según la fuente [n]’. \"\n",
    "        \"No imprimas tu propia sección de 'Fuentes'; solo escribe el cuerpo con citas [n].\\n\\n\"\n",
    "        f\"Pregunta: {query}\\n\\n\"\n",
    "        \"Fragmentos (con autor/año si disponible):\\n\"\n",
    "    )\n",
    "    parts = []\n",
    "    for i, b in enumerate(blocks, start=1):\n",
    "        autor_m = b.get(\"authors_mention\") or f\"fuente [{i}]\"\n",
    "        anio    = f\" ({b['year']})\" if b.get(\"year\") else \"\"\n",
    "        head    = f\"[{i}] {b['title']} — Autor(es): {autor_m}{anio}\"\n",
    "        parts.append(f\"{head}\\n{_safe_str(b['text'])}\\n\")\n",
    "\n",
    "    footer = (\n",
    "        \"\\nInstrucciones de redacción:\\n\"\n",
    "        f\"- Debes usar las referencias [1..{len(blocks)}] tal como están definidas.\\n\"\n",
    "        \"- No inventes datos fuera de los fragmentos.\\n\"\n",
    "        \"- Cada párrafo debe incluir al menos una mención autoral y su [n] correspondiente.\\n\"\n",
    "        \"- No generes la sección 'Fuentes'.\\n\"\n",
    "    )\n",
    "    # Incluimos igualmente las 'Fuentes' en el prompt como guía, pero pedimos explícitamente que NO las imprima.\n",
    "    fuentes_prompt = render_fuentes_from_blocks(blocks)\n",
    "    prompt = header + \"\\n\".join(parts) + footer + \"\\nFuentes (guía, no imprimir):\\n\" + fuentes_prompt\n",
    "    return prompt[:max_chars]\n",
    "\n",
    "# =================== Generación vía servicio HTTP de Ollama (sin fallbacks) ===================\n",
    "def generate_with_ollama_http(prompt: str,\n",
    "                              model: str = OLLAMA_MODEL,\n",
    "                              temperature: float = TEMPERATURE,\n",
    "                              max_new_tokens: int = MAX_NEW_TOKENS,\n",
    "                              base_url: str = OLLAMA_HOST,\n",
    "                              timeout: int = HTTP_TIMEOUT_SECS) -> str:\n",
    "    url = f\"{base_url.rstrip('/')}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente académico que escribe en español (APA 7ª), conciso y basado en evidencia.\"},\n",
    "            {\"role\": \"user\",   \"content\": _safe_str(prompt)}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"temperature\": float(temperature),\n",
    "            \"num_predict\": int(max_new_tokens),\n",
    "            # Cortamos cuando el modelo intente empezar su propia sección de fuentes:\n",
    "            \"stop\": [\"\\nFuentes\", \"\\nFUENTES\", \"\\nReferences\", \"\\nREFERENCIAS\"]\n",
    "        },\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(url, json=payload, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Fallo al invocar el servicio HTTP de Ollama. \"\n",
    "            \"Verifica daemon, modelo descargado y conectividad.\\nDetalle: {e}\"\n",
    "        )\n",
    "    content = _safe_str(data.get(\"message\", {}).get(\"content\", \"\"))\n",
    "    if not content.strip():\n",
    "        raise RuntimeError(\"La respuesta de Ollama está vacía.\")\n",
    "    return content\n",
    "\n",
    "# =================== Auditoría de citas usadas ===================\n",
    "def extract_used_refs(answer_text: str, n_max: int) -> List[int]:\n",
    "    \"\"\"Extrae números [n] usados en el texto, filtrando por rango 1..n_max y preservando orden de aparición.\"\"\"\n",
    "    nums = [int(m.group(1)) for m in re.finditer(r\"\\[(\\d+)\\]\", _safe_str(answer_text))]\n",
    "    seen, used = set(), []\n",
    "    for n in nums:\n",
    "        if 1 <= n <= n_max and n not in seen:\n",
    "            used.append(n); seen.add(n)\n",
    "    return used\n",
    "\n",
    "def render_used_refs_report(answer_text: str, blocks: List[Dict]) -> str:\n",
    "    used = extract_used_refs(answer_text, len(blocks))\n",
    "    if not used:\n",
    "        return \"No se detectaron citas [n] en el texto.\"\n",
    "    lines = [\"Citas usadas en el texto:\"]\n",
    "    for n in used:\n",
    "        b = blocks[n-1]\n",
    "        lines.append(f\" - [{n}] {_authors_cite_line(b.get('authors_raw',''))}; \\\"{_safe_str(b.get('title','Sin título'))}\\\"; {_doi_url(b.get('doi_raw',''))}\")\n",
    "    # Aviso si el modelo no usó alguna fuente disponible\n",
    "    missing = [i for i in range(1, len(blocks)+1) if i not in used]\n",
    "    if missing:\n",
    "        lines.append(f\"No usadas: {missing}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# =================== Ejecución (ejemplo) ===================\n",
    "if 'reranked' not in globals():\n",
    "    raise RuntimeError(\"No encuentro 'reranked'. Ejecuta primero tu celda de re-ranking.\")\n",
    "\n",
    "query  = os.environ.get(\"QUERY\", \"el turismo en las islas galagos que tan bueno es?\")\n",
    "blocks = build_context_blocks(reranked, top_k=TOP_CONTEXT, max_chunk_chars=MAX_CHUNK_CHARS)\n",
    "prompt = compose_prompt(query, blocks, max_chars=MAX_INPUT_CHARS)\n",
    "\n",
    "print(\"=== Prompt (vista previa) ===\")\n",
    "print(textwrap.shorten(prompt, width=1000, placeholder=\" ...\"))\n",
    "\n",
    "answer = generate_with_ollama_http(prompt)\n",
    "print(\"\\n=== Respuesta (LLM - Ollama HTTP) ===\\n\")\n",
    "print(answer)\n",
    "\n",
    "# Resumen de citas usadas por el modelo\n",
    "print(\"\\n=== Auditoría de citas usadas ===\")\n",
    "print(render_used_refs_report(answer, blocks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89214b3",
   "metadata": {},
   "source": [
    "# Evaluación"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
