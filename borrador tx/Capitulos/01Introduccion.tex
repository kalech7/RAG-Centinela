\chapter{Introducción}

\section{Planteamiento del problema}

\section{Justificación}

\section{Justificación Metodológica}

\section{Objetivos}
\subsection{Objetivo general}
Desarrollar e implementar un sistema RAG que mejore el desempeño del buscador de la plataforma
Centinela, permitiendo recuperar información científica relevante y generar respuestas automáticas
de valor para el usuario.
\subsection{Objetivos específicos}
\begin{itemize}[align=left, label=-]
    \item Realizar una revisión sistemática de la literatura sobre metodologías y/o frameworks para la implementación de RAG.
    \item Diseñar e implementar la arquitectura técnica del sistema RAG utilizando modelos de recuperación y generación de texto.
    \item Evaluar el sistema RAG desarrollado mediante métricas estándar.
\end{itemize}

\section{Alcance}

\section{Marco Teórico}

\section{Revision de literatura}
En los últimos años, la evolución de los modelos de lenguaje de gran escala (Large Language Models, LLM) han redefinido el procesamiento del lenguaje natural
e impulsado nuevas líneas de investigación. 
Sin embargo, estos modelos dependen únicamente de los datos empleados durante su entrenamiento, lo que limita su capacidad para ofrecer respuestas actualizadas, verificables y 
contextualizadas. En respuesta a esta limitación surge el enfoque de Retrieval-Augmented Generation (RAG), el cual combina la recuperación de información con la generación de 
lenguaje natural, logrando mejorar la precisión, la coherencia y la actualidad de las respuestas producidas por los modelos.

Dada la creciente relevancia de los LLM, resulta necesario llevar a cabo una revisión exhaustiva de la literatura que permita consolidar los avances recientes y evaluar los desafíos aún presentes.
En esta sección se presenta un análisis estructurado de la literatura disponible, considerando tanto los fundamentos conceptuales de RAG como sus fases de desarrollo, 
aplicaciones y el futuro. Para ello, el proceso de revisión se organiza en las fases que se presentan a continuación, las cuales buscan garantizar la consistencia, validez 
y pertinencia de la evidencia obtenida.

\subsection{Propósito y objetivos de la revisión}
El propósito de esta revisión es consolidar la información disponible sobre los RAG, abordando su estudio desde los fundamentos hasta las fases de desarrollo. 
Se inicia con su definición y arquitectura, para luego profundizar en las etapas clave del proceso: Extracción del corpus, preprocesamiento, vectorización, recuperación de información, 
evaluación, almacenamiento en bases vectoriales y generación de resultados. Asimismo, se examinan los paradigmas, las métricas de evaluación y el futuro de RAG.
Durante esta revision se busca lograr el objetivo general de proporcionar un panorama global y actualizado sobre los RAG,
exponiendo sus fundamentos, desarrollo y aplicación.

\subsection{Criterios de inclusión y exclusión}
Se incluyen únicamente revisiones sistemáticas y metaanálisis publicados entre 2018 y 2025, en inglés o español, dado que la producción 
científica en el área comenzó a incrementarse a partir de 2018, con base en información de Lens.org\footnote{Es una plataforma abierta
para la búsqueda, análisis y visualización de literatura científica y patentes. Accesible en: \href{https://www.lens.org/}{Lens.org}}, este incremento coincide
con la popularización 
de los modelos de lenguaje basados en transformers\footnote{Se atribuye a hitos como BERT (2018), GPT-2 (2019) y T5 (2020), que impulsaron un avance 
en la investigación del procesamiento del Lenguaje Natural}.
Los estudios deben provenir de fuentes confiables y ser, a su vez, revisados por 
un experto. Se da preferencia a aquellos que presenten una cobertura amplia de los temas más relevantes para el objeto de estudio.  

Se excluyen las revisiones narrativas, los documentos que carezcan de transparencia en sus métodos de búsqueda o síntesis, así como las publicaciones que no estén 
directamente relacionadas con el objeto de estudio delimitado.

\subsection{Identificación del estudio semilla y selección de revisiones relevantes}
El proceso de búsqueda se inicia con la identificación de dos estudios semilla, extraídos de Google Scholar mediante los parámetros “retrieval information” y “retrieval augmented generation”. 
Debido al análisis realizado en Lens.org, se estableció el filtro de 2018 a 2025, ya que se observa que a partir de 2018 el término retrieval-augmented generation 
comenzó a adquirir una relevancia en la literatura científica, mostrando interés de la comunidad investigadora hasta la actualidad.

El primer estudio seleccionado fue Information Retrieval: Recent Advances and Beyond (Hambarde \& Proença, 2023), publicado en IEEE Access. 
Este trabajo constituye una revisión exhaustiva de la recuperación de información, abarcando desde los métodos tradicionales hasta los enfoques 
basados en deep learning y transformers, por lo que resulta un punto de partida principal para explorar la literatura reciente y relevante.

El segundo estudio semilla corresponde al artículo Retrieval-Augmented Generation for Large Language Models (Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun \& Wang, 2023), publicado en
arXiv,
el cual presenta un marco conceptual y aplicado sobre la integración de recuperación de información y modelos generativos de gran escala. Su incorporación permite 
establecer una base teórica para contextualizar el análisis de las revisiones seleccionadas.

A partir de estos dos estudios semilla, y aplicando los criterios de inclusión y exclusión previamente definidos, se identificaron \textcolor{blue}{25} revisiones 
relevantes que cumplen con los criterios establecidos. 
Estas revisiones constituyen la base para el análisis y síntesis en el presente trabajo.



\subsection{Valoracion de la evidencias y extracion de la infomacion}
De los estudios seleccionados se procede a realizar un análisis, 
con el fin de excluir aquellos artículos que no cumplen con los criterios establecidos 
o que presentan un nivel de profundidad insuficiente para los objetivos de la revisión. 
La selección final de los estudios se realiza en consenso con expertos en el área, 
garantizando así la pertinencia y relevancia de la evidencia incluida.Para la organización, codificación y síntesis de la 
información se usa ATLAS.ti \footnote{Scientific Software Development GmbH. Disponible en: \href{https://atlasti.com/es}{Atlas.ti}} que facilitará la estructuración de los hallazgos.





% para resuemir la informacion se uso (herramienta) atlas por ejemplo

\subsection{Síntesis y representación de resultados}
Con la literatura seleccionada se identificó la hoja de ruta que se presenta a continuación en la Figura \ref{fig:secciones-rag}.

% --- Grafico resumen

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
  % distancia vertical entre pastillas
  \def\step{-1.5}

  % y inicial (arriba)
  \def\y{0}

  \pill{\y}{softcream}{I}{Fundamentos}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softpink}{II}{Arquitectura}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softblue}{III}{Fases de Implementación}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softpeach}{IV}{Paradigmas}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softyellow}{V}{Evaluación y metricas}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softgreen}{VI}{Futuro de RAG}
  
\end{tikzpicture}
\end{center}
\caption{Resumen esquemático de RAG}
\label{fig:secciones-rag}
\end{figure}

A partir de esta hoja de ruta se desarrolla un esquema más detallado, en el que se expone primero exploraremos su teoría, 
características y aplicaciones, como se muestra en la Fig \ref{fig:Fudamentos}. Luego profundizamos en su arquitectura en la cual 
se describe cada uno de los componentes que lo conforman (retriever, augmented y generation) y las variantes y mejoras que existen de cada uno. 
Posteriormente, se detalla el proceso de implementación, desde la preparación de datos hasta el componente de generación, incluyendo las técnicas y herramientas más relevantes.
A continuación en la Fig , se examinan los paradigmas de RAG, dando a conocer los tipos de paradigmas y sus clases, para luego en la Fig , se presentan las métricas y evaluadores automáticos 
utilizados en la evaluación de sistemas RAG, así como las consideraciones éticas y de equidad que deben tenerse en cuenta.
Finalmente, se discuten las tendencias emergentes, los desafíos que actualmente se tienen y futuras direcciones que podrían tomar los sistemas RAG.




\subsubsection{Fundamentos}

% --- Grafico fundamentos
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
\tikzset{
  mindoval/.style={
    rounded corners=18pt,
    draw=black!70,
    line width=0.7pt,
    minimum width=3.2cm,
    minimum height=1.1cm,
    inner sep=6pt,
    fill=white      % <<< todos los nodos en blanco
  },
  mindcenter/.style={
    rounded corners=8pt,
    draw=black!70,
    line width=0.9pt,
    minimum width=3.6cm,
    minimum height=1.1cm,
    inner sep=4pt,
    fill=softrose,  % <<< centro en morado
    font=\bfseries
  },
  mindapp/.style={
    mindoval,
    fill=softblue   % <<< nodo aplicación en azul
  },
  mindarrow/.style={-latex, line width=0.9pt}
}

% --- Nodo central
\node[mindcenter] (fund) at (0,0) {Fundamentos};

% --- Nodos arriba
\node[mindoval] (aport)  at (0,3) {4. Aportes de RAGs};
\node[mindoval] (ragft)  at (5,0) {5. RAG vs fine tuning};

% --- Nodos izquierda
\node[mindoval] (rolrag) at (-5,2.5) {3. Rol de RAG};
\node[mindoval] (qrag)   at (-5,0.75) {2. ¿Qué es RAG?};
\node[mindoval] (qllm)   at (-5,-1.0) {1. ¿Qué es un LLM?};

% --- Nodos abajo
\node[mindapp, minimum width=3.0cm] (apli) at (0,-3.0) {6. Aplicación};
\node[mindoval] (texto)  at (-5,-4.5) {Texto};
\node[mindoval] (img)    at (-2,-6) {Imagen};
\node[mindoval, minimum width=3.6cm] (audio)  at (2,-6) {Audio y video};
\node[mindoval] (codigo) at (5,-4.5) {Código};

% --- Flechas (tocando bordes de nodos)
\draw[mindarrow] (fund.north) -- (aport.south);
\draw[mindarrow] (fund.east) -- (ragft.west);
\draw[mindarrow] (fund.west) -- (rolrag.east);
\draw[mindarrow] (fund.west) -- (qrag.east);
\draw[mindarrow] (fund.west) -- (qllm.east);
\draw[mindarrow] (fund.south) -- (apli.north);

\draw[mindarrow] (apli.west) -- (texto.east);
\draw[mindarrow] (apli) -- (img.north);
\draw[mindarrow] (apli) -- (audio.north);
\draw[mindarrow] (apli.east) -- (codigo.west);

\end{tikzpicture}
\end{center}
\caption{Fundamentos de RAG}
\label{fig:Fudamentos}
\end{figure}

En esta subsección se presentan los fundamentos teóricos de Retrieval Augmented Generation (RAG), 
comenzando con la definición de los modelos de lenguaje de gran escala (LLMs) y su relación. Se expone 
también el papel que desempeña RAG, los principales aportes que ha generado en distintos ámbitos y su diferenciación frente al 
fine tuning. Finalmente, se introduce su aplicación práctica, lo que permite comprender la importancia y el impacto que RAG tiene en 
la actualidad.

\paragraph{Que es un LLM}
Son modelos de inteligencia artificial (IA) basados en la arquitectura transformer, entrenados con grandes volumnes de datos textuales con el objetivo de aprender representaciones
contextuales del lenguaje. Según \textcite{casola2022pretrained}, estos modelos utilizan técnicas de pre-entrenamiento no supervisado para captar patrones lingüísticos y semánticos,
lo que permite que posteriormente puedan ajustarse a tareas específicas como clasificación de texto, análisis de sentimientos, traducción automática, reconocimiento de entidades
o respuesta a preguntas. Ejemplos destacados son \textit{BERT, RoBERTa, ALBERT, XLNet, DistilBERT y GPT-3}, que han mostrado rendimientos sobresalientes en diversas aplicaciones de procesamiento de lenguaje natural (NLP).

De acuerdo con \textcite{ramdurai2025llm}, los LLMs también se definen como una clase de modelos de IA capaces de procesar y generar texto de forma similar al lenguaje humano,
gracias al uso de redes neuronales profundas y la capacidad de aprender no solo gramática y relaciones entre palabras, sino también aspectos más complejos como humor, 
tono emocional y contexto. Entrenados en enormes corpus de datos provenientes de libros, artículos y sitios web, estos modelos pueden responder preguntas, redactar ensayos,
traducir, resumir y crear contenido de manera autónoma. Ejemplos recientes incluyen \textit{GPT-4, T5, XLNet y PaLM}, los cuales demuestran su versatilidad en tareas avanzadas
de NLP y en sistemas aplicados en diferentes industrias. 

\paragraph{Que es un RAG}
Según \textcite{han2024rag}, Retrieval-Augmented Generation (RAG) es una técnica que integra la capacidad generativa de los modelos 
de lenguaje con la precisión de la recuperación de información en tiempo real. En lugar de basarse únicamente en el conocimiento almacenado en
los parámetros durante el entrenamiento, RAG permite consultar repositorios externos como bases de datos o motores de búsqueda para obtener
documentos relevantes y actualizados. Estos se incorporan al prompt del usuario, lo que fundamenta la respuesta en fuentes 
verificables y disminuye los problemas de errores y alucinaciones que suelen presentarse en los modelos de lenguaje de gran escala.  




\subsubsection{Arquitectura}
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
\tikzset{
  mindoval/.style={
    rounded corners=18pt,
    draw=black!70,
    line width=0.7pt,
    minimum width=3.6cm,
    minimum height=1.1cm,
    inner sep=6pt,
    fill=white
  },
  mindcenter/.style={
    rounded corners=8pt,
    draw=black!70,
    line width=0.9pt,
    minimum width=4.0cm,
    minimum height=1.2cm,
    inner sep=4pt,
    fill=softrose,
    font=\bfseries
  },
  mindarrow/.style={-latex, line width=0.9pt}
}

% --- Nodo central
\node[mindcenter] (arquitectura) at (0,0) {Arquitectura};

% --- Nodos conectados
\node[mindoval] (retrieval) at (-5,-3) {Retrieval};
\node[mindoval] (augmented) at (0,-3) {Augmented};
\node[mindoval] (generation) at (5,-3) {Generation};

% --- Flechas
\draw[mindarrow] (arquitectura.south west) -- (retrieval.north);
\draw[mindarrow] (arquitectura.south) -- (augmented.north);
\draw[mindarrow] (arquitectura.south east) -- (generation.north);

\end{tikzpicture}
\end{center}
\caption{Componentes de RAG}
\label{fig:arquitectura_rag}
\end{figure}

RAG se compone de tres fases: recuperación, augmentation y generación (ver Figura~\ref{fig:arquitectura_rag}). Como lo menciona \textcite{gao2023rag}, primero se 
localizan documentos relevantes para la consulta; luego, se enriquece la entrada del usuario con esos textos; finalmente, el modelo produce 
una respuesta basada tanto en su conocimiento interno como en la información recuperada. Gracias a este enfoque, RAG incrementa la exactitud 
de las respuestas, facilita la actualización del conocimiento sin necesidad de reentrenar el modelo y mejora la transparencia al permitir la 
cita de fuentes. Es una de las técnicas más relevantes en tareas que requieren gran 
cantidad de conocimiento, como el ámbito médico, legal o de investigación científica.  


\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{l l X}
\toprule
\textbf{Categoría} & \textbf{Subcategoría / Tipo} & \textbf{Técnicas} \\
\midrule
Indexing     & Vector DB               & FAISS, Annoy, Milvus, Weaviate, Pinecone, Qdrant\\
             & Algoritmos de búsqueda  & Approximate Nearest Neighbors, Locality-Sensitive Hashing \\
\midrule
Enhancements & Reranking               & Re2G, AceCoder, XRICL, monoT5\\
             & Retriever Finetuning    & REPLUG, APICoder, EDITSUM \\
             & Hybrid Search           & RAP-Gen, BlendedRAG, ReACC\\
             & Chunk Optimization      & RAPTOR, LlamaIndex  \\
             & Recursive Retrieval     & ReAct, RATP \\
             & Query Reformulation     &  HyDE \\
\midrule
Tipos de retrievers & Sparse            & BM25, TF-IDF \\
                    & Dense             & Embeddings (ej. BERT, OpenAI, etc.) \\
                    & Others            & Modelos híbridos u otros \\
\bottomrule
\end{tabularx}
\caption{Componente Retriever}
\end{table}


Indexing es encargado de organizar y representar la informacion de forma eficiente. En este sentido los vector databases (VDBs) han aumentado su popularidad ya que permiten alamacenar 
vectores de alta dimensionalidad y permiten realizar busquedas por similitud semantica. Como explica \textcite{joshi2025vector} estas bases de datos resultan esenciales para 
en aplicacion de inteligencia artificial generativa, ya que superan las limitacion de las bases realcionales en el manejo de datos no estructurados.
Los VDBs integran mecanismos de busqueda aproximada de vecinos mas cercanos (ANNS) lo que posibilita consultas rapidas incluso sobre miles de objetos. Ademas, como lo menciona
\textcite{ma2025vector} incorporan técnicas de optimización como particionamiento, sharding, cachés y replicación para garantizar escalabilidad y baja latencia en entornos distribuidos
Locality-Sensitive Hashing (LSH) es una técnica de indexación ampliamente utilizada en VDBs para acelerar la búsqueda de vecinos aproximados en espacios de alta 
dimensionalidad. A diferencia de los esquemas de hashing tradicionales, cuyo objetivo es dispersar uniformemente los datos para minimizar colisiones, LSH está diseñado 
para maximizar la probabilidad de que vectores similares se asignen al mismo bucket de hash \parencite{ma2025vector}. Según esta técnica, la preservación de la localidad 
se consigue mediante funciones de hash que reflejan la similitud entre vectores en colisiones más frecuentes dentro del espacio reducido. De esta manera, al realizar una 
consulta, el sistema solo necesita comparar el vector de entrada con aquellos almacenados en el mismo o en buckets cercanos, reduciendo drásticamente la complejidad 
computacional de la búsqueda \parencite{han2023vector}. Estas características explican por qué los VDBs se han consolidado como una infraestructura fundamental en el soporte
de sistemas RAG.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{l l >{\raggedright\arraybackslash}X}
\toprule
\textbf{Categoría} & \textbf{Subcategoría / Tipo} & \textbf{Técnicas / Ejemplos} \\
\midrule
Tipos   & Pre-training          & knowledge graph embeddings \\
        & Fine-tuning           & REPLUG,UPRISE \\
        & Inference             & Retrieve-then-Read-then-Revise (RARR), Lost in the Middle  \\
\midrule
Data    & Structured            & Knowledge Graphs \\
        & Unstructured          & open corpus (Common Crawl, PubMed, etc.) \\
        & LLM generated content & self-retrieval \\
\midrule
Process & Once                  & document augmentation \\
        & Iterative             & pseudo-relevance feedback (PRF) \\
        & Adaptative            & SKR (Selective Knowledge Retrieval)\\
\bottomrule
\end{tabularx}
\caption{Componente Augmentation}
\end{table}
Augmentation es el proceso el cual un modelo de lenguaje incorpora información adicional 
ya sea externa, como documentos, bases de conocimiento o corpus abiertos, o procesada internamente en diferentes etapas de su funcionamiento.
Los autores coinciden en que esta integración cumple objetivos clave: mejorar la precisión de las respuestas al aportar evidencia relevante, 
reducir las alucinaciones al contrastar el conocimiento implícito del modelo con fuentes verificables, y actualizar el conocimiento de los LLMs sin 
necesidad de reentrenarlos desde cero, ya que el acceso a información recuperada permite mantenerlos al día en dominios dinámicos como ciencia, medicina o derecho.

Los tipos de augmentation, \textcite{zhao2024rag} señalan que se puede aplicar en 3 momentos distintos. Durante el pre-training, se integran representaciones estructuradas 
como knowledge graph embeddings que dotan al modelo de memoria explícita sobre entidades y relaciones. En la etapa de fine-tuning, técnicas como REPLUG y UPRISE 
permiten alinear mejor el recuperador y el generador en dominios específicos. Finalmente, en la inferencia, se aplican métodos sin reentrenamiento: por ejemplo, 
RARR (Retrieve-then-Read-then-Revise) refina las respuestas con evidencia recuperada, mientras que la mitigación de Lost in the Middle reorganiza documentos recuperados
para que el modelo aproveche mejor la ventana de contexto.

Con respecto a los datos, \textcite{fan2024ragllm} detancan tres clases. La información estructurada, como los Knowledge Graphs, es fundamental para tareas de razonamiento 
factual ya que permite modelar entidades y relaciones explícitas. La información no estructurada, como corpus abiertos (Common Crawl, PubMed, Wikipedia), se ha vuelto estándar en open-domain QA.
Tal como explican \textcite{gao2023rag} estos corpus aportan amplitud temática, pero también requieren mecanismos de filtrado, chunking y re-ranking 
para evitar ruido y mitigar el problema de Lost in the Middle. Finalmente, surge la categoría de contenido generado por LLM, donde el propio modelo actúa 
como fuente en esquemas de self-retrieval, generando y reutilizando conocimiento de manera autónoma. En este caso, los LLMs generan documentos intermedios, 
hipótesis o representaciones que se utilizan posteriormente como consultas o evidencia emplean un módulo crítico que evalúa si es necesario recuperar información externa o 
si el propio contenido generado basta para resolver la tarea. Gracias a este enfoque , como subraya \textcite{fan2024ragllm} abre la posibilidad de que los modelos se autocomplementen y 
reutilicen su conocimiento previo sin depender exclusivamente de bases externas.

Los procesos de \textit{augmentation} se los clasifica en tres modalidades según \textcite{zhao2024rag}. 
Augmentation puede aplicarse una sola vez (Once), como en el caso de la document augmentation, 
donde se enriquece directamente la entrada con información adicional antes de la generación. 
Otra modalidad es la iterativa, que emplea técnicas como el pseudo-relevance feedback (PRF), 
mediante el cual la consulta inicial se reformula a partir de los resultados recuperados, repitiendo el ciclo 
para refinar la relevancia. 
La modalidad adaptativa es cuando el sistema decide dinámicamente si conviene recuperar 
información o no. Un ejemplo de esto es Selective Knowledge Retrieval (SKR), que evita búsquedas 
innecesarias cuando el modelo ya posee el conocimiento suficiente, reduciendo costes y minimizando la 
incorporación de ruido.

\begin{table}[h]
\centering
\begin{tabularx}{\textwidth}{l l X}
\toprule
\textbf{Categoría} & \textbf{Subcategoría / Tipo} & \textbf{Ejemplos} \\ \midrule
\multirow{4}{*}{Tipos} 
  & Transformers      & GPT, BART, T5 \\
  & LSTM              & Modelos secuencia a secuencia tradicionales \\
  & GANs              & Generación adversarial en imágenes y texto \\
  & Diffusion Models  & Imagen, audio y video \\ \midrule
\multirow{3}{*}{Enhancements}
  & Prompt Engineering   & Diseño de instrucciones, chain of thought, step-back prompts \\
  & Generator Fine-tuning& Ajuste del modelo al dominio específico \\
  & Decoding Tuning      & Beam search, nucleus sampling, top-k sampling \\ \bottomrule
\end{tabularx}
\caption{Componente Generator}
\end{table}

El componente generador es el encargado de producir texto,imagenes u otro tipo de contenido. La capacidad del generador no depende unicamente de su arquitectura sino de un conjunto 
de estrategias que optmizan su rendimiento y controlan la calidad de las salidas. Estas mejoras incluyen prompt engineering, el fine-tuning especializado en dominios y 
ajustes en los métodos de decodificación para equilibrar coherencia y diversidad en la generación. En este sentido, comprender tanto los tipos de modelos como las técnicas de optimización resulta fundamental para evaluar el papel del \textit{Generator} 
en sistemas avanzados como los de Retrieval-Augmented Generation (RAG), donde la combinación de arquitectura y optimización garantiza la generación de respuestas 
más fiables, contextualizadas y relevantes 
\parencite{casola2022pretrained,fan2024ragllm,zhang2025hallucination}

Los transformes son la arquitectura mas influyente en NLP ya que, según lo menciona \textcite{casola2022pretrained}, su capacidad de manejar dependencias a largo plazo 
mediante mecanismos de auto-atención ha permitido el desarrollo de modelos como GPT, BART y T5, los cuales han superado ampliamente a enfoques previos y marcado un cambio 
de paradigma en la generación de lenguaje. os LSTM se empleaban en arquitecturas secuencia a secuencia, resolviendo problemas de memoria en redes recurrentes, 
aunque presentaban limitaciones en el escalamiento y en la captura de dependencias largas \parencite{jing2024vecdb}. Por otro lado, introdujeron un enfoque basado 
en el enfrentamiento entre un generador y un discriminador, logrando avances en la creación de imágenes y, posteriormente, en la generación de texto. Mas recientemete los Diffusion Models
se han consolidado en el ámbito multimodal (imagen, audio y video), gracias a su capacidad de producir datos de alta calidad a partir de procesos de ruido inverso,
ampliando las fronteras de la generación más allá del texto. 

Con respecto a la mejora, el Prompt Engineering ha demostrado ser una técnica central para guiar los modelos hacia respuestas más precisas y controladas. Segun \textcite{zhang2025hallucination}
incluye el diseño de instrucciones específicas, así como métodos como chain of thought o step-back prompting, que mejoran la coherencia y reducen alucinaciones en los resultados
Otra estrategia es el fine tuning que consiste en adaptar un modelo  a un modelo concreto, optimizando su capacidad de manejar informacion especial especializada y aumentando su fiabilidad en tareas críticas,
por ejemplo en contextos científicos o legales
El Decoding Tuning se refiere al ajuste de los métodos utilizados por un modelo generativo para decidir qué palabra o token producir a continuación. 
En lugar de limitarse a elegir siempre la opción con mayor probabilidad, lo cual puede generar textos repetitivos y poco naturales, se aplican estrategias 
que permiten modular el balance entre coherencia y diversidad. Entre ellas, el beam search explora varias rutas posibles de generación en paralelo para seleccionar 
la más prometedora, garantizando coherencia aunque sacrificando creatividad. Por su parte, el nucleus sampling restringe las opciones a un conjunto dinámico de palabras 
que concentran la mayor parte de la probabilidad acumulada y selecciona una de ellas de manera aleatoria, lo que produce textos más naturales y variados. Finalmente, 
el top-k sampling limita las elecciones a las k palabras más probables y elige una de ellas de acuerdo con su distribución de probabilidad, lo que ofrece un equilibrio 
controlado entre precisión y diversidad. 

\begin{quote}
Estas estrategias permiten modular el equilibrio entre coherencia y diversidad en la generación de texto, 
adaptando el comportamiento del modelo sin necesidad de modificar su arquitectura 
\parencite{hu2024ragrau}.
\end{quote}


\subsubsection{Fases de Implementación}
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
  \def\step{-1.5}
  \def\y{0}

  % Paso I: Ingesta
  \pill{\y}{softcream}{I}{Ingesta}
  \pgfmathsetmacro\y{\y+\step}

  % Paso II: Preprocesamiento
  \pill{\y}{softpink}{II}{Preprocesamiento}
  \pgfmathsetmacro\y{\y+\step}

  % Paso III: Vectorización
  \pill{\y}{softblue}{III}{Vectorización (Embeddings)}
  \pgfmathsetmacro\y{\y+\step}

  % Paso IV: Indexación en Vector DB
  \pill{\y}{softpeach}{IV}{Vector DB}
  \pgfmathsetmacro\y{\y+\step}

  % Paso V: Recuperación
  \pill{\y}{softyellow}{V}{Recuperación}
  \pgfmathsetmacro\y{\y+\step}

  % Paso VI: Re-ranking
  \pill{\y}{softgreen}{VI}{Re-ranking}
  \pgfmathsetmacro\y{\y+\step}

  % Paso VII: Construcción de Contexto
  \pill{\y}{softcream}{VII}{Construcción de Contexto}
  \pgfmathsetmacro\y{\y+\step}

  % Paso VIII: Generación
  \pill{\y}{softpink}{VIII}{Generación (LLM)}
  \pgfmathsetmacro\y{\y+\step}

  % Paso IX: Evaluación
  \pill{\y}{softblue}{IX}{Evaluación}
\end{tikzpicture}
\end{center}
\caption{Pipeline de RAG}
\label{fig:pipeline-rag}
\end{figure}
Las fases de implementacion son una secuencia de pasos diseñados para enriquecer la generación de respuestas con información externa y actualizada. 
Según \textcite{tabassum2020}, todo inicia con la ingesta y el preprocesamiento de los datos, donde las técnicas de limpieza y segmentación garantizan 
que el texto sea utilizable para fases posteriores. Luego se vectoriza mediante embeddings los cuales transforman los fragmentos de texto
en representaciones numericas para la recuperacion semantica tal como lo expresa \textcite{minaee2021}. Para \textcite{hu2024ragrau}, estos vectores  se almacenan en una base de datos vectorial 
la cual permite realizar búsquedas eficientes por similitud semántica entre la consulta y los fragmentos de texto indexados.
Ademas, esta VDB permite trabajar en conjunto con indices tradicionales como BM25, dando lugar a la recuperación híbrida.
Con los datos obtenidos, como lo señala \textcite{sarthi2024raptor}, 
es necesario aplicar procesos de re-ranking y filtrado que permiten priorizar las partes más relevantes dando lugar la construcción de contexto, en la que los fragmentos de texto seleccionados 
se organizan, se condensan para reducir su extensión y se adaptan al límite de entrada del modelo.para conformar un contexto optimizado y manejable que servirá de entrada
al modelo generativo. En la fase final, \textcite{knollmeyer2024benchmarking} sostiene que el LLM debe complementarse con mecanismos de evaluación para aegurar la 
fidelidad y coherencia de las respuestas generadas.

\paragraph{Ingesta} Es el comienzo del pipeline de RAG, su objetivo es recopilar,normalizar y preparar las fuentes de informacion
que luego seran usadas para los procesos de recuperacion y generacion. La ingesta implica recolectar distintas fuentes  ya sean libros,literatura cientifica,corpus abiertos, etc y convertirlos en un formato manejable por el 
sistema. De acuerdo con \textcite{gao2023rag} y \textcite{jing2024vecdb} estos datos se almacenan como texto estructurado o semiestructurado o sin estructura, dependiendo de su naturaleza y del tipo de procesamiento requerido.

\paragraph{Preprocesamiento} Dentro de la ingesta de datos es necesario limpiar y estructurar el texto en bruto para que sea utilizable en etapas posteriores. De acuerdo con \textcite{tabassum2020} los procesos de tokenización,
 normalización, eliminación de stopwords, lematización y segmentación en oraciones permiten transformar datos no estructurados en unidades coherentes y consistentes, reduciendo así el ruido que podría propagarse en fases posteriores. 
 Este paso es importante ya que el texto procesado debe adaptarse a las limitaciones de los LLM, lo cual implica fragmentar en chunks y generar representaciones vectoriales, siendo así que \textcite{knollmeyer2024benchmarking} 
 afirma que la calidad del preprocesamiento incide directamente en dimensiones de evaluación como la relevancia contextual y la fidelidad de las respuestas. Una preparación deficiente del texto puede introducir ruido en el contexto recuperado 
 y favorecer inconsistencias en la generación, lo cual incrementa el riesgo de alucinaciones; en este sentido, se entiende que el preprocesamiento actua como una primera barrera de mitigación de errores (\textcite{zhang2025hallucination}).

\paragraph{Vectorizacion} Los fragmentos de texto una vez preprocesados se transforman en embeddings \footnote{Representacion numerica que codifica el significado del texto en un espacio vectorial} permitiendo capturar la semantica de tal manera que 
los textos similares esten proximos en el espacio vectorial. De acuerdo con \textcite{hu2024ragrau} existen dos enfoques: la recuperacion dispersa (sparse) como por ejemplo 
TF-IDF, BM25 y la recuperacion densa, la cual es basada en embeddings generados por modelos preentrenados como BERT. El primer enfoque se basa en su frecuencia estadistica del termino en los documentos \parencite{fan2021pretraining}.
Posteriormente, metodos como Latent Semantic Analysis (LSA) incorporaron técnicas de reducción dimensional a la matriz termino-documento.mediante descomposición en valores singulares, lo que permitió identificar estructuras semánticas latentes y superar 
limitaciones de los modelos basados únicamente en frecuencia. Estudios recientes lo consideran el punto de inicio de los embeddings neuronales los cuales ofrecen representaicones mas contextuales y expresivas que permiten clasificar y recuperar el texto adecuadamente.
\parencite{fan2021pretraining,minaee2021}. Con la popularizacion del deep learning, surgieron represetaciondes densas como Word2Vec y GloVe las cuales son capaces de capturar relaciones semanticas entre palabras en espacios vectoriales continuos. Los transformers son una 
evolucion del enfoque denso donde segun \textcite{casola2022pretrained} los transformers preentrenados al combinarlose con mecanismos de atencion y un preentrenamiento masivo, producen vectores contextualizados robustos capaces de capturar relaciones de dependencia a largo 
plazo entre palabras.

\paragraph{Vector DB} 
Los VDBs son importantes dentro del desarrollo de sistemas RAG, pues permiten almacenar y organizar representaciones densas de texto para su posterior
recuperación eficiente. Estos se emplean en lugar de los índices invertidos tradicionales, que basan la búsqueda en coincidencias exactas de palabras o estadísticas 
de frecuencia, lo cual dificulta la detección de sinónimos y relaciones semánticas. De acuerdo con \textcite{jing2024vecdb}, los VDBs están diseñados para manejar 
grandes volúmenes de representaciones de alta dimensionalidad y soportar búsquedas por similitud mediante métricas como la distancia euclidiana o el coseno. Además, 
como menciona \textcite{fan2021pretraining}, el uso de representaciones preentrenadas mejora la calidad de los índices, ya que los embeddings capturan relaciones 
semánticas profundas entre consultas y documentos. Para optimizar estas búsquedas se incorporan técnicas de indexación, que organizan los embeddings en estructuras preparadas para la búsqueda aproximada de vecinos más cercanos \parencite{ma2025vdb}.  

Para llevar a cabo estas operaciones, los VDBs recurren a diferentes enfoques de búsqueda, tales como:

\begin{itemize}
    \item \textbf{Nearest Neighbor Search (NNS):} compara de manera exhaustiva el vector de consulta con todos los vectores almacenados. Garantiza la máxima precisión,
     pero resulta poco escalable con colecciones masivas de datos \parencite{ma2025vdb}.
    
    \item \textbf{Graph-Based Approach:} emplea estructuras de grafos, como HNSW, que permiten navegar de forma eficiente entre vectores y localizar vecinos cercanos 
    en pocas iteraciones, reduciendo considerablemente los tiempos de búsqueda \parencite{jing2024vecdb}.
    
    \item \textbf{Tree-Based Approach:} organiza los vectores en estructuras jerárquicas (ej. KD-Tree, Ball-Tree), dividiendo recursivamente el espacio. Es eficiente 
    en dimensionalidades bajas o medias, pero pierde rendimiento en espacios de muy alta dimensión \parencite{fan2021pretraining}.
    
    \item \textbf{Approximate Nearest Neighbor Search (ANNS):} sacrifica una mínima exactitud para ganar eficiencia y escalabilidad. Métodos como IVF o PQ permiten 
    realizar búsquedas rápidas en grandes colecciones, siendo muy usados en producción \parencite{ma2025vdb}.
\end{itemize}
Cuando una consulta es transformada en vector, el sistema no necesita recorrer todo el corpus, sino que localiza la región del espacio donde se ubica y 
devuelve los vecinos más próximos. Para lograr esta eficiencia se emplean técnicas de indexación que organizan los embeddings en estructuras optimizadas para 
la búsqueda aproximada. Se implementa mediante enfoques como:
\begin{itemize}
    \item \textbf{HNSW (Hierarchical Navigable Small World graphs):} construye un grafo jerárquico navegable que permite acceder rápidamente a los vecinos más cercanos reduciendo la complejidad de la búsqueda.\parencite{ma2025vdb}
    \item \textbf{IVF (Inverted File Index):} agrupa los vectores en listas invertidas de centroides, de modo que la comparación se restringe a las regiones más prometedoras del espacio.\parencite{ma2025vdb}
    \item \textbf{PQ (Product Quantization):} divide los vectores en subespacios y los representa de forma comprimida, lo que acelera significativamente el cálculo de distancias en colecciones de gran tamaño \parencite{ma2025vdb}.
\end{itemize}

\paragraph{Recuperacion} 










\subsubsection{Paradigmas}
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
\tikzset{
  mindoval/.style={
    rounded corners=18pt,
    draw=black!70,
    line width=0.7pt,
    minimum width=3.6cm,
    minimum height=1.1cm,
    inner sep=6pt,
    fill=white
  },
  mindcenter/.style={
    rounded corners=8pt,
    draw=black!70,
    line width=0.9pt,
    minimum width=4.0cm,
    minimum height=1.2cm,
    inner sep=4pt,
    fill=softrose,
    font=\bfseries
  },
  mindarrow/.style={-latex, line width=0.9pt}
}

% --- Nodo central
\node[mindcenter] (paradigmas) at (0,0) {Paradigmas};

% --- Nodos conectados
\node[mindoval, text width=5cm, align=center] 
  (rag) at (-4,-3) {Retrieval-Augmented Generation (RAG)};
\node[mindoval, text width=4.2cm, align=center] 
  (hallucination) at (4,-3) {Hallucination Mitigation};

% --- Flechas
\draw[mindarrow] (paradigmas.south west) -- (rag.north);
\draw[mindarrow] (paradigmas.south east) -- (hallucination.north);

\end{tikzpicture}
\end{center}
\caption{Paradigmas principales en RAG}
\label{fig:paradigmas}
\end{figure}

Dentro del area de Retrieval Augmented Generation (RAG) \textcite{gao2023rag} distiguen los paradigmas Naive RAG, Advanced RAG y Modular RAG, que representan un progreso
desde enfoques básicos de recuperación y generación hasta arquitecturas modulares y flexibles. A su vez \textcite{zhang2025hallucination} clasifican los problemas, en el area de 
Hallucination Mitigation, en dos ejes: retrieval failure (fallos en fuentes, consultas o recuperadores ) y generation deficiency 
(ruido o conflicto contextual y límites de capacidad). FInalmente, 
\textcite{zhao2024rag} proponen fundaciones de RAG segun la forma que el retriver complementa al generador se clasifican en Query-based RAG, Latent Representation-based RAG,
Logit-based RAG y Speculative RAG, de esta forma se expande la aplicación de RAG a múltiples dominios y modalidades.



\paragraph{Tipos de Paradigmas}

\begin{itemize}
    \item \textbf{Retrieval-Augmented Generation (RAG) — Gao et al. (2023)}
    \begin{itemize}
        \item Naive RAG: enfoque básico de recuperación y generación.
        \item Advanced RAG: incorpora optimizaciones como segmentación fina, re-ranking y recuperación iterativa.
        \item Modular RAG: paradigma flexible con módulos especializados para búsqueda, memoria, alineación y validación.
    \end{itemize}

    \item \textbf{Tipos de RAG — Zhao et al. (2024)}
    \begin{itemize}
        \item Query-based RAG: integra directamente la consulta y la información recuperada en el input del generador.
        \item Latent Representation-based RAG: incorpora la información recuperada como representaciones latentes en el modelo generativo.
        \item Logit-based RAG: combina la información de recuperación en la fase de decodificación a nivel de logits.
        \item Speculative RAG: sustituye pasos de generación por recuperación para acelerar y reducir costes.
    \end{itemize}

    \item \textbf{Hallucination Mitigation — Zhang \& Zhang (2025)}
    \begin{itemize}
        \item Retrieval failure: fallos en las fuentes, consultas, recuperadores o estrategias de recuperación.
        \item Generation deficiency: deficiencias en la generación como ruido, conflictos contextuales, middle curse, problemas de alineación y límites de capacidad.
    \end{itemize}
\end{itemize}




\subsubsection{Evaluacion y Metricas}

La evaluación de los sistemas de Recuperación de Información (IR) y de Retrieval-Augmented Generation (RAG) ha sido objeto de un creciente interés en la literatura reciente. 
En el ámbito de IR, \textcite{bernard2025fate} destacan que las nociones de equidad, transparencia y responsabilidad requieren enfoques diversos: mientras la \textit{fairness} 
se mide mayormente con métricas automáticas, la \textit{transparency} y la \textit{accountability} suelen evaluarse mediante auditorías y estudios con usuarios. En contraste, 
la dimensión ética carece de métricas claras y se asocia más a aspectos como privacidad y seguridad.  

En el área de RAG, \textcite{knollmeyer2024benchmarking} identifican cinco dimensiones clave de evaluación: relevancia del contexto, fidelidad (\textit{faithfulness}), relevancia 
de la respuesta, corrección y calidad de las citas. Estas dimensiones permiten evaluar de manera más integral los sistemas que combinan recuperación y generación. Asimismo, 
\textcite{gao2023rag} señalan que, además de las métricas automáticas tradicionales de IR (precisión, recall, nDCG), resulta fundamental incluir evaluaciones humanas que capten 
aspectos como coherencia, transparencia y verificabilidad.  

\paragraph{Categorías de Evaluación}
\begin{itemize}
  \item \textbf{Enfoques de Evaluación - Gao et al. (2023)}
        \begin{itemize}
          \item Evaluación independiente por etapas: análisis separado de retrieval, augmentation y generation, con métricas específicas en cada módulo.
          \item Evaluación End-to-End: valoración directa de la salida final del sistema RAG, con dos variantes:
          \begin{itemize}
              \item Evaluación End-to-End automática: frameworks que miden habilidades/abilities clave como exactitud, fidelidad (\textit{faithfulness}), atribución de fuentes, reducción de alucinaciones y transparencia.
              \item Evaluación End-to-End con juicio humano: juicios expertos que valoran coherencia, verificabilidad, utilidad práctica y confianza.
          \end{itemize}
          \item Combinación de métricas: integración de métricas clásicas de recuperación (precisión, recall, nDCG), métricas de generación (coherencia, verificabilidad, calidad narrativa) y evaluación humana.
      \end{itemize}
    \item \textbf{Evaluación en IR con FATE — Bernard \& Balog (2025)}
    \begin{itemize}
        \item Fairness: métricas automáticas como \textit{top-k}, \textit{exposure} y \textit{pairwise metrics}.
        \item Transparency y Accountability: auditorías y estudios de usuarios, centrados en interpretabilidad y trazabilidad.
        \item Ethics: enfoques cualitativos vinculados a privacidad y seguridad.
        \item Tensiones: conflictos entre fairness individual vs. grupal y entre transparencia excesiva vs. carga cognitiva.
    \end{itemize}

    \item \textbf{Evaluación Clásica en IR — Hambarde \& Proença (2023)}
    \begin{itemize}
        \item Métricas de recuperación: precisión, recall, F1, MAP (Mean Average Precision), MRR (Mean Reciprocal Rank).
        \item Métricas de ranking: nDCG (Normalized Discounted Cumulative Gain) y calidad del ordenamiento.
        \item Evaluación de modelos neuronales: comparación con benchmarks tradicionales (TREC, MS MARCO).
        \item Dimensión multi-modal: evaluación de IR en escenarios que integran texto, imágenes y audio.
    \end{itemize}

    \item \textbf{Evaluación en RAG — Knollmeyer et al. (2024); Gao et al. (2023)}
    \begin{itemize}
        \item \textit{Fase de Recuperación}: 
        \begin{itemize}
            \item Context relevance: grado de pertinencia de los documentos recuperados.
            \item Dataset quality: uso de bases de datos curadas y representativas (ej. Wikipedia, MS MARCO).
            \item Métricas aplicadas: precisión, recall, nDCG, cobertura de conocimiento.
        \end{itemize}

        \item \textit{Fase de Generación}: 
        \begin{itemize}
            \item Faithfulness: consistencia factual entre recuperación y generación.
            \item Answer relevance: utilidad de la respuesta respecto a la consulta.
            \item Correctness: exactitud de la información producida y reducción de alucinaciones.
            \item Métricas aplicadas: BLEU, ROUGE, métricas de consistencia factual y evaluaciones humanas.
        \end{itemize}

        \item \textit{Fase de Integración}: 
        \begin{itemize}
            \item Citation quality: precisión, trazabilidad y cobertura de las fuentes citadas.
        \end{itemize}

        \item \textit{Evaluación Global}: 
        \begin{itemize}
            \item Evaluadores mixtos: combinación de métricas automáticas (similaridad semántica, BLEU, ROUGE) con juicios humanos (coherencia, verificabilidad, utilidad práctica).
            \item Datasets: necesidad de colecciones específicas adaptadas a RAG, más allá de benchmarks tradicionales de IR.
        \end{itemize}

        
    \end{itemize}
    
\end{itemize}



\subsubsection{Futuro de RAG}

\paragraph{Desafios Actuales}
De acuerdo con \textcite{zhai2024llmIR}, uno de los principales desafíos de los sistemas RAG es la generación de alucinaciones, que comprometen la confianza y 
limitan su uso en aplicaciones críticas. Asimismo, persisten limitaciones en la calidad de la recuperación y en la eficiencia computacional, especialmente 
en dominios donde se requiere información actualizada y especializada (Hu \& Lu, 2024). Además, \textcite{ramdurai2025llm} señala que la integración de RAG con otras
arquitecturas, como redes neuronales convolucionales, enfrenta problemas de escalabilidad y de adaptación a contextos heterogéneos.

\paragraph{Direcciones Potenciales}  
Las líneas de avance incluyen el desarrollo de retrievers más robustos, la optimización de las interacciones entre modelo y recuperación, 
así como mecanismos de evaluación que prioricen la relevancia y la coherencia.\textcite{ramdurai2025llm} enfatiza la sinergia con arquitecturas multimodales y con sistemas 
capaces de combinar información textual, visual y estructurada. Asimismo, enfoques como RAPTOR que introduce resúmenes recursivos y jerárquicos en la recuperación
muestran cómo superar la fragmentación del contexto y mejorar la integración semántica (Sarthi et al., 2024).

\paragraph{Perspectivas}
A medio plazo, RAG y los LLMs convergerán con la IR clásica hacia sistemas de acceso conversacional al conocimiento: motores de búsqueda “con opinión” pero con 
respaldo documental, capaces de planificar, citar y aprender de la interacción. Incluso si los LLMs de contexto largo siguen mejorando, la recuperación seguirá 
siendo diferencial por costo, frescura, control y gobernanza de la evidencia; veremos stacks donde RAG, herramientas y memoria trabajan de forma coordinada, 
con evaluación centrada en utilidad y veracidad para tareas compuestas y empresariales
