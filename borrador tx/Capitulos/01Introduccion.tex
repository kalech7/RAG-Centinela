\chapter{Introducción}

\section{Planteamiento del problema}

\section{Justificación}

\section{Justificación Metodológica}

\section{Objetivos}
\subsection{Objetivo general}
Desarrollar e implementar un sistema RAG que mejore el desempeño del buscador de la plataforma
Centinela, permitiendo recuperar información científica relevante y generar respuestas automáticas
de valor para el usuario.
\subsection{Objetivos específicos}
\begin{itemize}[align=left, label=-]
    \item Realizar una revisión sistemática de la literatura sobre metodologías y/o frameworks para la implementación de RAG.
    \item Diseñar e implementar la arquitectura técnica del sistema RAG utilizando modelos de recuperación y generación de texto.
    \item Evaluar el sistema RAG desarrollado mediante métricas estándar.
\end{itemize}

\section{Alcance}

\section{Marco Teórico}

\section{Revisión de literatura}
En los últimos años, la evolución de los modelos de lenguaje de gran escala (Large Language Models, LLM) ha redefinido el procesamiento del lenguaje natural
e impulsado nuevas líneas de investigación. 
Sin embargo, estos modelos dependen únicamente de los datos empleados durante su entrenamiento, lo que limita su capacidad para ofrecer respuestas actualizadas, verificables y 
contextualizadas. En respuesta a esta limitación surge el enfoque de Retrieval-Augmented Generation (RAG), el cual combina la recuperación de información con la generación de 
lenguaje natural, logrando mejorar la precisión, la coherencia y la actualidad de las respuestas producidas por los modelos.

Dada la creciente relevancia de los LLM, resulta necesario llevar a cabo una revisión exhaustiva de la literatura que permita consolidar los avances recientes y evaluar los desafíos aún presentes.
Para ello, esta revisión se apoya en Umbrella SLR y propagacion de citaciones usando el protocolo propuesto en la Sección~\ref{sec:revision-sistematica};
En esta sección se presenta un análisis estructurado de la literatura disponible, considerando tanto los fundamentos conceptuales de RAG como sus fases de desarrollo, 
aplicaciones y el futuro. Para ello, el proceso de revisión se organiza en las fases que se presentan a continuación:

\begin{itemize}
    \item \nameref{subsec:proposito-objetivos}
    \item \nameref{subsec:criterios-inclusion-exclusion}
    \item \nameref{subsec:estudio-semilla}
    \item \nameref{subsec:valoracion-evidencias}
    \item \nameref{subsec:sintesis-resultados}
\end{itemize}
Las cuales buscan garantizar la consistencia, validez y pertinencia de la evidencia obtenida.

\subsection{Propósito y objetivos de la revisión}
\label{subsec:proposito-objetivos}
El propósito de esta revisión es consolidar la información disponible sobre los RAG, abordando su estudio desde los fundamentos hasta las fases de desarrollo. 
Se inicia con su definición y arquitectura, para luego profundizar en las etapas clave del proceso: extracción del corpus, preprocesamiento, vectorización, recuperación de información, 
evaluación, almacenamiento en bases vectoriales y generación de resultados. Asimismo, se examinan los paradigmas, las métricas de evaluación y el futuro de RAG.
Durante esta revisión se busca lograr el objetivo general de proporcionar un panorama global y actualizado sobre los RAG,
exponiendo sus fundamentos, desarrollo y aplicación.

\subsection{Criterios de inclusión y exclusión}
\label{subsec:criterios-inclusion-exclusion}
Se incluyen únicamente revisiones sistemáticas y metaanálisis publicados entre 2018 y 2025, en inglés o español, dado que la producción 
científica en el área comenzó a incrementarse a partir de 2018, con base en información de Lens.org\footnote{Es una plataforma abierta
para la búsqueda, análisis y visualización de literatura científica y patentes. Accesible en: \href{https://www.lens.org/}{Lens.org}}, este incremento coincide
con la popularización 
de los modelos de lenguaje basados en transformers\footnote{Se atribuye a hitos como BERT (2018), GPT-2 (2019) y T5 (2020), que impulsaron un avance 
en la investigación del procesamiento del Lenguaje Natural}.
Los estudios deben provenir de fuentes confiables y ser, a su vez, revisados por 
un experto. Se da preferencia a aquellos que presenten una cobertura amplia de los temas más relevantes para el objeto de estudio.  

Se excluyen las revisiones narrativas, los documentos que carezcan de transparencia en sus métodos de búsqueda o síntesis, así como las publicaciones que no estén 
directamente relacionadas con el objeto de estudio delimitado.

\subsection{Identificación del estudio semilla y selección de revisiones relevantes}
\label{subsec:estudio-semilla}
El proceso de búsqueda se inicia con la identificación de dos estudios semilla, extraídos de Google Scholar mediante los parámetros “Retrieval Information” y “Retrieval Augmented Generation”. 
Debido al análisis realizado en Lens.org, se estableció el filtro de 2018 a 2025, ya que se observa que a partir de 2018 el término retrieval-augmented generation 
comenzó a adquirir una relevancia en la literatura científica, mostrando interés de la comunidad investigadora hasta la actualidad.

El primer estudio seleccionado fue Information Retrieval: Recent Advances and Beyond \textcite{hambarde2023ir}, publicado en IEEE Access. 
Este trabajo constituye una revisión exhaustiva de la recuperación de información, abarcando desde los métodos tradicionales hasta los enfoques 
basados en deep learning y transformers, por lo que resulta un punto de partida principal para explorar la literatura reciente y relevante.

El segundo estudio semilla corresponde al artículo Retrieval-Augmented Generation for Large Language Models \textcite{gao2023rag}, publicado en
arXiv,
el cual presenta un marco conceptual y aplicado sobre la integración de recuperación de información y modelos generativos de gran escala. Su incorporación permite 
establecer una base teórica para contextualizar el análisis de las revisiones seleccionadas.

A partir de estos dos estudios semilla, y aplicando los criterios de inclusión y exclusión previamente definidos, se identificaron \textcolor{blue}{25} revisiones 
relevantes que cumplen con los criterios establecidos. 
Estas revisiones constituyen la base para el análisis y síntesis en el presente trabajo.



\subsection{Valoración de las evidencias y extracción de la información}  
\label{subsec:valoracion-evidencias}
De los estudios seleccionados se procede a realizar un análisis,  
con el fin de excluir aquellos artículos que no cumplen con los criterios establecidos  
o que presentan un nivel de profundidad insuficiente para los objetivos de la revisión.  
La selección final de los estudios se realiza en consenso con expertos en el área,  
garantizando así la pertinencia y relevancia de la evidencia incluida.  
Para la organización, codificación y síntesis de la  
información se utiliza ATLAS.ti\footnote{Scientific Software Development GmbH. Disponible en: \href{https://atlasti.com/es}{Atlas.ti}},  
que facilitará la estructuración de los hallazgos.  




% para resuemir la informacion se uso (herramienta) atlas por ejemplo

\subsection{Síntesis y representación de resultados}
\label{subsec:sintesis-resultados}
Con la literatura seleccionada se identificó la hoja de ruta que se presenta a continuación en la Figura~\ref{fig:secciones-rag}.

% --- Grafico resumen

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
  % distancia vertical entre pastillas
  \def\step{-1.5}

  % y inicial (arriba)
  \def\y{0}

  \pill{\y}{softcream}{I}{Fundamentos}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softpink}{II}{Arquitectura}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softblue}{III}{Fases de Implementación}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softpeach}{IV}{Paradigmas}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softyellow}{V}{Evaluación y metricas}
  \pgfmathsetmacro\y{\y+\step}
  \pill{\y}{softgreen}{VI}{Futuro de RAG}
  
\end{tikzpicture}
\end{center}
\caption{Resumen esquemático de RAG}
\label{fig:secciones-rag}
\end{figure}

A partir de esta hoja de ruta se desarrolla un esquema más detallado, en el que primero se exploran la teoría, 
las características y las aplicaciones, como se muestra en la Figura.~\ref{fig:Fudamentos}. Posteriormente, se profundiza en la arquitectura, Figura. \ref{fig:arquitectura_rag}, 
donde se describe cada uno de los componentes que la conforman (Retriever, Augmented y Generation), así como las variantes y mejoras que existen en cada uno. 
Más adelante, se detalla el proceso de implementación (ver la  Figura.~\ref{fig:pipeline-rag}), desde la preparación de los datos hasta el componente de generación, incluyendo las técnicas y herramientas más relevantes.  
En la Figura.~\ref{fig:paradigmas} correspondiente se examinan los paradigmas de RAG, presentando sus tipos y clases; seguidamente, se introducen las métricas y evaluadores automáticos 
empleados en la evaluación de sistemas RAG, junto con las consideraciones éticas y de equidad que deben tenerse en cuenta.  
Finalmente, se discuten las tendencias emergentes, los desafíos actuales y las posibles direcciones futuras que podrían orientar la evolución de los sistemas RAG.  





\subsubsection{Fundamentos de RAG}

% --- Grafico fundamentos
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
\tikzset{
  mindoval/.style={
    rounded corners=18pt,
    draw=black!70,
    line width=0.7pt,
    minimum width=3.2cm,
    minimum height=1.1cm,
    inner sep=6pt,
    fill=white      % <<< todos los nodos en blanco
  },
  mindcenter/.style={
    rounded corners=8pt,
    draw=black!70,
    line width=0.9pt,
    minimum width=3.6cm,
    minimum height=1.1cm,
    inner sep=4pt,
    fill=softrose,  % <<< centro en morado
    font=\bfseries
  },
  mindapp/.style={
    mindoval,
    fill=softblue   % <<< nodo aplicación en azul
  },
  mindarrow/.style={-latex, line width=0.9pt}
}

% --- Nodo central
\node[mindcenter] (fund) at (0,0) {Fundamentos};

% --- Nodos arriba
\node[mindoval] (aport)  at (0,3) {4. Aportes de RAGs};
\node[mindoval] (ragft)  at (5,0) {5. RAG vs fine tuning};

% --- Nodos izquierda
\node[mindoval] (rolrag) at (-5,2.5) {3. Rol de RAG};
\node[mindoval] (qrag)   at (-5,0.75) {2. ¿Qué es RAG?};
\node[mindoval] (qllm)   at (-5,-1.0) {1. ¿Qué es un LLM?};

% --- Nodos abajo
\node[mindapp, minimum width=3.0cm] (apli) at (0,-3.0) {6. Aplicación};
\node[mindoval] (texto)  at (-5,-4.5) {Texto};
\node[mindoval] (img)    at (-2,-6) {Imagen};
\node[mindoval, minimum width=3.6cm] (audio)  at (2,-6) {Audio/Video};
\node[mindoval] (codigo) at (5,-4.5) {Código};

% --- Flechas (tocando bordes de nodos)
\draw[mindarrow] (fund.north) -- (aport.south);
\draw[mindarrow] (fund.east) -- (ragft.west);
\draw[mindarrow] (fund.west) -- (rolrag.east);
\draw[mindarrow] (fund.west) -- (qrag.east);
\draw[mindarrow] (fund.west) -- (qllm.east);
\draw[mindarrow] (fund.south) -- (apli.north);

\draw[mindarrow] (apli.west) -- (texto.east);
\draw[mindarrow] (apli) -- (img.north);
\draw[mindarrow] (apli) -- (audio.north);
\draw[mindarrow] (apli.east) -- (codigo.west);

\end{tikzpicture}
\end{center}
\caption{Fundamentos de RAG}
\label{fig:Fudamentos}
\end{figure}

En esta subsección se presentan los fundamentos teóricos de Retrieval-Augmented Generation (RAG), 
comenzando con la definición de los modelos de lenguaje de gran escala (LLMs) y su relación. Se expone 
también el papel que desempeña RAG, los principales aportes que ha generado en distintos ámbitos y su diferenciación frente al 
\textit{fine-tuning}. Finalmente, se introduce su aplicación práctica, lo que permite comprender la importancia y el impacto que RAG tiene en 
la actualidad.

\paragraph{¿Qué es un LLM?}
Son modelos de inteligencia artificial (IA) basados en la arquitectura \textit{transformer}\footnote{es una arquitectura para modelar secuencias que reemplaza la recurrencia y la convolución por auto-atención multi-cabeza y 
capas feed-forward por posición, con conexiones residuales, normalización y codificaciones posicionales; puede usarse como codificador, decodificador o codificador-decodificador}, entrenados con grandes volúmenes de datos textuales con el objetivo de aprender representaciones
contextuales del lenguaje. Según \textcite{casola2022pretrained}, estos modelos utilizan técnicas de preentrenamiento no supervisado para captar patrones lingüísticos y semánticos,
lo que permite que posteriormente puedan ajustarse a tareas específicas como clasificación de texto, análisis de sentimientos, traducción automática, reconocimiento de entidades
o respuesta a preguntas. Ejemplos destacados son \textit{BERT, RoBERTa, ALBERT, XLNet, DistilBERT y GPT-3}, que han mostrado rendimientos sobresalientes en diversas aplicaciones de procesamiento de lenguaje natural (NLP).

De acuerdo con \textcite{ramdurai2025llm}, los LLMs también se definen como una clase de modelos de IA capaces de procesar y generar texto de forma similar al lenguaje humano,
gracias al uso de redes neuronales profundas y la capacidad de aprender no solo gramática y relaciones entre palabras, sino también aspectos más complejos como humor, 
tono emocional y contexto. Entrenados en enormes corpus de datos provenientes de libros, artículos y sitios web, estos modelos pueden responder preguntas, redactar ensayos,
traducir, resumir y crear contenido de manera autónoma. Ejemplos recientes incluyen \textit{GPT-4, T5, XLNet y PaLM}, los cuales demuestran su versatilidad en tareas avanzadas
de NLP y en sistemas aplicados en diferentes industrias. 

\paragraph{¿Qué es un RAG?}
Según \textcite{han2024rag}, Retrieval-Augmented Generation (RAG) es una técnica que integra la capacidad generativa de los modelos 
de lenguaje con la precisión de la recuperación de información en tiempo real. En lugar de basarse únicamente en el conocimiento almacenado en
los parámetros durante el entrenamiento, RAG permite consultar repositorios externos como bases de datos o motores de búsqueda para obtener
documentos relevantes y actualizados. Estos se incorporan al prompt del usuario, lo que fundamenta la respuesta en fuentes 
verificables y disminuye los problemas de errores y alucinaciones que suelen presentarse en los modelos de lenguaje de gran escala. 

\paragraph{Rol del RAG} El rol de RAG es abordar los 3 límites existentes de los LLM: alucinaciones, desactualización del conocimiento y falta de trazabilidad.  
Como menciona \textcite{zhai2024llmIR} no es realista esperar que los LLM sustituyan al buscador; más bien, los futuros sistemas integrarán LLM + búsqueda/RAG, de modo que el modelo “aprenda a usar” la recuperación como herramienta fiable.

\paragraph{Aportes de RAG}  
El aporte de los modelos RAG no se limita a añadir texto al prompt, también consiste en seleccionar evidencia relevante, integrarla mediante filtros adecuados y exponer las fuentes utilizadas para que el usuario pueda verificar la confiabilidad de la información.  
De acuerdo con la revisión exhaustiva de la literatura los RAGs aportan:  
\begin{enumerate}
    \item \textbf{Mejora de la exactitud}: en tareas intensivas en conocimiento, al fundamentar las respuestas en documentos recuperados y relevantes \parencite{gao2023rag,zhao2024rag}.
    \item \textbf{Mitigación de alucinaciones} al condicionar la salida en evidencias recuperadas, exigir consistencia con estas y habilitar la verificación/citación de fuentes por parte del usuario \parencite{zhang2025hallucination,fan2024ragllm}.
    \item \textbf{Reducción de costos y actualización ágil del conocimiento}: en lugar de reentrenar o usar fine-tuning el LLM, basta con actualizar el corpus o el índice, lo que permite incorporar información reciente de manera eficiente \parencite{gao2023rag,zhai2024llmIR}.
    \item \textbf{Cobertura y ampliación de conocimiento especializado}: los RAG permiten abordar información de long-tail\footnote{Información poco frecuente, rara o muy especializada que casi no aparece en los datos de entrenamiento de un modelo.} que no suele estar bien representada en los datos de entrenamiento de un LLM, al tiempo que extienden su utilidad a campos especializados como derecho, medicina, finanzas y automatización de revisiones sistemáticas de literatura (SLR), apoyando procesos de búsqueda, filtrado, extracción y síntesis de evidencia \parencite{gao2023rag,hu2024ragrau,zhai2024llmIR}.
    \item \textbf{Manejo de documentos largos}: técnicas como RAPTOR organizan documentos extensos en árboles de resúmenes que permiten recuperar a distintos niveles de abstracción, mejorando el rendimiento en QA (Question Answering)\footnote{Es la tarea de responder preguntas a partir de evidencia extraída de uno o varios textos. }
    complejo y multi-hop \footnote{La respuesta a una pregunta no se puede obtener de una única fuente, sino que requiere combinar la evidencia de varias fuentes distintas.} \parencite{sarthi2024raptor}.
    \item \textbf{Aportes en evaluación}: se han desarrollado métricas específicas para auditar la calidad de un pipeline RAG (p. ej., context relevance, faithfulness, answer correctness, citation quality), hoy ya sistematizadas en la literatura \parencite{knollmeyer2024benchmarking}.
\end{enumerate}

\paragraph{RAG vs Fine Tuning}  
La diferencia entre ambos es en el tipo de problema que resuelven y en la manera en que se maneja la información.
 De acuerdo con \textcite{gao2023rag}, RAG añade conocimiento no paramétrico, lo que lo hace especialmente útil para datos cambiantes,
  dominios con documentación extensa y situaciones donde se requiere trazabilidad de las fuentes. Por otro lado, fine-tuning introduce conocimiento 
  paramétrico dentro de los pesos del modelo durante un proceso adicional de entrenamiento. Así, el modelo está adaptado a generar salidas con el estilo, formato o razonamiento esperado, incluso sin consultar fuentes externas. Esto es muy útil para casos donde se requiere que el modelo siga protocolos internos, produzca salidas con plantillas específicas o razone con un estilo particular.

\paragraph{Aplicación}  
RAG puede usarse en 4 modalidades:

\begin{itemize}
    \item \textbf{Texto}: RAG sustenta tareas de QA, asistentes de búsqueda, resúmenes y revisiones sistemáticas de literatura (SLR). 
    Para documentos largos se apoya en estructuras jerárquicas que organizan el corpus en un árbol construido de abajo hacia arriba donde 
    se agrupan trozos de texto por similitud, se resumen para formar nodos padre y se crean niveles de abstracción (fragmentos específicos hasta resúmenes globales), los cuales recupera el contexto más pertinente para responder preguntas con evidencia dispersa
     \parencite{sarthi2024raptor}. Además, se complementa con técnicas de \textit{text-matching} y \textit{re-ranking} para elevar la pertinencia del contexto recuperado \parencite{jiang2024textmatching}.
    
    \item \textbf{Código}: conecta el modelo con documentación, repositorios y bases de conocimiento para explicar fragmentos, ubicar APIs y resolver errores, integrando 
    recuperadores y bases vectoriales en el flujo del desarrollador \parencite{hu2024ragrau}.  
    
    \item \textbf{Imagen}: mediante el uso de embeddings multimodales y bases vectoriales, permite tareas de descripción asistida, búsqueda cruzada y grounding de respuestas,
     vinculando consultas textuales con representaciones visuales \parencite{jing2024vecdb,ma2025vector}.  
    
    \item \textbf{Audio y Video}: generalmente se parte de la transcripción, que luego se indexa en segmentos con marcas de tiempo. De esta manera, RAG posibilita responder,
     resumir o enlazar afirmaciones con evidencias auditables en el propio material~\parencite{jing2024vecdb,zhao2024rag}.  
\end{itemize}




\subsubsection{Arquitectura RAG}
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
\tikzset{
  mindoval/.style={
    rounded corners=18pt,
    draw=black!70,
    line width=0.7pt,
    minimum width=3.6cm,
    minimum height=1.1cm,
    inner sep=6pt,
    fill=white
  },
  mindcenter/.style={
    rounded corners=8pt,
    draw=black!70,
    line width=0.9pt,
    minimum width=4.0cm,
    minimum height=1.2cm,
    inner sep=4pt,
    fill=softrose,
    font=\bfseries
  },
  mindarrow/.style={-latex, line width=0.9pt}
}

% --- Nodo central
\node[mindcenter] (arquitectura) at (0,0) {Arquitectura};

% --- Nodos conectados
\node[mindoval] (retrieval) at (-5,-3) {Retrieval};
\node[mindoval] (augmented) at (0,-3) {Augmented};
\node[mindoval] (generation) at (5,-3) {Generation};

% --- Flechas
\draw[mindarrow] (arquitectura.south west) -- (retrieval.north);
\draw[mindarrow] (arquitectura.south) -- (augmented.north);
\draw[mindarrow] (arquitectura.south east) -- (generation.north);

\end{tikzpicture}
\end{center}
\caption{Componentes de RAG}
\label{fig:arquitectura_rag}
\end{figure}

RAG se compone de tres fases: recuperación, augmentation y generación (ver Figura~\ref{fig:arquitectura_rag}). Como lo menciona \textcite{gao2023rag}, primero se 
localizan documentos relevantes para la consulta (Recuperación); luego, se enriquece la entrada del usuario con esos textos (augmentation); y finalmente, el modelo produce 
una respuesta basada tanto en su conocimiento interno como en la información recuperada (Generación). Gracias a este enfoque, RAG incrementa la exactitud 
de las respuestas, facilita la actualización del conocimiento sin necesidad de reentrenar el modelo y mejora la transparencia al permitir la 
cita de fuentes. Se trata de una de las técnicas más relevantes en tareas que requieren una gran 
cantidad de conocimiento, como en los ámbitos médico, legal o de investigación científica.

La Tabla~\ref{tab:retriver} resume el componente retriever en tres categorías: (i) \textbf{indexación}, que incluye bases vectoriales y algoritmos de búsqueda ; 
(ii) \textbf{enhancements}, conjunto de técnicas que mejoran la calidad del contexto recuperado y su ordenación; y (iii)~\textbf{tipos de retrievers}, que abarcan enfoques 
sparse, dense e híbridos.

\begin{table}[htbp]
\centering
\begin{tabularx}{\textwidth}{l l X}
\toprule
\textbf{Categoría} & \textbf{Subcategoría / Tipo} & \textbf{Técnicas} \\
\midrule
Indexing     & Vector DB               & FAISS, Annoy, Milvus, Weaviate, Pinecone, Qdrant\\
              & Algoritmos de búsqueda  & Approximate Nearest Neighbors, Locality-Sensitive Hashing \\
\midrule
Enhancements & Reranking               & CEDR, DuoBERT, DRMM\\
              & Retriever Finetuning    & REPLUG, Learning to retrieve ICL, UDAPDR \\
              & Hybrid Search           & RAP-Gen, BlendedRAG, ReACC\\
              & Chunk Optimization      & RAPTOR, LlamaIndex  \\
              & Recursive Retrieval     & ReAct, RATP \\
              & RAG Pipeline Enhancement  & \textit{Rule-based}, \textit{Model-based} \\
              & Query Reformulation     &  HyDE, Multi-query, Query2Doc\\
\midrule
Tipos de retrievers & Sparse            & BM25, TF-IDF \\
                    & Dense             & Embeddings (ej. BERT, OpenAI, etc.) \\
                    & Others            & Modelos híbridos u otros \\
\bottomrule
\end{tabularx}
\caption{Componente Retriever}
\label{tab:retriver}
\end{table}

\paragraph{Indexing:} Es el encargado de organizar y representar la información de forma eficiente. En este sentido, los vector databases (VDBs) han aumentado su popularidad, ya que permiten almacenar 
vectores de alta dimensionalidad y realizar búsquedas por similitud semántica. Como explica \textcite{joshi2025vector}, estas bases de datos resultan esenciales para 
la aplicación de inteligencia artificial generativa, ya que superan las limitaciones de las bases relacionales en el manejo de datos no estructurados.  
Los VDBs integran mecanismos de búsqueda aproximada de vecinos más cercanos (ANNS), lo que posibilita consultas rápidas incluso sobre miles de objetos. Además, como lo menciona
\textcite{ma2025vector}, incorporan técnicas de optimización como particionamiento, sharding \footnote{ Es una técnica de partición horizontal que divide una base de datos
 en fragmentos distribuidos entre múltiples nodos, lo que permite escalar de forma eficiente y balancear la carga en bases de datos vectoriales},
  cachés y replicación para garantizar escalabilidad y baja latencia en entornos distribuidos.  

Locality-Sensitive Hashing (LSH) es una técnica de indexación ampliamente utilizada en VDBs para acelerar la búsqueda de vecinos aproximados en espacios de alta 
dimensionalidad. A diferencia de los esquemas de hashing tradicionales, cuyo objetivo es dispersar uniformemente los datos para minimizar colisiones, LSH está diseñado 
para maximizar la probabilidad de que vectores similares se asignen al mismo bucket de hash \parencite{ma2025vector}. Según esta técnica, la preservación de la localidad 
se consigue mediante funciones de hash que reflejan la similitud entre vectores, generando colisiones más frecuentes dentro del espacio reducido. De esta manera, al realizar una 
consulta, el sistema solo necesita comparar el vector de entrada con aquellos almacenados en el mismo o en buckets cercanos, reduciendo drásticamente la complejidad 
computacional de la búsqueda \parencite{ma2025vector}. Estas características explican por qué los VDBs se han consolidado como una infraestructura fundamental en el soporte
de sistemas RAG.

\paragraph{Enhancements:} Son técnicas complementarias que se añaden antes, durante o después de la recuperación para subir la calidad sin reemplazar al 
recuperador base.
\smallskip
\noindent\textbf{Reranking:} Es la fase de reordenamiento posterior a la recuperación inicial en un sistema multietapa de RI\footnote{Un sistema multietapa es un pipeline 
donde una primera etapa obtiene un top-k de documentos y una o más etapas posteriores lo reordenan con criterios más finos para mejorar la efectividad del ranking}. 
Parte de una lista top-k generada por el  recuperador y la vuelve a ordenar aplicando uno o varios reordenadores que estiman con mayor precisión la relevancia 
consulta-documento.

%  compactar un poco
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.10}

% EN EL TEXTO
\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{L{0.30\textwidth} X}
\toprule
\textbf{Técnicas} & \textbf{Descripción} \\
\midrule
\multicolumn{2}{l}{\textit{Reranking}}\\
CEDR (Contextualized Embeddings for Document Ranking) &
Re-ranker basado en BERT que concatena $q$ y el fragmento $d$ para obtener representaciones contextualizadas y estimar directamente la relevancia del par. \\
DuoBERT &
Re-ranker \textit{pairwise} que compara explícitamente la \emph{query}, el candidato positivo y el negativo \((q,\ d^{+},\ d^{-})\) en una sola secuencia para aprender preferencias y producir el ordenamiento final; muy usado en \textit{passage ranking}\footnotemark. \\
DRMM (Deep Relevance Matching Model) &
Modelo clásico de interacción que construye histogramas de similitud (consulta-documento) por término y usa \textit{term gating} (softmax) para ponderar la contribución de cada término al puntaje final. \\
\bottomrule
\end{tabularx}
\caption{Técnicas de Reranking}
\label{tab:enhancementsReranking}
\end{table}
\footnotetext{Passage ranking evalúa fragmentos de textos largos; puntúa pares (consulta-fragmento) con cross-encoders y, si se requiere nivel documento, agrega puntajes (p. ej., maxP/sumP, PARADE).}
\subparagraph{Definiciones clave:}
\begin{itemize}
    \item \textbf{Cross-encoder.} Arquitectura de re-ranking que concatena la consulta $q$ y el pasaje $d$ en una sola secuencia para codificarlos conjuntamente y producir un puntaje directo de relevancia del par $(q,d)$.
    \item \textbf{Pairwise.} Esquema de entrenamiento para re-rankers que recibe $(q,\ d^{+},\ d^{-})$ y optimiza para que el modelo puntúe $d^{+}$ por encima de $d^{-}$, generando el ordenamiento final.
    \item \textbf{Term gating.} Mecanismo de ponderación por término (softmax) que modula la contribución de cada término de la consulta al puntaje global de similitud; característico de modelos de interacción.
\end{itemize}

\noindent De acuerdo con la Tabla \ref{tab:enhancementsReranking}, el Re-ranking se apoya crecientemente en modelos neuronales y PTMs (pre-trained transformers) por su mayor efectividad;
 entre los ejemplos recurrentes figuran DRMM (como clásico de interacción) y re-rankers basados en BERT 
 como CEDR y variantes tipo pairwise (DuoBERT). No obstante, existen otras técnicas y variantes en la literatura que no se detallan aquí.











\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{l l >{\raggedright\arraybackslash}X}
\toprule
\textbf{Categoría} & \textbf{Subcategoría / Tipo} & \textbf{Técnicas / Ejemplos} \\
\midrule
Tipos   & Pre-training          & Knowledge Graph Embeddings \\
        & Fine-tuning           & SFT (supervised fine-tuning), PEFT (parameter-efficient fine-tuning), ajuste del retriever/reranker  \\
        & Inference             & Retrieve-then-Read-then-Revise (RARR), mitigación de Lost in the Middle  \\
\midrule
Data    & Structured            & Knowledge Graphs \\
        & Unstructured          & open corpus (Common Crawl, PubMed, etc.) \\
        & LLM generated content & self-retrieval \\
\midrule
Process & Once                  & document augmentation \\
        & Iterative             & pseudo-relevance feedback (PRF) \\
        & Adaptative            & SKR (Selective Knowledge Retrieval)\\
\bottomrule
\end{tabularx}
\caption{Componente Augmentation}
\label{tab:augmentation}
\end{table}

Augmentation es el proceso el cual un modelo de lenguaje incorpora información adicional 
ya sea externa, como documentos, bases de conocimiento o corpus abiertos, o procesada internamente en diferentes etapas de su funcionamiento.
Los autores coinciden en que esta integración cumple objetivos clave: mejorar la precisión de las respuestas al aportar evidencia relevante, 
reducir las alucinaciones al contrastar el conocimiento implícito del modelo con fuentes verificables, y actualizar el conocimiento de los LLMs sin 
necesidad de reentrenarlos desde cero, ya que el acceso a información recuperada permite mantenerlos al día en dominios dinámicos como ciencia, medicina o derecho.
La Tabla~\ref{tab:augmentation} contiene tres ejes del componente augmentation: Tipos, Data y Process. Los cuales enriquecen el contexto disponible y mejoran la pertinencia
de la recuperación.

\paragraph{Tipos de augmentation:} \textcite{zhao2024rag} señalan que se puede aplicar en tres momentos distintos. En el pre-training, se integran representaciones estructuradas 
como knowledge graph embeddings que dotan al modelo de memoria explícita sobre entidades y relaciones. En la fase de fine-tuning, se ajustan los parámetros del modelo 
(y/o del recuperador) con datos del dominio para alinear formato y criterios de evaluación con la tarea. Para lograr este objetivo, existen estrategias como SFT el cual 
consiste en volver a entrenar el modelo usando datos etiquetados de la tarea objetivo para alinear el formato y criterios de salida. 
Este ajuste del modelo es efectivo, sin embargo resulta costoso y sensible a los hiperparámetros~\footnote{%
Variables de configuración externas al modelo que controlan cómo aprende durante el entrenamiento (e.g., tasa de aprendizaje, batch size, warmup, regularización, scheduler
y número de épocas), regulando el tamaño y el ritmo de las actualizaciones y la duración del entrenamiento.}%
e incluso a la semilla inicial (números aleatorios del entorno de entrenamiento , por ejemplo, la inicialización de pesos y el orden de muestreo de los datos), 
lo que impacta la selección de modelo y la fiabilidad de la evaluación~\textcite{casola2022pretrained}.
Por otra parte, PEFT modifican muy pocos parámetros usando prompt tunning (soft prompts) donde se aprenden vectores continuos que se anteponen al input para guiar la generación, 
sin cambiar la base del modelo y prefix tunning es una variante de prompt tunning el cual agrega prefijos de claves o valores en las capas de atención del transformer y a su vez
puede anteponer vectores al input. Asi, cada capa contiene el contexto que dirige la atención del modelo hacia patrones deseados, manteniendo intactos los pesos base del modelo.
El ajuste del recuperador en el método REPLUG se realiza mediante la divergencia de Kullback–Leibler (KL), que compara la distribución de probabilidad de documentos
estimada por el recuperador $P_{R}(d \mid x)$ con la distribución inducida por el modelo de lenguaje $Q_{\mathrm{LM}}(d \mid x,y)$. La función objetivo es:
\[
\mathrm{KL}\!\big(P_{R}(d\mid x)\,\|\,Q_{\mathrm{LM}}(d\mid x,y)\big)
= \sum_{d\in\mathcal{C}(x)} P_{R}(d\mid x)\,\log\!\frac{P_{R}(d\mid x)}{Q_{\mathrm{LM}}(d\mid x,y)}.
\]
Minimizar esta divergencia significa entrenar el recuperador para que su distribución de documentos recuperados se acerque a la que el propio LLM, tratado como caja negra, 
considera más adecuada. De este modo, el recuperador aprende a imitar las preferencias del LLM respecto a qué documentos son más relevantes, 
logrando así seleccionar de manera más efectiva aquellos fragmentos que aportan información útil. Este alineamiento mejora el \textit{recall@k} y asegura que el contexto
suministrado al generador no solo sea pertinente, sino también consistente con la forma en que el LLM procesa y utiliza la evidencia externa~\textcite{gao2023rag}.

UPRISE, por su parte, entrena un prompt retriever usando a un LLM congelado como supervisor: el LLM puntúa qué prompts (de un banco de plantillas) 
ayudan más a producir buenas respuestas para cada entrada; esos puntajes se convierten en señales de entrenamiento para que el retriever aprenda a seleccionar, 
en inferencia y sin reentrenar el LLM, los prompts más útiles en zero-shot\footnote{Resolver una tarea sin ejemplos etiquetados de esa tarea durante el ajuste}, elevando la calidad del contexto que finalmente consume el 
generador~\textcite{zhao2024rag,fan2024ragllm}.

Finalmente, en la inferencia, se aplican métodos sin reentrenamiento: por ejemplo, 
RARR (Retrieve-then-Read-then-Revise) refina las respuestas con evidencia recuperada, mientras que la mitigación de Lost in the Middle reorganiza documentos recuperados
para que el modelo aproveche mejor la ventana de contexto.

\paragraph{Datos:} \textcite{fan2024ragllm} destacan tres clases. La información estructurada, como los Knowledge Graphs, es fundamental para tareas de razonamiento 
factual ya que permite modelar entidades y relaciones explícitas. La información no estructurada, como corpus abiertos (Common Crawl, PubMed, Wikipedia), se ha vuelto estándar en open-domain QA.
Tal como explican~\textcite{gao2023rag} estos corpus aportan amplitud temática, pero también requieren mecanismos de filtrado, chunking y re-ranking 
para evitar ruido y mitigar el problema de Lost in the Middle. Finalmente, surge la categoría de contenido generado por LLM, donde el propio modelo actúa 
como fuente en esquemas de self-retrieval, generando y reutilizando conocimiento de manera autónoma. En este caso, los LLMs generan documentos intermedios, 
hipótesis o representaciones que se utilizan posteriormente como consultas o evidencia, empleando un módulo crítico que evalúa si es necesario recuperar información externa o 
si el propio contenido generado basta para resolver la tarea. Gracias a este enfoque, como subraya~\textcite{fan2024ragllm}, se abre la posibilidad de que los modelos se autocomplementen y 
reutilicen su conocimiento previo sin depender exclusivamente de bases externas.

\paragraph{Procesos de augmentation:} se clasifican en tres modalidades según~\textcite{zhao2024rag}. 
Augmentation puede aplicarse una sola vez (Once), como en el caso de la document augmentation, 
donde se enriquece directamente la entrada con información adicional antes de la generación. 
Otra modalidad es la iterativa, que emplea técnicas como el pseudo-relevance feedback (PRF), 
mediante el cual la consulta inicial se reformula a partir de los resultados recuperados, repitiendo el ciclo 
para refinar la relevancia. 
La modalidad adaptativa es cuando el sistema decide dinámicamente si conviene recuperar 
información o no. Un ejemplo de esto es Selective Knowledge Retrieval (SKR), que evita búsquedas 
innecesarias cuando el modelo ya posee el conocimiento suficiente, reduciendo costes y minimizando la 
incorporación de ruido.


\begin{table}[htbp]
\centering
\begin{tabularx}{\textwidth}{l l X}
\toprule
\textbf{Categoría} & \textbf{Subcategoría / Tipo} & \textbf{Ejemplos} \\ \midrule
\multirow{4}{*}{Tipos} 
  & Transformers      & GPT, BART, T5 \\
  & LSTM             & Modelos secuencia a secuencia tradicionales \\
  & GANs            & Generación adversarial en imágenes y texto \\
  & Diffusion Models  & Imagen, audio y video \\ \midrule
\multirow{3}{*}{Enhancements}
  & Prompt Engineering   & Diseño de instrucciones, chain of thought, step-back prompts \\
  & Generator Fine-tuning& Ajuste del modelo al dominio específico \\
  & Decoding Tuning      & Beam search, nucleus sampling, top-k sampling \\ \bottomrule
\end{tabularx}
\caption{Componente Generator}
\label{tab:generator}
\end{table}

El componente generador (ver Tabla~\ref{tab:generator}) es el encargado de producir texto, imágenes u otro tipo de contenido. La capacidad del generador no depende únicamente de su arquitectura sino de un conjunto 
de estrategias que optimizan su rendimiento y controlan la calidad de las salidas. Estas mejoras incluyen prompt engineering, el fine-tuning especializado en dominios y 
ajustes en los métodos de decodificación para equilibrar coherencia y diversidad en la generación. En este sentido, comprender tanto los tipos de modelos como las técnicas de optimización resulta fundamental para evaluar el papel del \textit{Generator} 
en sistemas avanzados como los de Retrieval-Augmented Generation (RAG), donde la combinación de arquitectura y optimización garantiza la generación de respuestas 
más fiables, contextualizadas y relevantes 
\parencite{casola2022pretrained,fan2024ragllm,zhang2025hallucination}

Los transformers son la arquitectura más influyente en NLP ya que, según lo menciona \textcite{casola2022pretrained}, su capacidad de manejar dependencias a largo plazo 
mediante mecanismos de auto-atención ha permitido el desarrollo de modelos como GPT, BART y T5, los cuales han superado ampliamente a enfoques previos y marcado un cambio 
de paradigma en la generación de lenguaje. Los LSTM (Long Short-Term Memory) se empleaban en arquitecturas secuencia a secuencia, resolviendo problemas de memoria en redes recurrentes, 
aunque presentaban limitaciones en el escalamiento y en la captura de dependencias largas \parencite{jing2024vecdb}. Por otro lado, introdujeron un enfoque basado 
en el enfrentamiento entre un generador y un discriminador, logrando avances en la creación de imágenes y, posteriormente, en la generación de texto. 
Al mismo tiempo, se desarrollaron las GANs (Generative Adversarial Networks) las cuales plantean un aprendizaje adversarial \footnote{ Dos modelos 
con objetivos opuestos se enfrentan para la mejora mutua} entre un generador y discriminador, donde modelan distribuciones complejas 
y producir datos sintéticos realistas, inicialmente en imágenes, pero posteriormente también en texto y audio.
Más recientemente los Diffusion Models
se han consolidado en el ámbito multimodal (imagen, audio y video), gracias a su capacidad de producir datos de alta calidad a partir de procesos de ruido inverso,
ampliando las fronteras de la generación más allá del texto. 

Con respecto a la mejora, el Prompt Engineering ha demostrado ser una técnica central para guiar los modelos hacia respuestas más precisas y controladas. Según \textcite{zhang2025hallucination}
incluye el diseño de instrucciones específicas, así como métodos como chain of thought o step-back prompting, que mejoran la coherencia y reducen alucinaciones en los resultados.
Otra estrategia es el fine-tuning que consiste en adaptar un modelo a un dominio concreto, optimizando su capacidad de manejar información especializada y aumentando su fiabilidad en tareas críticas,
por ejemplo en contextos científicos o legales.  
El Decoding Tuning se refiere al ajuste de los métodos utilizados por un modelo generativo para decidir qué palabra o token producir a continuación. 
En lugar de limitarse a elegir siempre la opción con mayor probabilidad, lo cual puede generar textos repetitivos y poco naturales, se aplican estrategias 
que permiten modular el balance entre coherencia y diversidad. Entre ellas, el beam search explora varias rutas posibles de generación en paralelo para seleccionar 
la más prometedora, garantizando coherencia aunque sacrificando creatividad. Por su parte, el nucleus sampling restringe las opciones a un conjunto dinámico de palabras 
que concentran la mayor parte de la probabilidad acumulada y selecciona una de ellas de manera aleatoria, lo que produce textos más naturales y variados. Finalmente, 
el top-k sampling limita las elecciones a las k palabras más probables y elige una de ellas de acuerdo con su distribución de probabilidad, lo que ofrece un equilibrio 
controlado entre precisión y diversidad. Estas estrategias permiten modular el equilibrio entre coherencia y diversidad en la generación de texto, 
adaptando el comportamiento del modelo sin necesidad de modificar su arquitectura \parencite{hu2024ragrau}.



\subsubsection{Fases de Implementación}
\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
  \def\step{-1.5}
  \def\y{0}

  % Paso I: Ingesta
  \pill{\y}{softcream}{I}{Ingesta}
  \pgfmathsetmacro\y{\y+\step}

  % Paso II: Preprocesamiento
  \pill{\y}{softpink}{II}{Preprocesamiento}
  \pgfmathsetmacro\y{\y+\step}

  % Paso III: Vectorización
  \pill{\y}{softblue}{III}{Vectorización (Embeddings)}
  \pgfmathsetmacro\y{\y+\step}

  % Paso IV: Indexación en Vector DB
  \pill{\y}{softpeach}{IV}{Vector DB}
  \pgfmathsetmacro\y{\y+\step}

  % Paso V: Recuperación
  \pill{\y}{softyellow}{V}{Recuperación}
  \pgfmathsetmacro\y{\y+\step}

  % Paso VI: Re-ranking
  \pill{\y}{softgreen}{VI}{Re-ranking}
  \pgfmathsetmacro\y{\y+\step}

  % Paso VII: Construcción de Contexto
  \pill{\y}{softcream}{VII}{Construcción de Contexto}
  \pgfmathsetmacro\y{\y+\step}

  % Paso VIII: Generación
  \pill{\y}{softpink}{VIII}{Generación (LLM)}
  \pgfmathsetmacro\y{\y+\step}

  % Paso IX: Evaluación
  \pill{\y}{softblue}{IX}{Evaluación}
\end{tikzpicture}
\end{center}
\caption{Pipeline de RAG}
\label{fig:pipeline-rag}
\end{figure}

Las fases de implementación de los sistemas RAG son una secuencia de pasos (Ver Figura.~\ref{fig:pipeline-rag}) diseñados para enriquecer la generación de respuestas con
información externa y actualizada de estos sistemas. 
Según \textcite{tabassum2020}, todo inicia con la ingesta y el preprocesamiento de los datos, donde las técnicas de limpieza y segmentación garantizan 
que el texto sea utilizable para fases posteriores. Luego se vectoriza mediante embeddings los cuales transforman los fragmentos de texto
en representaciones numéricas para la recuperación semántica tal como lo expresa \textcite{minaee2021}. Para \textcite{hu2024ragrau}, estos vectores se almacenan en una base de datos vectorial 
la cual permite realizar búsquedas eficientes por similitud semántica entre la consulta y los fragmentos de texto indexados.
Además, esta VDB permite trabajar en conjunto con índices tradicionales como BM25, dando lugar a la recuperación híbrida.
Con los datos obtenidos, como lo señala \textcite{sarthi2024raptor}, 
es necesario aplicar procesos de re-ranking y filtrado que permiten priorizar las partes más relevantes dando lugar a la construcción de contexto, en la que los fragmentos de texto seleccionados 
se organizan, se condensan para reducir su extensión y se adaptan al límite de entrada del modelo para conformar un contexto optimizado y manejable que servirá de entrada
al modelo generativo. En la fase final, \textcite{knollmeyer2024benchmarking} sostiene que el LLM debe complementarse con mecanismos de evaluación para asegurar la 
fidelidad y coherencia de las respuestas generadas.

\paragraph{Ingesta:} Es el comienzo del pipeline de RAG, su objetivo es recopilar, normalizar y preparar las fuentes de información
que luego serán usadas para los procesos de recuperación y generación. La ingesta implica recolectar distintas fuentes ya sean libros, literatura científica, corpus abiertos, etc., y convertirlos en un formato manejable por el 
sistema. De acuerdo con \textcite{gao2023rag}, \textcite{ibrihich2022review} y \textcite{jing2024vecdb} estos datos se almacenan como texto estructurado, semiestructurado, sin estructura o heterogéneos, dependiendo de su naturaleza y del tipo de procesamiento requerido.

\paragraph{Preprocesamiento:} Dentro de la ingesta de datos es necesario limpiar y estructurar el texto en bruto para que sea utilizable en etapas posteriores. De acuerdo con \textcite{tabassum2020} los procesos de tokenización\footnote{Proceso de segmentar el texto en unidades básicas (palabras, signos de puntuación o incluso caracteres), por ejemplo: “NLP”, “is”, “the”, “future”, etc},
normalización, eliminación de stopwords, lematización y segmentación en oraciones permiten transformar datos no estructurados en unidades coherentes y consistentes, reduciendo así el ruido que podría propagarse en fases posteriores. 
Este paso es importante ya que el texto procesado debe adaptarse a las limitaciones de los LLM, lo cual implica fragmentar en chunks y generar representaciones vectoriales, siendo así que \textcite{knollmeyer2024benchmarking} 
afirma que la calidad del preprocesamiento incide directamente en dimensiones de evaluación como la relevancia contextual y la fidelidad de las respuestas. Una preparación deficiente del texto puede introducir ruido en el contexto recuperado 
y favorecer inconsistencias en la generación, lo cual incrementa el riesgo de alucinaciones; en este sentido, se entiende que el preprocesamiento actúa como una primera barrera de mitigación de errores (\textcite{zhang2025hallucination}).

\paragraph{Vectorización:} Los fragmentos de texto una vez preprocesados se transforman en embeddings \footnote{Representación numérica que codifica el significado del texto en un espacio vectorial} permitiendo capturar la semántica de tal manera que 
los textos similares estén próximos en el espacio vectorial. De acuerdo con \textcite{hu2024ragrau} existen dos enfoques: la recuperación dispersa (sparse) como por ejemplo 
TF-IDF, BM25 y la recuperación densa, la cual es basada en embeddings generados por modelos preentrenados como BERT. El primer enfoque se basa en la frecuencia estadística del término en los documentos \parencite{fan2021pretraining}.
Posteriormente, métodos como Latent Semantic Analysis (LSA) incorporaron técnicas de reducción dimensional a la matriz término-documento mediante descomposición en valores singulares, lo que permitió identificar estructuras semánticas latentes y superar 
limitaciones de los modelos basados únicamente en frecuencia. Estudios recientes lo consideran el punto de inicio de los embeddings neuronales, los cuales ofrecen representaciones más contextuales y expresivas que permiten clasificar y recuperar el texto adecuadamente
\parencite{fan2021pretraining,minaee2021}. Con la popularización del deep learning, surgieron representaciones densas como Word2Vec y GloVe, las cuales son capaces de capturar relaciones semánticas entre palabras en espacios vectoriales continuos. Los transformers son una 
evolución del enfoque denso donde, según \textcite{casola2022pretrained}, los transformers preentrenados al combinarse con mecanismos de atención y un preentrenamiento masivo, producen vectores contextualizados robustos capaces de capturar relaciones de dependencia a largo
plazo entre palabras.

\paragraph{Vector DB:}
Los VDBs son importantes dentro del desarrollo de sistemas RAG, pues permiten almacenar y organizar representaciones densas de texto para su posterior
recuperación eficiente. Estos se emplean en lugar de los índices invertidos tradicionales, que basan la búsqueda en coincidencias exactas de palabras o estadísticas 
de frecuencia, lo cual dificulta la detección de sinónimos y relaciones semánticas. De acuerdo con \textcite{jing2024vecdb}, los VDBs están diseñados para manejar 
grandes volúmenes de representaciones de alta dimensionalidad y soportar búsquedas por similitud mediante métricas como la distancia euclidiana o el coseno. Además, 
como menciona \textcite{fan2021pretraining}, el uso de representaciones preentrenadas mejora la calidad de los índices, ya que los embeddings capturan relaciones 
semánticas profundas entre consultas y documentos. Para optimizar estas búsquedas se incorporan técnicas de indexación, que organizan los embeddings en estructuras preparadas para la búsqueda aproximada de vecinos más cercanos \parencite{ma2025vector}.  

Para llevar a cabo estas operaciones, los VDBs recurren a diferentes enfoques de búsqueda, tales como:

\begin{itemize}
    \item \textbf{Nearest Neighbor Search (NNS):} compara de manera exhaustiva el vector de consulta con todos los vectores almacenados. Garantiza la máxima precisión,
    pero resulta poco escalable con colecciones masivas de datos \parencite{ma2025vector}.
    
    \item \textbf{Graph-Based Approach:} emplea estructuras de grafos, como HNSW, que permiten navegar de forma eficiente entre vectores y localizar vecinos cercanos 
    en pocas iteraciones, reduciendo considerablemente los tiempos de búsqueda \parencite{jing2024vecdb}.
    
    \item \textbf{Tree-Based Approach:} organiza los vectores en estructuras jerárquicas (ej. KD-Tree, Ball-Tree), dividiendo recursivamente el espacio. Es eficiente 
    en dimensionalidades bajas o medias, pero pierde rendimiento en espacios de muy alta dimensión \parencite{fan2021pretraining}.
    
    \item \textbf{Approximate Nearest Neighbor Search (ANNS):} sacrifica una mínima exactitud para ganar eficiencia y escalabilidad. Métodos como IVF o PQ permiten 
    realizar búsquedas rápidas en grandes colecciones, siendo muy usados en producción \parencite{ma2025vector}.
\end{itemize}

Cuando una consulta es transformada en vector, el sistema no necesita recorrer todo el corpus, sino que localiza la región del espacio donde se ubica y 
devuelve los vecinos más próximos. Para lograr esta eficiencia se emplean técnicas de indexación que organizan los embeddings en estructuras optimizadas para 
la búsqueda aproximada. Se implementa mediante enfoques como:
\begin{itemize}
    \item \textbf{HNSW (Hierarchical Navigable Small World graphs):} construye un grafo jerárquico navegable que permite acceder rápidamente a los vecinos más cercanos, reduciendo la complejidad de la búsqueda \parencite{ma2025vector}.
    \item \textbf{IVF (Inverted File Index):} agrupa los vectores en listas invertidas de centroides, de modo que la comparación se restringe a las regiones más prometedoras del espacio \parencite{ma2025vector}.
    \item \textbf{PQ (Product Quantization):} divide los vectores en subespacios y los representa de forma comprimida, lo que acelera significativamente el cálculo de distancias en colecciones de gran tamaño \parencite{ma2025vector}.
\end{itemize}

\paragraph{Recuperación:} Esta fase es el punto donde el usuario interactúa con el conocimiento almacenado y su eficiencia y eficacia se reflejan en la calidad 
de la respuesta obtenida. Como señalan \textcite{fan2021pretraining}, la recuperación en sistemas de IR se centra en estimar la relevancia entre la consulta y los documentos, devolviendo una lista ordenada
de resultados que responden a la necesidad de información.  
De acuerdo con \textcite{hambarde2023ir}, los enfoques de recuperación se dividen en cuatro categorías principales:

\begin{itemize}
    \item \textbf{Convencional}: se basa en modelos clásicos como el booleano, el de espacio vectorial o BM25, donde la similitud depende de coincidencias exactas de términos y estadísticas de frecuencia. Estos métodos son eficientes para recuperar información en grandes colecciones, aunque limitados ante problemas de sinonimia o polisemia \parencite{hambarde2023ir}.
    
    \item \textbf{Dispersa (sparse)}: representa consultas y documentos como vectores escasos. Una línea reciente ha mejorado este enfoque mediante esquemas neuronales que aprenden a ponderar términos o expandir documentos, como DeepCT o Doc2Query, integrando así señales semánticas en modelos léxicos tradicionales \parencite{hambarde2023ir,gao2023rag}. 
    
    \item \textbf{Densa (dense)}: emplea embeddings generados por modelos preentrenados (p. ej., BERT o variantes de transformers), que permiten medir similitud en espacios vectoriales continuos. Según \textcite{hu2024ragrau}, este enfoque captura relaciones semánticas profundas y resulta más eficaz en tareas de recuperación semántica abierta. Ejemplos destacados son Dense Passage Retrieval (DPR) y ANCE \parencite{hambarde2023ir}. 
    
    \item \textbf{Híbrida}: combina la precisión léxica de los enfoques dispersos con la riqueza semántica de los densos. Como indican \textcite{zhai2024llmIR}, este tipo de estrategias incrementa tanto la cobertura como la pertinencia, al integrar coincidencias exactas y relaciones semánticas, lo que resulta fundamental para sistemas RAG modernos.
\end{itemize}

\paragraph{Re-Ranking:} Una vez recuperada la información requerida, el sistema aplica técnicas de ranking para refinar el orden y priorizar los fragmentos más relevantes.
Este proceso puede apoyarse en métodos tradicionales como BM25, en algoritmos de learning to rank (LTR)\footnote{ LTR se refiere al uso de algoritmos de aprendizaje supervisado para optimizar el orden de los resultados en un sistema de recuperación de información.}.
Como explican \textcite{bernard2025fate}, LTR permitió combinar múltiples señales de ranking en un modelo unificado, superando las limitaciones de los métodos
heurísticos. Entre los algoritmos más representativos se encuentran RankNet, LambdaRank y LambdaMART, que introdujeron redes neuronales y técnicas de gradiente 
boosting para mejorar la calidad del ordenamiento. Según \textcite{hambarde2023ir}, el LTR representa un punto de transición entre los métodos tradicionales y los basados en 
deep learning, abriendo el camino hacia arquitecturas neuronales.

Dentro de los enfoques de deep learning para re-ranking, se distinguen los siguientes componentes principales:

\begin{itemize}
    \item \textbf{Representación basada en modelos}: codifican la consulta y el documento de manera independiente en un espacio vectorial, y luego miden la similitud entre ambas representaciones. Son eficientes, aunque limitados en capturar interacciones finas entre texto y consulta \parencite{minaee2021}.
    
    \item \textbf{Interacción basada en modelos}: en lugar de representaciones independientes, modelan explícitamente las relaciones término a término entre consulta y documento. Esto permite capturar señales de correspondencia más precisas, a costa de mayor costo computacional \parencite{jiang2024textmatching}.
    
    \item \textbf{Modelos attention-based}: utilizan mecanismos de atención (como el self-attention de los transformers) para ponderar dinámicamente qué partes del texto y la consulta son más relevantes, mejorando la identificación de dependencias contextuales \parencite{hambarde2023ir}.
    
    \item \textbf{Transformers preentrenados}: arquitecturas como BERT, RoBERTa o T5 han revolucionado el re-ranking al permitir un entendimiento contextual profundo. Estos modelos, ajustados con tareas de recuperación, han demostrado superar ampliamente métodos previos \parencite{zhai2024llmIR}.
\end{itemize}

\paragraph{Construcción de Contexto:} 
En esta etapa, los fragmentos recuperados y reordenados se integran para formar un contexto que será utilizado por el modelo generador. El objetivo es seleccionar 
fragmentos de texto relevantes, eliminar redundancias y organizar la información de manera que aporte evidencia sólida a la respuesta final. De acuerdo con 
\textcite{sarthi2024raptor}, este proceso implica técnicas de agregación y filtrado que permiten priorizar el contenido más pertinente, evitando 
la sobrecarga de información irrelevante. Asimismo, \textcite{gao2023rag} destacan que la construcción de contexto debe considerar las limitaciones de los modelos 
de lenguaje, como la longitud máxima de entrada, lo cual obliga a fragmentar, resumir o comprimir la información antes de su uso. Una preparación deficiente puede introducir ruido y
aumentar el riesgo de alucinaciones.  
En investigaciones recientes se han propuesto enfoques más avanzados, como la construcción jerárquica de contexto o el uso de resúmenes intermedios, que 
buscan maximizar la relevancia semántica y mantener la coherencia global en la generación de respuestas \parencite{hambarde2023ir}.

\paragraph{Generación (LLM):} Durante esta fase, el modelo de lenguaje no solo transforma los fragmentos de texto en un texto general, sino que también decide cómo integrar la información 
recuperada con su conocimiento preentrenado. En la literatura, este proceso se conceptualiza mediante diferentes estrategias de fusión, que buscan equilibrar el aporte
externo con las capacidades internas del modelo \parencite{zhao2024rag,hambarde2023ir}. Las técnicas de fusión son:  

\begin{itemize}
    \item \textbf{Query-based Fusions}: la información recuperada se incorpora desde la etapa de entrada, concatenando la consulta con los pasajes relevantes para que el modelo los procese conjuntamente. Este enfoque, también denominado \textit{early fusion}, es sencillo de implementar y aprovecha la arquitectura del transformador para contextualizar documentos y consulta en un mismo espacio semántico \parencite{gao2023rag}.  

    \item \textbf{Logits-based Fusions}: en este caso, la fusión ocurre en la etapa de salida, combinando las distribuciones de probabilidad (logits) producidas por el modelo generador con las provenientes de la información recuperada. De esta forma, se refuerzan las respuestas que cuentan con evidencia externa, reduciendo la probabilidad de alucinaciones \parencite{zhai2024llmIR}.  

    \item \textbf{Latent Fusions}: aquí la integración se realiza en un espacio latente intermedio, donde los embeddings de la consulta y los documentos recuperados se mezclan con las representaciones internas del LLM. Según \textcite{hu2024ragrau}, esto permite un balance más fino entre lo que sabe el LLM y lo que aporta la recuperación externa.  
\end{itemize}

En conjunto, estas estrategias reflejan que la generación no es un proceso lineal, sino una etapa flexible donde se decide cómo y en qué nivel se integra la 
recuperación con el modelo. Como señalan \textcite{zhao2024rag}, la elección del mecanismo de fusión es determinante para la fidelidad y coherencia de las respuestas 
generadas.

\paragraph{Evaluación:} 
La evaluación en los sistemas RAG es un proceso donde no se centra únicamente en evaluar la calidad de la respuesta generada, sino que debe evaluarse en todas las fases de implementación. 
Empezando por el retriever, donde se verifica que esté encontrando evidencia relevante y suficiente. Para llevar a cabo esta verificación se utilizan métricas tradicionales como Recall@k, nDCG o MRR que permiten  
cuantificar qué proporción de los documentos útiles fueron efectivamente recuperados. Es importante verificar que estén recuperando los documentos adecuadamente, ya que la etapa de generación no podrá producir respuestas fieles si carece de evidencia adecuada, por más sofisticado que sea el generador.  
En segundo lugar, medimos la generación más allá de indicadores como BLEU, ROUGE o F1, introduciendo el concepto de fidelidad o groundedness, que examina si el texto producido está sustentado en las fuentes recuperadas, reduciendo el riesgo de alucinaciones. 
Este criterio debe complementarse con la atribución de citas, es decir, si el sistema enlaza explícitamente sus afirmaciones con los fragmentos de textos extraídos, lo que facilita la trazabilidad \parencite{zhang2025hallucination}.  
Finalmente, es necesaria una evaluación general del sistema donde se considere la utilidad práctica en un escenario real. Aquí entran en juego dimensiones como eficiencia (latencia y costo), robustez frente a consultas adversas y adaptabilidad en dominios específicos. \textcite{knollmeyer2024benchmarking} destacan que requiere combinar métricas automáticas con juicios humanos o LLM-as-a-judge calibrados.  
Sin embargo, en los últimos años se ha propuesto integrar criterios de Fairness, Accountability, Transparency and Ethics (FATE) como parte de la evaluación. Según \textcite{bernard2025fate}, la confianza en los sistemas de recuperación y generación depende no solo de su precisión, sino también de que sean justos (sin sesgos desproporcionados hacia ciertos grupos), responsables (documentando el procedimiento 
de evaluación y anotadores), transparentes (explicando cómo se recuperó y procesó la información) y éticos (respetando la privacidad, la diversidad de fuentes y evitando desinformación).  
Incluir FATE como un parámetro de evaluación permite generar confianza social y regulatoria, además de estar alineado con la calidad técnica. Como sugiere \textcite{ramdurai2025llm}, la evaluación debe ser iterativa y multimodal: incluir tanto experimentos controlados en benchmarks como pruebas de usabilidad en escenarios reales. De esta manera se garantiza que el sistema sea confiable, útil, socialmente responsable y no solamente funcione bien en métricas numéricas.  

\subsubsection{Paradigmas}
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[x=1cm,y=1cm]
\tikzset{
  mindoval/.style={
    rounded corners=18pt,
    draw=black!70,
    line width=0.7pt,
    minimum width=3.6cm,
    minimum height=1.1cm,
    inner sep=6pt,
    fill=white
  },
  mindcenter/.style={
    rounded corners=8pt,
    draw=black!70,
    line width=0.9pt,
    minimum width=4.0cm,
    minimum height=1.2cm,
    inner sep=4pt,
    fill=softrose,
    font=\bfseries
  },
  mindarrow/.style={-latex, line width=0.9pt}
}

% ================== Parámetros  ==================
\def\sideW{4.5cm}  % MISMO ancho para ambos hijos de nivel 1
\def\sideX{4.0}    % desplazamiento horizontal simétrico de hijos
\def\sideY{2.7}    % desplazamiento vertical de hijos
\def\suby{-2.0}    % caída vertical de subnodos de RAG
\def\dx{3.8}       % separación horizontal de subnodos de RAG
\def\childShift{-1.5}

% --- Nodo central, exactamente en el centro de la figura
\node[mindcenter] (paradigmas) at (0,0) {Paradigmas};

% --- Hijos de primer nivel (SIMÉTRICOS respecto del centro)
\node[mindoval, text width=\sideW, align=center]
  (rag) at ($ (paradigmas.south) + (-\sideX - \childShift, -\sideY) $)
  {Retrieval-Augmented Generation (RAG)};

% Mover Hallucination a la izquierda (acércalo a RAG)
\node[mindoval, text width=\sideW, align=center, xshift=-1.2cm] % <-- ajusta -1.2cm a gusto
  (hallucination) at ($ (paradigmas.south) + (\sideX - \childShift, -\sideY) $)
  {Hallucination Mitigation};

% --- Flechas desde el centro (salida simétrica)
\draw[mindarrow] (paradigmas.south) --(rag.north);
\draw[mindarrow] (paradigmas.south) -- (hallucination.north);

% --- Punto común bajo RAG para los subnodos
\coordinate (ragout) at ($(rag.south)+(0,-0.05)$);

% --- Subnodos de RAG (compactos y CENTRADOS respecto a RAG)
\node[mindoval, text width=3.2cm, align=center]
  (naiverag)    at ($(ragout)+(-\dx,\suby)$) {Naive RAG};
\node[mindoval, text width=3.2cm, align=center]
  (advancedrag) at ($(ragout)+(0,\suby)$)    {Advanced RAG};
\node[mindoval, text width=3.2cm, align=center]
  (modularrag)  at ($(ragout)+(\dx,\suby)$)  {Modular RAG};

% --- Flechas a subnodos saliendo del mismo punto
\draw[mindarrow] (ragout) -- (naiverag.north);
\draw[mindarrow] (ragout) -- (advancedrag.north);
\draw[mindarrow] (ragout) -- (modularrag.north);

\end{tikzpicture}
\end{center}
\caption{Paradigmas principales en RAG}
\label{fig:paradigmas}
\end{figure}

La Figura \ref{fig:paradigmas} muestra el campo de Retrieval-Augmented Generation (RAG) que, según \textcite{gao2023rag} distinguen los paradigmas Naive RAG, Advanced RAG y Modular RAG, que representan un progreso
desde enfoques básicos de recuperación y generación hasta arquitecturas modulares y flexibles. A su vez, \textcite{zhang2025hallucination} clasifican los problemas en el área de 
Hallucination Mitigation en dos ejes: retrieval failure (fallos en fuentes, consultas o recuperadores) y generation deficiency 
(ruido o conflicto contextual y límites de capacidad). Finalmente, 
\textcite{zhao2024rag} proponen fundaciones de RAG según la forma en que el retriever complementa al generador: Query-based RAG, Latent Representation-based RAG,
Logit-based RAG y Speculative RAG. De esta forma se expande la aplicación de RAG a múltiples dominios y modalidades.



\paragraph{Tipos de Paradigmas}

\begin{itemize}
    \item \textbf{Retrieval-Augmented Generation (RAG) — Gao et al. (2023)}
    \begin{itemize}
        \item Naive RAG: enfoque básico de recuperación y generación.
        \item Advanced RAG: incorpora optimizaciones como segmentación fina, re-ranking y recuperación iterativa.
        \item Modular RAG: paradigma flexible con módulos especializados para búsqueda, memoria, alineación y validación.
    \end{itemize}

    \item \textbf{Tipos de RAG — Zhao et al. (2024)}
    \begin{itemize}
        \item Query-based RAG: integra directamente la consulta y la información recuperada en la entrada del generador.
        \item Latent Representation-based RAG: incorpora la información recuperada como representaciones latentes en el modelo generativo.
        \item Logit-based RAG: combina la información de recuperación en la fase de decodificación a nivel de logits.
        \item Speculative RAG: sustituye pasos de generación por recuperación para acelerar y reducir costes.
    \end{itemize}

    \item \textbf{Hallucination Mitigation — Zhang \& Zhang (2025)}
    \begin{itemize}
        \item Retrieval failure: fallos en las fuentes, consultas, recuperadores o estrategias de recuperación.
        \item Generation deficiency: deficiencias en la generación como ruido, conflictos contextuales, middle curse, problemas de alineación y límites de capacidad.
    \end{itemize}
\end{itemize}

\subsubsection{Evaluación y Métricas}

La evaluación de los sistemas de Recuperación de Información (IR) y de Retrieval-Augmented Generation (RAG) ha sido objeto de un creciente interés en la literatura reciente. 
En el ámbito de IR, \textcite{bernard2025fate} destacan que las nociones de equidad, transparencia y responsabilidad requieren enfoques diversos: mientras la \textit{fairness} 
se mide mayormente con métricas automáticas, la \textit{transparency} y la \textit{accountability} suelen evaluarse mediante auditorías y estudios con usuarios. En contraste, 
la dimensión ética carece de métricas claras y se asocia más a aspectos como privacidad y seguridad.  

En el área de RAG, \textcite{knollmeyer2024benchmarking} identifican cinco dimensiones clave de evaluación: relevancia del contexto, fidelidad (\textit{faithfulness}), relevancia 
de la respuesta, corrección y calidad de las citas. Estas dimensiones permiten evaluar de manera más integral los sistemas que combinan recuperación y generación. Asimismo, 
\textcite{gao2023rag} señalan que, además de las métricas automáticas tradicionales de IR (precisión, recall, nDCG), resulta fundamental incluir evaluaciones humanas que capten 
aspectos como coherencia, transparencia y verificabilidad.  

\paragraph{Categorías de Evaluación}
\begin{itemize}
  \item \textbf{Enfoques de Evaluación — Gao et al. (2023)}
        \begin{itemize}
          \item Evaluación independiente por etapas: análisis separado de retrieval, augmentation y generation, con métricas específicas en cada módulo.
          \item Evaluación End-to-End: valoración directa de la salida final del sistema RAG, con dos variantes:
          \begin{itemize}
              \item Evaluación End-to-End automática: frameworks que miden habilidades clave como exactitud, fidelidad (\textit{faithfulness}), atribución de fuentes, reducción de alucinaciones y transparencia.
              \item Evaluación End-to-End con juicio humano: juicios expertos que valoran coherencia, verificabilidad, utilidad práctica y confianza.
          \end{itemize}
          \item Combinación de métricas: integración de métricas clásicas de recuperación (precisión, recall, nDCG), métricas de generación (coherencia, verificabilidad, calidad narrativa) y evaluación humana.
      \end{itemize}
    \item \textbf{Evaluación en IR con FATE — Bernard \& Balog (2025)}
    \begin{itemize}
        \item Fairness: métricas automáticas como \textit{top-k}, \textit{exposure} y \textit{pairwise metrics}.
        \item Transparency y Accountability: auditorías y estudios de usuarios, centrados en interpretabilidad y trazabilidad.
        \item Ethics: enfoques cualitativos vinculados a privacidad y seguridad.
        \item Tensiones: conflictos entre fairness individual vs. grupal y entre transparencia excesiva vs. carga cognitiva.
    \end{itemize}

    \item \textbf{Evaluación Clásica en IR — Hambarde \& Proença (2023)}
    \begin{itemize}
        \item Métricas de recuperación: precisión, recall, F1, MAP (Mean Average Precision), MRR (Mean Reciprocal Rank).
        \item Métricas de ranking: nDCG (Normalized Discounted Cumulative Gain) y calidad del ordenamiento.
        \item Evaluación de modelos neuronales: comparación con benchmarks tradicionales (TREC, MS MARCO).
        \item Dimensión multimodal: evaluación de IR en escenarios que integran texto, imágenes y audio.
    \end{itemize}

    \item \textbf{Evaluación en RAG — Knollmeyer et al. (2024); Gao et al. (2023)}
    \begin{itemize}
        \item \textit{Fase de Recuperación}: 
        \begin{itemize}
            \item Context relevance: grado de pertinencia de los documentos recuperados.
            \item Dataset quality: uso de bases de datos curadas y representativas (ej. Wikipedia, MS MARCO).
            \item Métricas aplicadas: precisión, recall, nDCG, cobertura de conocimiento.
        \end{itemize}

        \item \textit{Fase de Generación}: 
        \begin{itemize}
            \item Faithfulness: consistencia factual entre recuperación y generación.
            \item Answer relevance: utilidad de la respuesta respecto a la consulta.
            \item Correctness: exactitud de la información producida y reducción de alucinaciones.
            \item Métricas aplicadas: BLEU, ROUGE, métricas de consistencia factual y evaluaciones humanas.
        \end{itemize}

        \item \textit{Fase de Integración}: 
        \begin{itemize}
            \item Citation quality: precisión, trazabilidad y cobertura de las fuentes citadas.
        \end{itemize}

        \item \textit{Evaluación Global}: 
        \begin{itemize}
            \item Evaluadores mixtos: combinación de métricas automáticas (similaridad semántica, BLEU, ROUGE) con juicios humanos (coherencia, verificabilidad, utilidad práctica).
            \item Datasets: necesidad de colecciones específicas adaptadas a RAG, más allá de benchmarks tradicionales de IR.
        \end{itemize}
    \end{itemize}
    
\end{itemize}


\subsubsection{Futuro de RAG}

\paragraph{Desafíos Actuales:}
De acuerdo con~\textcite{zhai2024llmIR}, uno de los principales desafíos de los sistemas RAG es la generación de alucinaciones, que comprometen la confianza y 
limitan su uso en aplicaciones críticas. Asimismo, persisten limitaciones en la calidad de la recuperación y en la eficiencia computacional, especialmente 
en dominios donde se requiere información actualizada y especializada~\parencite{hu2024ragrau}. Además, \textcite{ramdurai2025llm} señala que la integración de RAG con otras
arquitecturas, como redes neuronales convolucionales, enfrenta problemas de escalabilidad y de adaptación a contextos heterogéneos. También es clave subrayar que persisten problemas éticos y de transparencia (FATE) en los sistemas de recuperación y generación.
En la literatura de~\textcite{bernard2025fate} muestra que aún no existen marcos estandarizados para evaluar imparcialidad, transparencia y responsabilidad en IR, lo que limita la confianza social en los sistemas RAG. 
Otro reto es la reproducibilidad y robustez de los modelos preentrenados, ya que la variabilidad en hiperparámetros y semillas afecta la consistencia de los resultados~\parencite{fan2021pretraining}.

\paragraph{Direcciones Potenciales:}  
Las líneas de avance incluyen el desarrollo de retrievers más robustos, la optimización de las interacciones entre modelo y recuperación, 
así como mecanismos de evaluación que prioricen la relevancia y la coherencia. \textcite{ramdurai2025llm} enfatiza la sinergia con arquitecturas multimodales y con sistemas 
capaces de combinar información textual, visual y estructurada. Asimismo, enfoques como RAPTOR, que introduce resúmenes recursivos y jerárquicos en la recuperación,
muestran cómo superar la fragmentación del contexto y mejorar la integración semántica \parencite{sarthi2024raptor}. Otra dirección destacada es la incorporación de 
bases de datos vectoriales como memorias semánticas persistentes, que permiten gestionar grandes volúmenes de embeddings, mitigar el olvido catastrófico de los modelos
 y ofrecer personalización en la recuperación~\parencite{jing2024vecdb}.
Finalmente, cabe mencionar que los pre-trained transformers aplicados a IR han abierto el camino hacia modelos más sofisticados para tareas de ranking y matching semántico, 
aunque todavía requieren diseños de preentrenamiento adaptados a los desafíos específicos de la recuperación de información~\parencite{fan2021pretraining}.

\paragraph{Perspectivas:}
A medio plazo, RAG y los LLM convergerán con la IR clásica hacia sistemas de acceso conversacional al conocimiento: motores de búsqueda capaces de generar respuestas enriquecidas con análisis propio,
pero con respaldo en documentación verificable, capaces de planificar, citar y aprovechar el historial conversacional. Incluso si los LLM siguen mejorando, la recuperación seguirá 
siendo un componente clave porque permite reducir costos, mantener la información siempre actualizada, control y gobernanza de la evidencia usada y en consecuencia se espera la aparición de arquitecturas integradas donde RAG funcione junto con
herramientas externas y módulos memoria trabajen de forma coordinada. Esta coordinación hará posible planificar y ejecutar tareas más largas o compuestas. La evaluación de estos sistemas ya no se limitará a medir métricas técnicas como la exactitud, sino que pondrá el énfasis en
 dos partes clave: la utilidad práctica y la veracidad, especialmente en contextos de alta exigencia.

