\chapter{Metodología}
\section{Revisión sistemática}
\label{sec:revision-sistematica}
Umbrella Review, según los lineamientos del Instituto Joanna Briggs (JBI), es un tipo de revisión sistemática que recopila y analiza evidencia secundaria, 
es decir, revisiones sistemáticas y metaanálisis ya publicados.
Su propósito es consolidar el conocimiento disponible, identificar coincidencias y contradicciones en la literatura existente, así como señalar vacíos de evidencia.
Para ello, requiere la elaboración de un protocolo previo que establezca criterios de inclusión y exclusión, estrategias de búsqueda y métodos de síntesis, garantizando 
un proceso transparente y riguroso.

Por otra parte, la estrategia de propagación de citaciones (Back-and-Forward Citation Propagating) complementa este enfoque al permitir encontrar dinámicamente la literatura. 
A través de la propagación de citaciones se amplía y actualiza la literatura encontrada en las bases de datos tradicionales. De este modo, se superan limitaciones como la 
indexación incompleta, las variaciones en el uso de palabras clave o la exclusión de ciertas publicaciones.


\subsection*{Metodología: Umbrella Review con Propagación de Citaciones}
Como parte de la metodología Umbrella Review es necesario establecer un protocolo para ejecutar la revisión.
Se han considerado las siguientes fases para dicho protocolo:
\begin{enumerate}
    \item \textbf{Propósito de la revisión}  
    
    La revisión se justifica en la necesidad de consolidar evidencia secundaria de calidad, aprovechando el enfoque de propagación de citaciones 
    para garantizar una búsqueda amplia, estructurada y actualizada.

    \item \textbf{Objetivos específicos}  

    Se definen los objetivos generales y específicos que guiarán la identificación de literatura mediante la propagación de citaciones, 
    así como el proceso de síntesis de resultados.

    \item \textbf{Criterios de inclusión y exclusión}  

    Se definen de manera general como la incorporación de revisiones y metaanálisis que sean pertinentes, de calidad y relacionados con el tema de estudio, 
    y la exclusión de aquellos trabajos que no cumplan con estos requisitos de relevancia.
    
    \item \textbf{Identificación del estudio semilla y propagación de citaciones}  
    
    La búsqueda se inicia en bases de datos académicas como \textit{Scopus}, \textit{Web of Science}, \textit{IEEE Xplore} o \textit{Google Scholar}, a fin de localizar 
    un estudio semilla (revisión o resumen amplio) que ofrezca una cobertura representativa del tema.  
    A partir de este estudio, se aplica la estrategia de Back-and-Forward Citation Propagation, que combina:  
    \begin{itemize}
        \item \textit{Backward citation:} revisión de las referencias citadas en el estudio semilla.  
        \item \textit{Forward citation:} identificación de trabajos más recientes que citan al estudio semilla.  
    \end{itemize}
    De este modo, el corpus de literatura se amplía progresivamente hasta alcanzar un punto de saturación en el que la propagación deja de aportar nueva evidencia relevante.

    \item \textbf{Selección de revisiones relevantes}  

    A partir de la propagación de citaciones, se aplican los criterios de inclusión - exclusión para determinar qué revisiones serán incorporadas al análisis.
    

    \item \textbf{Valoración de la calidad de la evidencia}  

    La calidad de los estudios se evalúa según los criterios definidos, garantizando su consistencia al tema de estudio.
    Para apoyar este proceso se usa una herramienta de análisis que facilite la organización y valoración sistemática de la evidencia.

    \item \textbf{Extracción de información clave}  

    De cada revisión seleccionada se extraerán datos esenciales, organizados en una tabla de extracción que incluirá:  
    \begin{itemize}
        \item Autor y año de publicación  
        \item Objetivo del estudio  
        \item Tipo de revisión  
        \item Número de estudios primarios incluidos  
        \item Principales hallazgos  
        \item Conclusiones generales  
        \item Limitaciones reportadas  
    \end{itemize}

    \item \textbf{Síntesis y representación de resultados}  

    Los hallazgos se organizarán en dos niveles complementarios:  
    \begin{itemize}
        \item \textbf{Tabular:} tablas comparativas de las revisiones incluidas.
        \item \textbf{Narrativo:} síntesis descriptiva de los principales hallazgos.  
        \item \textbf{Temático y visual:} mapas de evidencia y esquemas que reflejen la propagación de citaciones, mostrando las conexiones entre estudios clave.  
    \end{itemize}

    \item \textbf{Discusión y conclusiones}  

    Los resultados se interpretan desde una perspectiva crítica, destacando fortalezas, limitaciones y la evolución de la evidencia en el tiempo. Se identifican coincidencias y divergencias entre revisiones, así como vacíos de conocimiento, y se proponen líneas de investigación futura.
\end{enumerate}
En esta metodología, el Umbrella Review se utiliza como marco general para sintetizar evidencia secundaria a partir de revisiones de exhaustivas de la literatura, 
complementándose 
con la propagación de citaciones para integrar aportes recientes y 
reflejar la evolución del conocimiento disponible.







\section{Enfoque Design Science Research (DSR)}
De acuerdo con vom Brocke et al. \textcite{vombrocke2020}, Design Science Research, desarrollada en 1969, es un paradigma de resolución de problemas que busca mejorar el conocimiento humano mediante la creación de artefactos innovadores. En otras palabras, es una metodología que crea soluciones a problemas reales y, al mismo tiempo, genera conocimiento útil y aplicable sobre cómo diseñar estas soluciones. Las etapas que se aplicarán en el presente trabajo son las siguientes:

\begin{itemize}[align=left, label=--]
\item \textbf{Identificación del problema y motivación} \
En esta etapa se precisa el problema y se justifica por qué es necesaria una solución. De acuerdo con Peffers et al. (2008), esta etapa exige analizar el problema en detalle, descomponiéndolo en sus partes clave para identificar sus causas, efectos y alcance. Además, es crucial justificar la relevancia del problema, tanto desde una perspectiva teórica (es decir, cómo contribuye al conocimiento académico) como desde una perspectiva práctica (cómo afecta a organizaciones, usuarios o sistemas reales).
También implica explorar la literatura para verificar que el problema es relevante, desafiante y nuevo, lo que permite definir los límites del proyecto de investigación.

\item \textbf{Definir los objetivos para la solución} \
Se plantean los criterios que debe cumplir una solución exitosa basándose en el conocimiento existente y en la factibilidad técnica y organizacional.
Los objetivos deberán permitir construir algo efectivo y deseable, no solamente desde el ámbito académico sino también en el entorno en que se aplicará. Estos pueden expresarse en términos cualitativos o cuantitativos; el investigador establece aquí la meta hacia donde se dirigirá el artefacto.

\item \textbf{Diseño y desarrollo del artefacto} \
En esta etapa se construye una solución concreta, como un modelo, software o sistema, que responde directamente a los objetivos planteados. Para ello, se utiliza el conocimiento existente que fundamenta las decisiones del diseño y la estructura del artefacto. No solo se trata de crear algo, sino de asegurar que pueda ser comprendido, evaluado y replicado por otros.

\item \textbf{Demostración del uso del artefacto para resolver el problema} \
Se muestra cómo se usa el artefacto en un escenario real o simulado. Esta demostración no valida científicamente su efectividad, sino que muestra su aplicabilidad, evidenciando que el artefacto propuesto puede operar de forma efectiva.
Por su parte, vom Brocke et al. (2020) destacan que esta etapa es fundamental para conectar el diseño teórico con la realidad del usuario o del entorno organizacional, permitiendo detectar oportunidades de mejora antes de una evaluación rigurosa.

\item \textbf{Evaluación del desempeño del artefacto} \
Se busca medir su efectividad, eficiencia e impacto, aportando evidencia que justifique su valor y utilidad. Además, según vom Brocke et al. (2020), esta puede asumirse de forma continua mediante una evaluación 
formativa que permita ciclos iterativos de rediseño y mejora a lo largo del proceso de investigación.
\item \textbf{Comunicación de los resultados al público académico y profesional} \
Finalmente, esta etapa consiste en difundir de forma clara los resultados del diseño y de la investigación realizada.
\end{itemize}

Estos pasos están basados en el modelo clásico de DSR de Peffers (2008), que vom Brocke adapta y expande en su guía. En la Figura \ref{fig: DSR} muestra el ciclo de DSR  dedicado a Centinela. La fila superior muestra las 6 fases 
y los recuadros inferiores brindan mayor informacion acerca de cada fase. Además, el contenedor inferior resume tres puntos de entrada de investigación que guían las
 decisiones de diseño y delimitan el alcance. En conjunto, conectan el marco teórico con las decisiones técnicas y la evaluación, 
 asegurando coherencia a lo largo del proyecto.
\begin{landscape}
\begin{figure}[ht]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tikzpicture}[scale=0.9, transform shape, node distance=1cm and 1.5cm,
        squarednode/.style={rectangle, draw=black, fill=white, very thick, minimum size=5mm, text width=4cm},
        everynode/.style={align=center}, 
        rectalbecirc/.style={
        draw=black,           % borde negro
        fill=white,           % fondo blanco
        line width=0.1pt,           % grosor del borde
        rounded corners=9pt,  % esquinas redondeadas
        align=center,         % alinear texto al centro
        text width=3cm,       % ancho del texto
        minimum height=1cm,   % altura mínima del nodo
        inner sep=6.5pt,
        font=\small
        }
        ]
        % nodos
        %horizontal
        \node[squarednode] (problema) {Identificación del problema};
        \node[squarednode] (objetivos) [right=of problema] {Objetivos para la solución};
        \node[squarednode] (diseno) [right=of objetivos] {Diseño y desarrollo del artefacto};
        \node[squarednode] (demostracion) [right=of diseno] {Demostración};
        \node[squarednode] (evaluacion) [right=of demostracion] {Evaluación del desempeño};
        \node[squarednode] (comunicacion) [right=of evaluacion] {Comunicación de resultados};
        %vertical
        \node[rectalbecirc] (start) [below=of problema] {Centinela tiene que presentar mejores resultados en las respuestas al momento de buscar información y generar contenido de valor para el usuario.};
        \node[rectalbecirc] (obj) [below=of objetivos] {
            \vspace*{-\baselineskip} % reduce el espacio inicial de la lista
            \begin{itemize} [left=0pt]\setlength\itemsep{1pt}\setlength\leftmargini{1pt}
                \renewcommand\labelitemi{\tiny$\bullet$}
                \item Mejorar la precisión de las respuestas.
                \item Aumentar la relevancia del contenido generado.
                \item Optimizar la experiencia del usuario.
            \end{itemize}
        };
        \node[rectalbecirc] (design) [below=of diseno] {Se contempla un diseño que integra retriver, augmented y generation lo que da como resultado un buscador el cual su respuesta es en lenguaje natural.};
        \node[rectalbecirc] (dem) [below=of demostracion] {Se demuestra el uso del buscador en un entorno controlado, mostrando su capacidad para responder preguntas y generar contenido relevante.};
        \node[rectalbecirc] (eval) [below=of evaluacion] {Se evalúa el buscador mediante métricas estándar y FATE};
        \node[rectalbecirc] (com) [below=of comunicacion] {En el presente trabajo se presenta el proceso que se ha realizado para el desarrollo del buscador, así como los resultados obtenidos y recomendaciones para futuras mejoras.};

        %conexiones horizontales
        \draw[-to] (problema) -- (objetivos);
        \draw[-to] (objetivos) -- (diseno);
        \draw[-to] (diseno) -- (demostracion);
        \draw[-to] (demostracion) -- (evaluacion);
        \draw[-to] (evaluacion) -- (comunicacion);
        %conexiones verticales
        \draw[-to] (problema) -- (start);
        \draw[-to] (objetivos) -- (obj);
        \draw[-to] (diseno) -- (design);    
        \draw[-to] (demostracion) -- (dem);
        \draw[-to] (evaluacion) -- (eval);
        \draw[-to] (comunicacion) -- (com);

        %contenedor
        \node[draw=black, rounded corners=10pt, thick, minimum width=23cm, minimum height=4cm, fill=gray!5, anchor=north, name=contenedor] 
at ([yshift=-0.75cm]current bounding box.south) {};
        \coordinate (center) at (contenedor.center);
        \def\sep{7} % Separación horizontal
        % Elipses dentro del contenedor
        \node[draw=black, ellipse, minimum width=3cm, minimum height=2.2cm, fill=blue!10, align=center, font=\small] 
            (elipse1) at ($(center) + (-\sep,0)$) 
            {Definición de RAG};

        \node[draw=black, ellipse, minimum width=3cm, minimum height=2.2cm, fill=blue!10, align=center, font=\small] 
            (elipse2) at ($(center)$) 
            {Arquitectura de RAG};

        \node[draw=black, ellipse, minimum width=3cm, minimum height=2.2cm, fill=blue!10, align=center, font=\small] 
            (elipse3) at ($(center) + (\sep,0)$) 
            {Optimización de RAG};
        \node[align=center, text width=30cm, yshift=-1.5cm] at (center) {\fontsize{10}{12}\selectfont
    Puntos de entrada a la investigación};

        % Conexiones entre elipses y nodos
        \draw[-to] (elipse1) -- (start);
        \draw[-to] (elipse1) -- (obj);
        \draw[-to] (elipse2) -- (design);
        \draw[-to] (elipse3) -- (eval);
        \draw[-to] (elipse3) -- (dem);
        \draw[-to, bend right=20] (comunicacion) to node[pos=0.06, above,sloped]{Iterativo} (objetivos);
        \draw[-to, bend right=20] (comunicacion) to (diseno);
        \draw[-to, bend right=20] (comunicacion) to (evaluacion);
    \end{tikzpicture}
    }
    \caption{Proceso de Diseño de Investigación para el desarrollo de RAG en Centinela}
    \label{fig: DSR}
\end{figure}
\end{landscape}

\section{Diseño y desarrollo del artefacto}
De acuerdo con a la literatura, el desarrollo de RAG para Centinela se realiza usando las fases expuestas en la Figura~\ref{fig:secciones-rag}. La implementación,
en \texttt{Jupyter Notebook}, está disponible en el repositorio de GitHub \href{https://github.com/kalech7/RAG-Centinela}{kalech7/RAG-Centinela}.

\subsection{Ingesta} 
De acuerdo al estudio exhaustiva de la literatura, el primer paso para la construccion de RAG es la obtencion de los datos. En nuestro caso
los datos provienen de Scopus donde se extrae los metadatos de los articulos, en particular título y abstract, los cuales concentran la representación semántica principal del artículo y son los puntos 
de entrada más frecuentes para consultas y embeddings\parencite{zhai2024llmIR}. De acuerdo con la literatura~\parencite{zhai2024llmIR,knollmeyer2024benchmarking,fan2021pretraining,ibrihich2022review,bernard2025fate,ma2025vector}, se han saleccionado los siguientes campos que 
contienen la información necesaria para representar el contenido del documento e identificarlo.

\begin{itemize}
    \item \textbf{Título y abstract}: condensan la información temática central del articulo y son la base principal para la recuperación y la generación de embeddings.
    \item \textbf{DOI}: garantiza trazabilidad y constata la fuente, enfrenta a los problemas de confianza y alucinación en RAG.
    \item \textbf{Autores}: son un metadato clave ya que permite analizar impacto académico y coautorías. 
    \item \textbf{Afiliaciones}: aportan contexto institucional y geográfico, lo cual resulta útil para filtrar información por país o institución.
    \item \textbf{Número de citaciones}: es un metadato adicional para priorizar documentos de mayor impacto en la fase de ranking.
    \item \textbf{ScopusID}: asegura que cada documento tenga un identificador único, evitando duplicados en la VDB. 
\end{itemize}

De esta forma los campos título y abstract concentran la mayor parte de semántica relevante para crear embeddings y recuperar documentos, por lo que conviene tratarlos como el bloque principal de indexación.
En cambio, DOI, ScopusID, Autores y Afiliaciones funcionan como metadatos de soporte: ayudan a auditar la procedencia, filtrar por institución o región, destingir entre entidades y evitar duplicados en la base 
vectorial. Separar título/resumen de metadatos de control (DOI, ScopusID, autores, afiliaciones)  mejora la interpretabilidad del sistema y evita sobrecargar el espacio vectorial 
con señales que no aportan significado textual directo.

Con la siguiente función se extrae el corpus y se guardan en un CSV (UTF-8) para su procesamiento en fases posteriores.
\begin{algorithmEN}
\caption{Export\_Articles\_to\_CSV}
\begin{algorithmic}[1]
\Require NEO4J\_URI,\ NEO4J\_AUTH,\ OUT\_CSV=\texttt{scopusdata.csv}
\Ensure  CSV (UTF-8, \texttt{sep="|"}, no index) with flattened metadata
\State \textbf{Query Neo4j (filtered + ordered).}
\State $query \Assign$ \textsc{CypherForArticles} \Comment{filter by non-empty \texttt{scopus\_id}, \texttt{title}, ...}
\State $rows \Assign \textsc{RunCypher}(NEO4J\_URI,\ NEO4J\_AUTH,\ query)$
\State $df \Assign \textsc{DataFrame}(rows)$ \Comment{tabularize query results}
\Statex \textbf{Join list-valued columns}
\Function{JoinList}{x}
  \State \Return $\textsc{IsList}(x) \ ?\ \textsc{Join}(x,\ \texttt{", "})\ :\ x$
\EndFunction
\ForAll{$c \in \textsc{Cols}(df)$} \Comment{for all c in columns}
    \ForAll{$r \in \textsc{Rows}(df)$}
      \If{\textsc{IsList}(df[c][r])}
        \State $df[c][r] \Assign \textsc{Join}(df[c][r],\ \texttt{", "})$
      \EndIf
    \EndFor
\EndFor
\Statex \textbf{Set final column order.}
\State $cols \Assign [\texttt{title},\ \texttt{abstract},\ \texttt{doi},\ \texttt{authors},...]$
\State $df \Assign \textsc{ReindexColumns}(df,\ cols)$
\Statex \textbf{Export CSV.}
\State \textsc{WriteCSV}(df,\ OUT\_CSV,\ sep=\texttt{"|"},\ encoding=\texttt{utf-8},\ index=\texttt{False})
\end{algorithmic}
\label{alg:export-articles-csv}
\end{algorithmEN}
\subsection{Preprocesamiento}
Una vez completada la ingesta de datos provienientes de Scopus, es fundamental preprocesarlos para normalizar y limpiar el texto, reducir ruido y mejorar la calidad semántica de las representaciones.
En esta etapa aplicare el siguiente flujo reproducible compuesto por estos pasos:
\begin{enumerate}
    \item \textbf{Normalización}. Consiste en estandarizar la forma del texto mediante 
    UTF-8, la corrección de caracteres extraños, la conversión a minúsculas, la eliminación de puntuación no informativa, 
    símbolos y números aislados, así como la normalización de espacios y se etiqueta el idioma de cada documento; no se fuerza la traducción, preservando la variación lingüística para análisis por idioma.
    para garantizar la coherencia. Cuando corresponde, se desduplican registros o resúmenes 
    (comparación por DOI o por título+autor+año) con el fin de homogeneizar el vocabulario y reducir sesgos por repeticiones.
Con el siguiente pseudocodigo se procedera a normalizar las columnas que contienen toda la semantica:
\begin{algorithmEN}
\caption{Process and Label\_Language}
\begin{algorithmic}[1]
\Require IN\_CSV=\texttt{scopusdata.csv}
\Require PARQ\_1=\texttt{processed.parquet}
\Require SRC=\texttt{abstract\_norm}
\Ensure   \texttt{processed\_lbl.parquet} (con \texttt{lang})
\Statex \textbf{Load CSV}
\State $df \Assign \textsc{ReadCSV}(IN\_CSV,\ sep="|")$
\Statex \textbf{Normalize text fields}
\Function{Normalize}{s}
  \State $s \Assign \textsc{FixText}(s)$ 
  \State $s \Assign \textsc{Lower}(s)$;\ $s \Assign \textsc{UnicodeNFC}(s)$ \Comment{lowercase and canonical Unicode}
  \State $s \Assign \textsc{CleanSymbols}(s)$;\ $s \Assign \textsc{CollapseWhitespace}(s)$ \Comment{punctuation; spaces}
  \State \Return $s$
\EndFunction
\State $df[\texttt{title\_norm}] \Assign \textsc{Map}(\textsc{Normalize},\ df[\texttt{title}])$
\State $df[\texttt{abstract\_norm}] \Assign \textsc{Map}(\textsc{Normalize},\ df[\texttt{abstract}])$

\Statex \textbf{Label language from abstract\_norm}
\State \textsc{SetLangDetectSeed}(0)

\Function{DetectLangSafe}{t}
  \If{$\neg \textsc{IsString}(t)\ \lor\ \textsc{IsBlank}(t)$} \State \Return \texttt{und} \EndIf
  \State \textbf{try} \State \Return $\textsc{DetectLang}(t)$
  \State \textbf{catch} \State \Return \texttt{und}
\EndFunction
\State $df[\texttt{lang}] \Assign \textsc{Map}(\textsc{DetectLangSafe},\ df[SRC])$

\Statex \textbf{Save labeled table}
\State \textsc{SaveParquet}(df,\ OUT,\ compression=\texttt{gzip})
\end{algorithmic}
\label{alg:process-and-label}
\end{algorithmEN}

Se lee el CSV (separador “|”) y se generan title-norm y abstract-norm corrigiendo codificación, pasando a 
minúsculas y limpiando caracteres no informativos; luego se guarda todo en formato Parquet (columna-orientado y 
comprimido) que conserva el esquema, reduce tamaño y acelera lecturas/filtrado frente a CSV. A continuación se 
carga el Parquet procesado, se detecta el idioma de abstract-norm con langdetect (columna lang, usando “und” si
es indeterminado) y se guarda un nuevo Parquet. La detección de idioma permite tratar corpus multilingües de forma óptima (stopwords/tokenización por lengua y elección del modelo de embeddings multilingüe).

 \item \textbf{Segmentación en oraciones}. Dividir el texto en oraciones completas utilizando delimitadores válidos (puntos, signos de cierre), con el fin de 
    estructurar unidades coherentes para el análisis posterior. 
\begin{algorithmEN}
\caption{Segment Text }
\begin{algorithmic}[1]
\Require IN(parquet),OUT(parquet-out),COL=\texttt{abstract\_norm},LANG\_COL=\texttt{lang}
\Ensure  Table with a sentence segmentation derived from \texttt{COL} using \texttt{LANG\_COL}

\State $df \Assign \textsc{ReadParquet}(IN)$ \Comment{load table}
\State $segs \Assign \{\,\texttt{en}\mapsto \textsc{PySBD}(en),\ \texttt{es}\mapsto \textsc{PySBD}(es)\,\}$ \Comment{load language segmenters}

\Function{SegmentByLang}{text, lg}
  \State $k \Assign \textsc{LowerPrefix}(lg)$ \Comment{e.g., "en", "es"}
  \State $seg \Assign \textsc{Get}(segs,\ k)$
  \State \Return $seg.\textsc{Segment}(text)$
\EndFunction

\ForAll{$r \in \textsc{Rows}(df)$} \Comment{iterate over rows}
  \State $text \Assign df[COL][r]$
  \State $lg \Assign df[LANG\_COL][r]$
  \State $df[\texttt{sentences}][r] \Assign \textsc{SegmentByLang}(text,\ lg)$ \Comment{segment in its language}
\EndFor

\Statex \textbf{One row per sentence:}
\State $out \Assign \textsc{ExplodeRename}(df,\ \texttt{sentences}\rightarrow\texttt{sentence})$
\State $\textsc{SaveParquet}(out,\ OUT)$
\end{algorithmic}
\label{alg:segment-text-lang-aware}
\end{algorithmEN}

Se lee el Parquet de entrada, elige la columna abstract-norm y segmenta sus textos en oraciones usando pysbd según el idioma que se detecto previamente.
Devuelve una lista de oraciones por fila; luego crea una fila por oración, conserva metadatos útiles 
añadiendo el índice de oración sentence-idx dentro de cada documento y escribe el resultado en processed-sentences.parquet.

    \item \textbf{Tokenización}. Segmentar cada oración en tokens (palabras o subpalabras) preservando frases clave frecuentes (p.\,ej.,\textit{aprendizaje automático}), a fin de mantener unidades semánticas estables.

  \begin{algorithmEN}[H]
  \caption{Tokenize} 
  \begin{algorithmic}[1] 
    \Require IN = Parquet path, COL = \texttt{sentence} 
    \Ensure Table with column \texttt{tokens\_gram} 
    \State $df \Assign \textsc{ReadParquet}(\texttt{IN})$ \Comment{load} 
    \State $df[\texttt{tokens\_base}] \Assign \textsc{SimpleTokenizeColumn}(df,\ \texttt{COL})$ \Comment{create} 
    \State \textbf{Load Gensim phrase models.} \State $S \Assign \textsc{ToList}(df[\texttt{tokens\_base}])$ \Comment{prepare list of token sequences} \State $bigP \Assign \textsc{Phrases}(S,\ \textit{min\_count}=5,\ \textit{threshold}=10.0)$ \Comment{train bigram model} \State $bigF \Assign \textsc{Phraser}(bigP)$ \Comment{wrap bigram} 
    \State $S\_bi \Assign \textsc{TransformAll}(bigF,\ S)$ \Comment{apply bigram} 
    \State $triP \Assign \textsc{Phrases}(S\_bi,\ \textit{min\_count}=5,\ \textit{threshold}=10.0)$ \Comment{train trigram model} 
    \State $triF \Assign \textsc{Phraser}(triP)$ \Comment{wrap trigram} 
    \State $df[\texttt{tokens}] \Assign \textsc{TransformAllChain}([bigF,\,triF],\ df[\texttt{tokens\_base}])$ \Comment{apply} 
    \State $df[\texttt{tokens\_gram}] \Assign \textsc{JoinTokensByComma}(df[\texttt{tokens}])$ \Comment{create} 
    \State \textsc{SaveParquet}(df,\ \texttt{OUT},\ \textit{engine}=\texttt{fastparquet},\ \textit{comp}=\texttt{gzip}) \Comment{save} 
  \end{algorithmic}
  \label{alg:tokenize} 
  \end{algorithmEN}

Carga el parquet y tokeniza con una expresión regular y entrena detectores de bigramas y trigramas con Gensim usando mincount=5 y threshold=10. 
Luego aplica ambos phrasers para unir collocations frecuentes en un solo token el cual a su vez genera tokens. Los tokens son unidos por comas y se guardan en el nuevo parquet.

    \item \textbf{Eliminación de stop words}. Remover términos de alta frecuencia y escaso valor semántico.

\begin{algorithmEN}[H]
  \caption{Remove Stopwords by Language}
  \begin{algorithmic}[1]
    \Require IN = Parquet path,\; COL = \texttt{tokens},\; LANGCOL = \texttt{lang} (\texttt{"es"} or \texttt{"en"})
    \Ensure Table with \texttt{tokens\_nostop} and \texttt{text\_for\_embed}
    \State $df \Assign \textsc{ReadParquet}(\texttt{IN})$ \Comment{load input}
    \State $LANGMAP \Assign \{\texttt{"es"}\!\to\!\texttt{"spanish"},\ \texttt{"en"}\!\to\!\texttt{"english"}\}$
    \Statex \textbf{Stopword sets}
    \State $S_{\mathrm{es}} \Assign \textsc{NLTK.Stopwords}(\texttt{spanish})$
    \State $S_{\mathrm{en}} \Assign \textsc{NLTK.Stopwords}(\texttt{english})$
    \State $\textsc{norm}(s) \Assign \textsc{Lower}(\textsc{StripDiacritics}(s))$
    \State \textbf{Define} $\textsc{IsStop}(t,\ c)$:
      \Statex \quad $S \Assign \textsc{LangStop}(c)$;\; $u \Assign \textsc{norm}(t)$
      \Statex \quad \textbf{return} $(u \in S)$
    \State \textbf{Define} $\textsc{FilterTokens}(xs,\ c,\ \textit{min\_len}=2)$:
    \Statex \quad \textbf{return} $[\, t\in xs \mid \textsc{Len}(t)\ge \textit{min\_len} \land \neg\textsc{IsNum}(t) \land \neg\textsc{IsStop}(t,c)\,]$ 
    \State $df[\texttt{tokens\_nostop}] \Assign \textsc{Map}(\textsc{FilterTokens},\ df[\texttt{COL}],\ df[\texttt{LANGCOL}])$
    \State $df[\texttt{text\_for\_embed}] \Assign \textsc{JoinWithSpace}(df[\texttt{tokens\_nostop}])$
    \Statex \textsc{SaveParquet}(df,\ \textit{comp}=\texttt{gzip})
  \end{algorithmic}
  \label{alg:remove_stopwords_lang}
\end{algorithmEN}

Una vez tokenizado eliminamos las stop words asegura las listas, normaliza a minúsculas y quita tildes.
La función is-stop considera tokens compuestos (espacios/guiones) y solo los descarta si todas sus partes 
son stopwords; además, filter-tokens aplica filtros básicos (longitud mínima, quitar numéricos)

\item \textbf{Aplicación de Lematización o stemming}. Preferir lematización sobre \textit{stemming} para conservar la forma canónica y reducir 
    la varianza léxica sin pérdida de significado \parencite{tabassum2020}.
\begin{algorithmEN}[H]
\caption{Lemmatize\_spaCy}
\begin{algorithmic}[1]
\Require IN=\texttt{corpus\_stop.parquet}, TEXT\_COL=\texttt{text\_for\_embed}
\Ensure  Parquet with \texttt{text\_lemma}
\Statex \textbf{Load table}
\State $df \Assign \textsc{ReadParquet}(IN)$;
\Statex \textbf{Load spaCy pipelines}
\State $nlp\_es \Assign \textsc{LoadSpacy}(\texttt{es\_core\_news\_sm})$
\State $nlp\_en \Assign \textsc{LoadSpacy}(\texttt{en\_core\_web\_sm})$
\Statex \textbf{Build language masks from \texttt{lang} column}
\State $idx\_es \Assign [\,\textsc{LowerPrefix}(lg)=\texttt{es}\ \textbf{for}\ lg \in df[\texttt{LANG\_COL}]\,]$
\State $idx\_en \Assign [\,\textsc{LowerPrefix}(lg)=\texttt{en}\ \textbf{for}\ lg \in df[\texttt{LANG\_COL}]\,]$
\State \textbf{Function to lemmatize a string}: return space-separated lowercase lemmas
\Statex \textbf{Process ES batch}
\State $stream\_es \Assign nlp\_es.\textsc{Pipe}(\dots)$ \Comment{batched iterator of Docs}
\State $lem\_es \Assign [\ \textsc{LemmaJoin}(d)\ \textbf{for}\ d \in stream\_es\ ]$

\Statex \textbf{Process EN batch}
\State $docs\_en \Assign nlp\_en.\textsc{Pipe}(df[\texttt{TEXT\_COL}][idx\_en],\ batch\_size{=}512)$
\State $lem\_en \Assign [\ \textsc{LemmaJoin}(d)\ \textbf{for}\ d \in docs\_en\ ]$

\Statex \textbf{Assemble output column}
\State $text\_lemma[idx\_es] \Assign lem\_es$;\quad $text\_lemma[idx\_en] \Assign lem\_en$
\State $df[\texttt{text\_lemma}] \Assign \textsc{FillNA}(text\_lemma,\ \texttt{""})$ \Comment{replace nulls with empty string}

\Statex \textbf{Save}
\State \textsc{SaveParquet}(df,\ OUT,\ compression=\texttt{gzip}) 
\end{algorithmic}
\label{alg:lemmatize-spacy}
\end{algorithmEN}

Iniciamos leyendo el parquet de entrada de la fase anterior y y lematiza en lote con spaCy usando dos modelos (ES/EN) dependiendo de la etiqueta que habiamos usado agregado anteriormente.
Genera  tokens lematizados en minúsculas, separados por espacios y guarda el resultado en el nuevo parquet.

\item \textbf{\textit{Chunking} para LLM/RAG}. Dividir documentos largos en fragmentos (\(\sim 200\text{--}400\) tokens) con solape (15–30\,\%) para respetar 
    límites de contexto de los transformadores y mejorar la recuperación a nivel de pasaje (Hambarde \& Proença, 2023).
\begin{algorithmEN}[H]
\caption{Chunking \texttt{text\_lemma} }
\begin{algorithmic}[1]
\Require Parquet $IN\_PARQUET$ with \texttt{scopus\_id}, \texttt{text\_lemma}
\State Load \textit{tokenizer} E5 (\textit{multilingual-e5-base}) 
\State $df \gets \textsc{ReadParquet}(IN\_PARQUET)$

\Statex \textbf{Group and clean text}
\State $agg\_text \gets \textsc{GroupByJoin}(df,\ key=\texttt{scopus\_id},\ col=\texttt{text\_lemma},\ sep=" ")$
\State $doc\_df \gets \textsc{ResetIndex}(agg\_text)$;\quad asignar \texttt{doc\_id} consecutivo
\State $doc\_df.\texttt{text\_for\_chunk} \gets \textsc{NormalizeSpaces}(doc\_df.\texttt{text\_lemma})$

\Statex \textbf{Chunking por tokens}
\Function{ChunkByTokens}{$text,\ MAX\_TOKENS,\ STRIDE$}
  \State $ids \gets \textsc{TokEncode}(text)$;\quad $n \gets |ids|$;\quad $chunks \gets [\ ]$ \Comment{map text $\to$ token IDs}
  \For{$start \gets 0$ \textbf{step} $STRIDE$ \textbf{to} $n$}
    \State $end \gets \min(start{+}MAX\_TOKENS,\ n)$
\State $chunks.\textsc{Append}\big(\langle start,\ end,\ \textsc{TokDecode}(ids[start{:}end])\rangle\big)$ \Comment{decode token slice $\to$ text}

    \If{$end = n$} \State \textbf{break} \EndIf
  \EndFor
  \State \Return $chunks$
\EndFunction

\Statex \textbf{Build chunk table}
\State $rows \gets [\ ]$
\ForAll{document $d$ in $doc\_df$}
  \State $chunks \gets \textsc{ChunkByTokens}(d.\texttt{text\_for\_chunk},\ MAX\_TOKENS,\ STRIDE)$
  \For{each chunk $c$ with index $j$ in $chunks$}
    \State \textsc{AppendColumns}($rows$, \{\texttt{doc\_id}, \texttt{chunk\_id}, \ldots, \texttt{text\_chunk}\})
  \EndFor
\EndFor
\State $CHUNKS \gets \textsc{ToDataFrame}(rows)$
\Statex \textbf{Save}
\State $save\_cols \gets [\texttt{doc\_id},\texttt{chunk\_id},\texttt{chunk\_uid},\texttt{scopus\_id},...]$
\State $out \gets chunks[save\_cols]$
\State \textsc{ToParquet}$(out,\ \texttt{pyarrow})$ 
\end{algorithmic}
\end{algorithmEN}

Finalmente, para fragmentar el texto tomamos el Parquet lematizado, consolidamos un solo documento por artículo 
y, con el tokenizer E5 (bi-encoder basado en transformer), lo separamos por conteo de tokens (tamaño = 300) con un solapamiento de 0.2.
\end{enumerate}
Con el uso de este pipeline se disminuye el ruido de contexto suministrado al LLM en RAG, 
lo que reduce inconsistencias y riesgo de alucinaciones en la generación. En términos de evaluación, una mejor normalización y segmentación
 favorecen la relevancia contextual del material recuperado y la fidelidad de las respuestas, al entregar fragementos más coherentes, comparables y medibles permitiendo
 calcular su similitud y una mejor relevancia contextual del material recuperado.


\subsection{Vectorización}\label{subsec:vectorizacion}
Para el diseño de este sistema se usa un retriever denso de arquitectura bi-encoder tipo BERT para la generación de embeddings, ya que permite codificar por separado la 
consulta y los fragmentos en un espacio vectorial de baja dimensión (vector corto y continuo de tamaño fijo) y comparar por similitud, 
capturando relaciones semánticas más allá de coincidencias léxicas y mejorando la recuperación en escenarios RAG y open-domain QA.
Además, estos embeddings se indexan en bases de datos vectoriales con búsqueda aproximada de vecinos cercanos (ANN), lo que viabiliza el escalamiento eficiente del sistema\parencite{joshi2025vector}.
\begin{algorithmEN}[H]
  \caption{Vectorizacion }
  \begin{algorithmic}[1]
    \Require \texttt{chunks\_df[text\_chunk]}, \texttt{EMB\_MODEL}, \texttt{FAISS\_PATH}, \texttt{PKL\_PATH}, \texttt{INIT\_BATCH}, \texttt{EMB\_MAX\_SEQ\_LEN}
    \State $model \gets \textsc{LoadModel}(\texttt{EMB\_MODEL}, cpu)$ \Comment{Load E5 on CPU}
    \State $\textsc{SetMaxSeqLen}(model,\ \min(\texttt{EMB\_MAX\_SEQ\_LEN},512))$ \Comment{Bound to 512}
    \State $P \gets \textsc{Prefix}(\texttt{chunks\_df.text\_chunk},\ "passage:\ ")$ \Comment{E5 passage prefix}
    \State $index \gets \varnothing$ \Comment{Will hold IndexFlatIP}
    \For{$batch \in \textsc{Batches}(P,\ \texttt{INIT\_BATCH})$} \Comment{Encode in mini-batches}
      \State $E \gets \textsc{Encode}(model,\ batch,\ \textit{normalize}=True)$ 
      \If{$index=\varnothing$} \State $index \gets \textsc{NewIndexFlatIP}(d)$ \EndIf
      \State $\textsc{Add}(index,\ E)$ \Comment{Append vectors to FAISS}
    \EndFor
    \State $\textsc{WriteFAISS}(index,\ \texttt{FAISS\_PATH})$ \Comment{Persist FAISS}
    \State $\textsc{EnsureVecID}(\texttt{chunks\_df},\ 0..|P|-1)$ \Comment{0-based contiguous ids}
  \State $cols \gets [\texttt{vec\_id},\ \texttt{chunk\_uid},\ \texttt{doc\_id},\ \texttt{chunk\_id},...,\ \texttt{scopus\_id}]$
  \State $meta\_min \gets \textsc{Select}(\texttt{chunks\_df},\ cols)$
    \State $\textsc{WritePKL}(\{\texttt{model},\ \texttt{device},\ \texttt{dim},\ \texttt{meta\_min}\},\ \texttt{PKL\_PATH})$

  \end{algorithmic}
\end{algorithmEN}


Toma los textos del archivo parquet, los limpia y les asigna identificadores (vec-id, chunk-uid), luego usa un modelo SentenceTransformer (E5) en CPU para convertir cada fragmento en un vector numérico normalizado (embedding).
Procesa los textos por lotes (reduce el tamaño si hay errores de memoria), junta todos los embeddings en una matriz (N, dim) y finalmente guarda en dos archivos .npy (donde estan los vectores) y .pkl con la información mínima (modelo, dimensiones e índices que vinculan cada vector con su texto original).

\subsection{Vector DB}\label{subsec:vector-db}
\begin{algorithmEN}[H]
  \caption{Build Vector Index from Embedding Matrix}
  \begin{algorithmic}[1]
    \Require Paths: \texttt{IN\_EMB}, \texttt{IN\_META}, \texttt{OUT\_INDEX}, \texttt{OUT\_INFO}
    \Ensure Serialized vector index and metadata file

    \State $X \gets \textsc{LoadArray}(\texttt{IN\_EMB})$ \Comment{embedding matrix}
    \State $meta \gets \textsc{LoadObject}(\texttt{IN\_META})$ \Comment{load metadata info}
    \State $index \gets \textsc{InitIndex}(\textit{dim}=meta.dim,\ \textit{metric}=\texttt{inner\_product})$
    \State $\textsc{AddVectors}(index,\ X)$
    \State $\textsc{SaveIndex}(index,\ \texttt{OUT\_INDEX})$
    \State $info \gets \textsc{BuildRecord}(model,\ dim,\ count,\ paths)$
    \State $\textsc{SaveObject}(\texttt{OUT\_INFO},\ info)$
  \end{algorithmic}
  \label{alg:build_vector_index}
\end{algorithmEN}

Carga los embeddings normalizados verifica que las dimensiones coincidan, construye un índice FAISS y  el resultado se guarda en disco (.bin)

\subsection{Recuperación}\label{subsec:recuperacion}
\begin{algorithmEN}[H]
  \caption{Query Search over Vector Index}
  \begin{algorithmic}[1]
    \Require Paths: \texttt{PKL\_MIN\_PATH}, \texttt{FAISS\_PATH}, \texttt{SCOPUS\_CSV}
    \Ensure Table of top-$k$ retrieved documents with metadata

    \Statex \textbf{load artifacts:}
    \State \quad $meta \gets \textsc{LoadPKL}(\texttt{PKL\_MIN\_PATH})[\texttt{"meta\_min"}]$
    \State \quad $index \gets \textsc{ReadFAISS}(\texttt{FAISS\_PATH})$
    \State \quad $model \gets \textsc{LoadModel}(\textsc{LoadPKL}(\texttt{PKL\_MIN\_PATH})[\texttt{"model"}],\ \textit{device}=\texttt{cpu})$

    \Statex \textbf{encode query and search:}
    \State \quad $q\_emb \gets \textsc{Encode}(model,\ ["query: "+Q],\ \textit{normalize}=\texttt{True})$
    \State \quad $(scores,\ ids) \gets \textsc{Search}(index,\ q\_emb,\ k)$ \Comment{retrieve top-$k$ similar vectors}

    \Statex \textbf{assemble top-$k$ results:}
    \State \quad $hits \gets \textsc{SelectRows}(meta,\ ids[0])$
    \State \quad $\textsc{AddColumn}(hits,\ \texttt{"score"},\ scores[0])$

    \Statex \textbf{merge with external CSV:}
    \State \quad $sc \gets \textsc{ReadCSV}(\texttt{SCOPUS\_CSV},\ \textit{sep}=\texttt{"|"})$ \Comment{load full Scopus metadata}
    \State \quad $\textsc{EnsureType}(sc[\texttt{"scopus\_id"}],\ \texttt{str})$
    \State \quad $hits \gets \textsc{MergeLeft}(hits,\ sc,\ \textit{on}=\texttt{"scopus\_id"})$
  \end{algorithmic}
  \label{alg:search_vector_index}
\end{algorithmEN}

Primero cargamos el vector DB, luego vectorizamos  la consulta (query) y normaliza el embedding con ese vector hace búsqueda k-NN en FAISS y obtiene scores e ids.
Para obtener un contexto mejorado se une el resultado con el CSV de Scopus por scopus-id para agregar la demas metadata que contiene el csv.
\subsection{Re-ranking}\label{subsec:re-ranking}
Se implementa un cross encoder entrenado de segunda capa que reordena los candidatos recuperados en la primera etapa densa. El puntaje final se obtiene por fusión lineal (min-max + suma ponderada)
entre el score denso de la recuperación inicial y el score del cross-encoder. 
Esta estrategia equilibra eficacia (precisión en el top-k) y eficiencia (coste computacional), y se ejecuta en CPU con MiniLM por ser un modelo compacto (menor latencia y memoria).
\begin{algorithmEN}[H]
  \caption{CE Re-Ranking (concise, with brief comments)}
  \begin{algorithmic}[1]
    \Require query $q$, table $D$, text cols $\texttt{text\_cols}$, model $\texttt{CE\_MODEL}$, device, batch $B$, weights $(w_{ce},w_d)$
    \Ensure $D'$ sorted by $\texttt{score\_final}$

    \Function{SelectText}{$r,\ C$}                  \Comment{pick first non-empty text field}
      \For{$c\in C$}
        \If{$\textsc{NonEmpty}(r[c])$} \Return $r[c]$ \EndIf
      \EndFor
      \State \textsc{Error}                         \Comment{no valid text}
    \EndFunction

    \Function{MinMax}{$x$}                          \Comment{normalize to [0,1] per query}
      \State $m\gets\min(x),\ M\gets\max(x)$
      \If{$M\le m$} \State \textsc{Error}           \Comment{constant vector}
      \EndIf
      \State \Return $(x-m)/(M-m)$
    \EndFunction

    \Function{Fuse}{$z_{ce},z_d,w_{ce},w_d$}        \Comment{simple convex fusion}
      \State \Return $w_{ce}z_{ce}+w_d z_d$
    \EndFunction

    \State \textsc{Assert}($|D|>0 \land |\texttt{text\_cols}|>0$) \Comment{basic checks}
    \State $CE\gets\textsc{CrossEncoder}(\texttt{CE\_MODEL},\ \texttt{device})$ \Comment{load CE (once)}
    \State $P,I\gets[\ ],[\ ]$                       \Comment{(q,doc) pairs and index map}
    \For{row $r$ at idx $i$ in $D$}
      \State append $(q,\ \Call{SelectText}{r,\texttt{text\_cols}})$ to $P$ \Comment{build CE input}
      \State append $i$ to $I$                       \Comment{keep original index}
    \EndFor

    \State $S_{ce}\gets \textsc{BatchedPredict}(CE,P,B)$ \Comment{batched CE inference}
    \State $D'\gets D$                               \Comment{work on a copy}
    \For{$j$} $D'[\texttt{score\_ce}]_{I[j]}\gets S_{ce}[j]$ \EndFor \Comment{write CE scores}

    \State $z_{ce}\gets \Call{MinMax}{D'[\texttt{score\_ce}]}$ \Comment{normalize CE}
    \If{$\texttt{score\_dense}\in \textsc{Cols}(D')$}
      \State $z_d\gets \Call{MinMax}{D'[\texttt{score\_dense}]}$ \Comment{normalize dense}
      \State $D'[\texttt{score\_final}]\gets \Call{Fuse}{z_{ce},z_d,w_{ce},w_d}$ \Comment{fuse signals}
    \Else
      \State $D'[\texttt{score\_final}]\gets z_{ce}$ \Comment{no dense: CE only}
    \EndIf

    \State \Return $\textsc{SortBy}(D',\ \texttt{score\_final},\ \textit{desc}=True)$ \Comment{final ranking}
  \end{algorithmic}
\end{algorithmEN}

Primero toma una lista de candidatos recuperadaos para una consulta y los reordena usando un cross encoder.
Construye pares (consulta, texto) a partir de las columnas de texto prioritarias, infiere puntajes del CE por lotes, normaliza esos puntajes (y, si existe, 
el puntaje denso del recuperador) con min-max y luego los fusiona mediante una suma ponderada convexa.


\subsection{Construcción de Contexto}\label{subsec:construccion-de-contexto}
La evidencia recolectada en el paso anterior se convierte en un bloque de evidencia utilizable por el LLM, equilibrando cobertura semántica y límite de longitud. 
Proporcionar cada texto en chunks manejables (longitud fija), preservando su trazabilidad (título, autores, año, DOI); y ensamblar un “paquete” de contexto con instrucciones de uso para el modelo
\begin{algorithmEN}[H]
\caption{BuildContextBlocks}
\begin{algorithmic}[1]
\Require reranked table $D$; top-$k$; character limit $L$; flag $trim$
\Ensure list $blocks$ with fields \texttt{cite\_id}, \texttt{title}, \texttt{text}, \texttt{authors\_mention}, \texttt{authors\_raw}, \texttt{year}, \texttt{doi\_raw}

\State $T \gets$ first column present in \texttt{[title, chunk\_title]} \Comment pick title source
\State $B \gets$ first column present in \texttt{[abstract, chunk\_text, summary]} \Comment pick body source
\State $n \gets \min(k, |D|)$;\quad $blocks \gets [\ ]$ \Comment cap $k$ and init output

\For{$i \gets 0..n-1$} \Comment iterate top-$n$ rows
  \State $r \gets D[i]$ \Comment current row
  \State $title \gets \textsc{FirstNonEmpty}(r,[T])$ \textbf{or} ``\!Untitled!'' \Comment get title
  \State $body \gets \textsc{FirstNonEmpty}(r,[B])$ \Comment get body text
  \If{$trim$} \Comment optional clipping
    \State $body \gets \textsc{Shorten}(body, L)$ \Comment enforce char limit
  \EndIf
  \State $auth\_raw \gets r[\texttt{authors}]$ \textbf{or} ``'' \Comment raw authors
  \State $mention \gets \textsc{FormatAuthorsForMention}(auth\_raw)$ \Comment short citation string
  \State $year \gets \textsc{ExtractYear}(r)$ \Comment publication year
  \State $doi \gets r[\texttt{doi}]$ \textbf{or} ``'' \Comment DOI if present
  \State $cid \gets \textsc{RowId}(r)$ \Comment prefer scopus\_id"

  \State \textbf{append} \{
    \texttt{cite\_id}: $cid$,\,
    \texttt{title}: $title$,\,
    \texttt{text}: $body$,\,
    \texttt{authors\_mention}: $mention$,\,
    \texttt{authors\_raw}: $auth\_raw$,\,
    \texttt{year}: $year$,\,
    \texttt{doi\_raw}: $doi$
  \} to $blocks$ 
\EndFor

\State \Return $blocks$ \Comment final list
\end{algorithmic}
\end{algorithmEN}

A partir de la tabla reranqueada, se eligen las columnas para título (title/chunk title) y cuerpo (abstract/chunktext/summary), se limita el número de filas a top-k y se crea la lista blocks.
Para las n primeras filas se obtienen el título y el texto; si trim está activo, el cuerpo se recorta hasta L caracteres.
 Se capturan los autores completos y se genera una mención abreviada, se extraen el año, el DOI y el identificador (scopusid).
 Con estos campos se arma cada bloque y devuelve blocks, listo para alimentar el prompt.

\subsection{Generación (LLM)}\label{subsec:generacion-(llm)}
En esta penultima etapa el LLM produce una respuesta dependiendo de la evidencia externa extraída previamente.
Recibe un prompt estructurado que incluye la instrucción de estilo y formato
la pregunta del usuario y los fragmentos seleccionados tras la recuperación y reraking el llm cita explícitamente las fuentes dentro de cada párrafo para asegurar trazabilidad y verificabilidad
\begin{algorithmEN}[H]
\caption{LLMGenerateViaHTTP}
\begin{algorithmic}[1]
\Require query $q$; list $blocks$; \texttt{MODEL}; \texttt{BASE\_URL}; \texttt{TEMPERATURE}; \texttt{MAX\_NEW\_TOKENS}; \texttt{TIMEOUT}; \texttt{MAX\_INPUT\_CHARS}
\Ensure text $\textit{answer\_text}$
\State $prompt \gets \textsc{ComposePrompt}(q,\ blocks,\ \texttt{MAX\_INPUT\_CHARS})$
\State $prompt \gets \textsc{SafeStr}(prompt)$ \Comment  Max input chars
\State $url \gets \texttt{BASE\_URL}\ \Vert\ \texttt{"/api/chat"}$ \Comment chat endpoint
\State $payload \gets \{$
\Statex \quad \texttt{"model"}:\ \texttt{MODEL},
\Statex \quad \texttt{"messages"}:\ \big[\{\texttt{role}:\texttt{"system"},\ \texttt{content}:\ ``You are an academic assistant. Answer only using the provided context.''\},\ \{\texttt{role}:\texttt{"user"},\ \texttt{content}:\ prompt\}\big],
\Statex \quad \texttt{"options"}:\ \{\texttt{temperature}:\ \texttt{TEMPERATURE},\ \texttt{num\_predict}:\ \texttt{MAX\_NEW\_TOKENS}\},
\Statex \quad \texttt{"stream"}:\ \texttt{false}
\Statex \ \}
\State $\textit{resp} \gets \textsc{HTTP\_POST}(url,\ \textit{json}=payload,\ \textit{timeout}=\texttt{TIMEOUT})$ \Comment call to the LLM backend
\State $data \gets \textsc{ParseJSON}(\textit{resp})$
\State $\textit{answer\_text} \gets \textsc{SafeStr}(data[\texttt{"message"}][\texttt{"content"}])$ 
\State \Return $\textit{answer\_text}$
\end{algorithmic}
\end{algorithmEN}

Para iniciar la generación, se construye el prompt a partir de la consulta y de los bloques de contexto, ajustándolos a un tope de caracteres y limpiandolos con SafeStr. 
Ese prompt se envía como payload al endpoint de Ollama\footnote{Ollama es un servidor para ejecutar modelos de lenguaje localmente (CPU o GPU).} 
(api/chat) junto con las opciones del modelo.
Luego, se parsea el JSON de respuesta, se extrae message.content, se limpia nuevamente con SafeStr para asegurar texto válido
y, finalmente, devuelve la respuesta.


\subsection{Evaluación}\label{subsec:evaluacion}
Para realizar la evaluación del sistema RAG desarrollado, se utilizarán las métricas presentadas en la Tabla~\ref{tab:metrics},
las cuales han sido identificadas y clasificadas según su propósito y etapa de desarrollo.
El plan de evaluación iniciará detallando las \textit{características del equipo} (hardware) empleado para ejecutar
los experimentos, a fin de asegurar reproducibilidad; continuará con la descripción del \textit{dataset} utilizado
y terminará con el \textit{diseño experimental}, en el que se definen conjuntos de consultas por áreas temáticas,
construidas a partir de palabras clave representativas, y se valida con expertos el resultado
para verificar la pertinencia de las consultas y la corrección de los juicios
de relevancia.

\paragraph{Características del equipo:} El equipo empleado es una ASUS TUF A15 (2022),
con procesador AMD Ryzen 7 (serie 6000), GPU NVIDIA GeForce RTX 3050 Ti con 4 GB de VRAM, 32 GB de memoria
RAM, unidad de estado sólido de 500 GB y sistema operativo Fedora 42.

\paragraph{Dataset:} El dataset se construye a partir de la base de datos Scopus y reúne 27\,754 artículos.
Dichos artículos fueron procesados para su indexación y posterior uso en un esquema RAG. A partir de este corpus se
formularon conjuntos de preguntas \textit{single-hop}, \textit{multi-hop} y no respondibles, con el objetivo de evaluar
el sistema por etapas:
\begin{itemize}[leftmargin=*, itemsep=0.25em]
    \item \textit{Retriever}: Cobertura y ranking.
    \item \textit{Augmentation}: Relevancia y suficiencia del contexto inyectado.
    \item \textit{Generation}: Corrección, fidelidad a las evidencias y calidad de la citación.
\end{itemize}

\paragraph{Diseño del experimento.}
Para realizar una evaluación controlada, se ha construido un conjunto de pruebas compuesto por 15 consultas las cuales
se han diversificado para abarcar la diversidad temática del corpus.
Este conjunto se ha dividido para cubrir los 3 tipos de inferencia clave del sistema:
\begin{itemize}[leftmargin=*,itemsep=0.2em]
    \item 5 consultas \emph{single-hop} (la respuesta está en un solo fragmento).
    \item 5 consultas \emph{multi-hop} (requiere fusionar 2--3 evidencias).
    \item 5 consultas \emph{no answer} (no existe respuesta en el corpus).
\end{itemize}
Las 15 consultas completas se presentan en el Anexo~\ref{tab:consultas}.
Para cada una se definió manualmente el ground truth, identificando los fragmentos de evidencia relevantes
(para las métricas de \textit{retrieval}) y estableciendo la respuesta ideal.
Asimismo, las respuestas generadas por el sistema se validaron con apoyo de expertos
(para las métricas de \textit{generación}).
\subparagraph{Condiciones comparadas:}
Evaluamos tres configuraciones del sistema para aislar el efecto de cada componente:
\begin{itemize}[leftmargin=*,itemsep=0.2em]
    \item \textbf{LLM-solo (sin retriever):} establece la \emph{línea base}.
    \item \textbf{RAG-básico:} retriever + \emph{concatenación} de los documentos \emph{top-\(k\)} (sin reranqueo) + generación.
    \item \textbf{RAG-completo:} retriever + (\emph{opcional}) \emph{reranking} + \emph{construcción de bloques de contexto} + generación.
\end{itemize}

\noindent\begin{minipage}{\linewidth}
             \centering
             \renewcommand{\arraystretch}{1.1}
             \begin{tabular}{@{}lccccc@{}}
                 \toprule
                 \textbf{Condición} & \textbf{Retriever} & \textbf{Reranker} & \textbf{Contexto} & \textbf{Estrategia de contexto} & \textbf{Generación} \\
                 \midrule
                 LLM-solo        & --          & --          & --          & --                       & \checkmark \\
                 RAG-básico      & \checkmark  & --          & \checkmark  & Concatenación top-\(k\)  & \checkmark \\
                 RAG-completo    & \checkmark  & \checkmark   & \checkmark  & Bloques (builder)        & \checkmark \\
                 \bottomrule
             \end{tabular}
\end{minipage}

\vspace{0.4em}
\noindent Para cada condición se ejecutan las 15 consultas, se calculan las métricas por consulta y se reportan
\emph{promedios macro} sobre el conjunto de consultas.
La \textbf{Ganancia de augmentation} se define frente a la línea base:
\(\Delta\mathcal{M}=\mathcal{M}(\text{RAG})-\mathcal{M}(\text{LLM}_{\text{solo}})\).
También reportamos la mejora de \emph{RAG-completo} sobre \emph{RAG-básico} para aislar el efecto del reranking y del
builder.

\begin{enumerate}[label=\textbf{(\roman*)},leftmargin=*,itemsep=0.35em]
    \item \textit{Retriever}: usamos \textbf{Recall@k} (con \(k=10\)) para medir cuánta evidencia relevante entra
    efectivamente en el top-\(k\) (cobertura), y \textbf{MRR} para reflejar \emph{qué tan arriba} aparece el primer
    relevante en el ranking (sensibilidad a la posición).
    Ambas se reportan como promedios macro sobre las consultas.

    \item \textit{Augmentation}: reportamos la \textbf{Ganancia de augmentation}, definida como la mejora de la métrica
    objetivo al inyectar contexto frente a usar el LLM solo,
    \(\mathcal{M}(\text{RAG})-\mathcal{M}(\text{LLM}_{\text{sin retriever}})\).
    Esto indica el valor añadido por la construcción de contexto.
    \item \textit{Generation}: para respuestas cortas empleamos \textbf{Exact Match (EM)}.
    Para preguntas abiertas, evaluamos con un juez (\emph{LLM-as-judge} usando GPT~4.1) los siguientes criterios:
    \begin{itemize}[leftmargin=*,itemsep=0.2em]
        \item \textbf{Fidelidad (Faithfulness):} cada afirmación de la respuesta está lógicamente derivada del contexto recuperado, sin contradicciones ni invenciones.
        \item \textbf{Cobertura de citas (Citation Coverage):} porcentaje de afirmaciones de la respuesta correctamente sustentadas con una cita al contexto.
    \end{itemize}
    Reportamos la \textbf{Tasa de alucinación} como \(1 - \text{Fidelidad}\).
    El prompt exacto utilizado por el juez se detalla en el Anexo~\ref{ch:prompt_juez}.


    \item \textit{Operación}: medimos \textbf{Latencia (Response Time)} por consulta con cronometraje de \emph{end-to-end}
    (retrieve + rerank + construcción de contexto + generación).
\end{enumerate}



