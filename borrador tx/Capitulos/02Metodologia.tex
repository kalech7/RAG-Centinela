\chapter{Metodología}
\section{Revisión sistemática}
\label{sec:revision-sistematica}
Umbrella Review, según los lineamientos del Instituto Joanna Briggs (JBI), es un tipo de revisión sistemática que recopila y analiza evidencia secundaria, 
es decir, revisiones sistemáticas y metaanálisis ya publicados.
Su propósito es consolidar el conocimiento disponible, identificar coincidencias y contradicciones en la literatura existente, así como señalar vacíos de evidencia.
Para ello, requiere la elaboración de un protocolo previo que establezca criterios de inclusión y exclusión, estrategias de búsqueda y métodos de síntesis, garantizando 
un proceso transparente y riguroso.

Por otra parte, la estrategia de propagación de citaciones (Back-and-Forward Citation Propagating) complementa este enfoque al permitir encontrar dinámicamente la literatura. 
A través de la propagación de citaciones se amplía y actualiza la literatura encontrada en las bases de datos tradicionales. De este modo, se superan limitaciones como la 
indexación incompleta, las variaciones en el uso de palabras clave o la exclusión de ciertas publicaciones.


\subsection*{Metodología: Umbrella Review con Propagación de Citaciones}
Como parte de la metodología Umbrella Review es necesario establecer un protocolo para ejecutar la revisión.
Se han considerado las siguientes fases para dicho protocolo:
\begin{enumerate}
    \item \textbf{Propósito de la revisión}  
    
    La revisión se justifica en la necesidad de consolidar evidencia secundaria de calidad, aprovechando el enfoque de propagación de citaciones 
    para garantizar una búsqueda amplia, estructurada y actualizada.

    \item \textbf{Objetivos específicos}  

    Se definen los objetivos generales y específicos que guiarán la identificación de literatura mediante la propagación de citaciones, 
    así como el proceso de síntesis de resultados.

    \item \textbf{Criterios de inclusión y exclusión}  

    Se definen de manera general como la incorporación de revisiones y metaanálisis que sean pertinentes, de calidad y relacionados con el tema de estudio, 
    y la exclusión de aquellos trabajos que no cumplan con estos requisitos de relevancia.
    
    \item \textbf{Identificación del estudio semilla y propagación de citaciones}  
    
    La búsqueda se inicia en bases de datos académicas como \textit{Scopus}, \textit{Web of Science}, \textit{IEEE Xplore} o \textit{Google Scholar}, a fin de localizar 
    un estudio semilla (revisión o resumen amplio) que ofrezca una cobertura representativa del tema.  
    A partir de este estudio, se aplica la estrategia de Back-and-Forward Citation Propagation, que combina:  
    \begin{itemize}
        \item \textit{Backward citation:} revisión de las referencias citadas en el estudio semilla.  
        \item \textit{Forward citation:} identificación de trabajos más recientes que citan al estudio semilla.  
    \end{itemize}
    De este modo, el corpus de literatura se amplía progresivamente hasta alcanzar un punto de saturación en el que la propagación deja de aportar nueva evidencia relevante.

    \item \textbf{Selección de revisiones relevantes}  

    A partir de la propagación de citaciones, se aplican los criterios de inclusión - exclusión para determinar qué revisiones serán incorporadas al análisis.
    

    \item \textbf{Valoración de la calidad de la evidencia}  

    La calidad de los estudios se evalúa según los criterios definidos, garantizando su consistencia al tema de estudio.
    Para apoyar este proceso se usa una herramienta de análisis que facilite la organización y valoración sistemática de la evidencia.

    \item \textbf{Extracción de información clave}  

    De cada revisión seleccionada se extraerán datos esenciales, organizados en una tabla de extracción que incluirá:  
    \begin{itemize}
        \item Autor y año de publicación  
        \item Objetivo del estudio  
        \item Tipo de revisión  
        \item Número de estudios primarios incluidos  
        \item Principales hallazgos  
        \item Conclusiones generales  
        \item Limitaciones reportadas  
    \end{itemize}

    \item \textbf{Síntesis y representación de resultados}  

    Los hallazgos se organizarán en dos niveles complementarios:  
    \begin{itemize}
        \item \textbf{Tabular:} tablas comparativas de las revisiones incluidas.
        \item \textbf{Narrativo:} síntesis descriptiva de los principales hallazgos.  
        \item \textbf{Temático y visual:} mapas de evidencia y esquemas que reflejen la propagación de citaciones, mostrando las conexiones entre estudios clave.  
    \end{itemize}

    \item \textbf{Discusión y conclusiones}  

    Los resultados se interpretan desde una perspectiva crítica, destacando fortalezas, limitaciones y la evolución de la evidencia en el tiempo. Se identifican coincidencias y divergencias entre revisiones, así como vacíos de conocimiento, y se proponen líneas de investigación futura.
\end{enumerate}
En esta metodología, el Umbrella Review se utiliza como marco general para sintetizar evidencia secundaria a partir de revisiones de exhaustivas de la literatura, 
complementándose 
con la propagación de citaciones para integrar aportes recientes y 
reflejar la evolución del conocimiento disponible.







\section{Enfoque Design Science Research (DSR)}
De acuerdo con vom Brocke et al. \textcite{vombrocke2020}, Design Science Research, desarrollada en 1969, es un paradigma de resolución de problemas que busca mejorar el conocimiento humano mediante la creación de artefactos innovadores. En otras palabras, es una metodología que crea soluciones a problemas reales y, al mismo tiempo, genera conocimiento útil y aplicable sobre cómo diseñar estas soluciones. Las etapas que se aplicarán en el presente trabajo son las siguientes:

\begin{itemize}[align=left, label=--]
\item \textbf{Identificación del problema y motivación} \
En esta etapa se precisa el problema y se justifica por qué es necesaria una solución. De acuerdo con Peffers et al. (2008), esta etapa exige analizar el problema en detalle, descomponiéndolo en sus partes clave para identificar sus causas, efectos y alcance. Además, es crucial justificar la relevancia del problema, tanto desde una perspectiva teórica (es decir, cómo contribuye al conocimiento académico) como desde una perspectiva práctica (cómo afecta a organizaciones, usuarios o sistemas reales).
También implica explorar la literatura para verificar que el problema es relevante, desafiante y nuevo, lo que permite definir los límites del proyecto de investigación.

\item \textbf{Definir los objetivos para la solución} \
Se plantean los criterios que debe cumplir una solución exitosa basándose en el conocimiento existente y en la factibilidad técnica y organizacional.
Los objetivos deberán permitir construir algo efectivo y deseable, no solamente desde el ámbito académico sino también en el entorno en que se aplicará. Estos pueden expresarse en términos cualitativos o cuantitativos; el investigador establece aquí la meta hacia donde se dirigirá el artefacto.

\item \textbf{Diseño y desarrollo del artefacto} \
En esta etapa se construye una solución concreta, como un modelo, software o sistema, que responde directamente a los objetivos planteados. Para ello, se utiliza el conocimiento existente que fundamenta las decisiones del diseño y la estructura del artefacto. No solo se trata de crear algo, sino de asegurar que pueda ser comprendido, evaluado y replicado por otros.

\item \textbf{Demostración del uso del artefacto para resolver el problema} \
Se muestra cómo se usa el artefacto en un escenario real o simulado. Esta demostración no valida científicamente su efectividad, sino que muestra su aplicabilidad, evidenciando que el artefacto propuesto puede operar de forma efectiva.
Por su parte, vom Brocke et al. (2020) destacan que esta etapa es fundamental para conectar el diseño teórico con la realidad del usuario o del entorno organizacional, permitiendo detectar oportunidades de mejora antes de una evaluación rigurosa.

\item \textbf{Evaluación del desempeño del artefacto} \
Se busca medir su efectividad, eficiencia e impacto, aportando evidencia que justifique su valor y utilidad. Además, según vom Brocke et al. (2020), esta puede asumirse de forma continua mediante una evaluación 
formativa que permita ciclos iterativos de rediseño y mejora a lo largo del proceso de investigación.
\item \textbf{Comunicación de los resultados al público académico y profesional} \
Finalmente, esta etapa consiste en difundir de forma clara los resultados del diseño y de la investigación realizada.
\end{itemize}

Estos pasos están basados en el modelo clásico de DSR de Peffers (2008), que vom Brocke adapta y expande en su guía.\\
\begin{landscape}
\begin{figure}[ht]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tikzpicture}[scale=0.9, transform shape, node distance=1cm and 1.5cm,
        squarednode/.style={rectangle, draw=black, fill=white, very thick, minimum size=5mm, text width=4cm},
        everynode/.style={align=center}, 
        rectalbecirc/.style={
        draw=black,           % borde negro
        fill=white,           % fondo blanco
        line width=0.1pt,           % grosor del borde
        rounded corners=9pt,  % esquinas redondeadas
        align=center,         % alinear texto al centro
        text width=3cm,       % ancho del texto
        minimum height=1cm,   % altura mínima del nodo
        inner sep=6.5pt,
        font=\small
        }
        ]
        % nodos
        %horizontal
        \node[squarednode] (problema) {Identificación del problema};
        \node[squarednode] (objetivos) [right=of problema] {Objetivos para la solución};
        \node[squarednode] (diseno) [right=of objetivos] {Diseño y desarrollo del artefacto};
        \node[squarednode] (demostracion) [right=of diseno] {Demostración};
        \node[squarednode] (evaluacion) [right=of demostracion] {Evaluación del desempeño};
        \node[squarednode] (comunicacion) [right=of evaluacion] {Comunicación de resultados};
        %vertical
        \node[rectalbecirc] (start) [below=of problema] {Centinela tiene que presentar mejores resultados en las respuestas al momento de buscar información y generar contenido de valor para el usuario.};
        \node[rectalbecirc] (obj) [below=of objetivos] {
            \vspace*{-\baselineskip} % reduce el espacio inicial de la lista
            \begin{itemize} [left=0pt]\setlength\itemsep{1pt}\setlength\leftmargini{1pt}
                \renewcommand\labelitemi{\tiny$\bullet$}
                \item Mejorar la precisión de las respuestas.
                \item Aumentar la relevancia del contenido generado.
                \item Optimizar la experiencia del usuario.
            \end{itemize}
        };
        \node[rectalbecirc] (design) [below=of diseno] {Se contempla un diseño que integra retriver, augmented y generation lo que da como resultado un buscador el cual su respuesta es en lenguaje natural.};
        \node[rectalbecirc] (dem) [below=of demostracion] {Se demuestra el uso del buscador en un entorno controlado, mostrando su capacidad para responder preguntas y generar contenido relevante.};
        \node[rectalbecirc] (eval) [below=of evaluacion] {Se evalúa el buscador mediante métricas estándar y FATE};
        \node[rectalbecirc] (com) [below=of comunicacion] {En el presente trabajo se presenta el proceso que se ha realizado para el desarrollo del buscador, así como los resultados obtenidos y recomendaciones para futuras mejoras.};

        %conexiones horizontales
        \draw[-to] (problema) -- (objetivos);
        \draw[-to] (objetivos) -- (diseno);
        \draw[-to] (diseno) -- (demostracion);
        \draw[-to] (demostracion) -- (evaluacion);
        \draw[-to] (evaluacion) -- (comunicacion);
        %conexiones verticales
        \draw[-to] (problema) -- (start);
        \draw[-to] (objetivos) -- (obj);
        \draw[-to] (diseno) -- (design);    
        \draw[-to] (demostracion) -- (dem);
        \draw[-to] (evaluacion) -- (eval);
        \draw[-to] (comunicacion) -- (com);

        %contenedor
        \node[draw=black, rounded corners=10pt, thick, minimum width=23cm, minimum height=4cm, fill=gray!5, anchor=north, name=contenedor] 
at ([yshift=-0.75cm]current bounding box.south) {};
        \coordinate (center) at (contenedor.center);
        \def\sep{7} % Separación horizontal
        % Elipses dentro del contenedor
        \node[draw=black, ellipse, minimum width=3cm, minimum height=2.2cm, fill=blue!10, align=center, font=\small] 
            (elipse1) at ($(center) + (-\sep,0)$) 
            {Definición de RAG};

        \node[draw=black, ellipse, minimum width=3cm, minimum height=2.2cm, fill=blue!10, align=center, font=\small] 
            (elipse2) at ($(center)$) 
            {Arquitectura de RAG};

        \node[draw=black, ellipse, minimum width=3cm, minimum height=2.2cm, fill=blue!10, align=center, font=\small] 
            (elipse3) at ($(center) + (\sep,0)$) 
            {Optimización de RAG};
        \node[align=center, text width=30cm, yshift=-1.5cm] at (center) {\fontsize{10}{12}\selectfont
    Puntos de entrada a la investigación};

        % Conexiones entre elipses y nodos
        \draw[-to] (elipse1) -- (start);
        \draw[-to] (elipse1) -- (obj);
        \draw[-to] (elipse2) -- (design);
        \draw[-to] (elipse3) -- (eval);
        \draw[-to] (elipse3) -- (dem);
        \draw[-to, bend right=20] (comunicacion) to node[pos=0.06, above,sloped]{Iterativo} (objetivos);
        \draw[-to, bend right=20] (comunicacion) to (diseno);
        \draw[-to, bend right=20] (comunicacion) to (evaluacion);
    \end{tikzpicture}
    }
    \caption{Proceso de Diseño de Investigación para el desarrollo de RAG en Centinela}
\end{figure}
\end{landscape}

\section{Diseño y desarrollo del artefacto}
De acuerdo a la literatura, se realiza el desarollo del RAG para Centinela usando las fases expuestas en la Figura \ref{fig:secciones-rag}. 

\subsection{Ingesta} 
De acuerdo al estudio exhaustiva de la literatura, el primer paso para la construccion de RAG es la obtencion de los datos. En nuestro caso
los datos provienen de Scopus donde se extrae los metadatos de los articulos, en particular título y abstract, los cuales concentran la representación semántica principal del artículo y son los puntos 
de entrada más frecuentes para consultas y embeddings\parencite{zhai2024llmIR}. De acuerdo con la literatura, se ha saleccionado los siguientes campos que 
contienen la información necesaria para representar el contenido del documento e identificarlo.

\begin{itemize}
    \item \textbf{Título y abstract}: condensan la información temática central del articulo y son la base principal para la recuperación y la generación de embeddings~\parencite{zhai2024llmIR}.
    \item \textbf{DOI}: garantiza trazabilidad y constata la fuente, enfrenta a los problemas de confianza y alucinación en RAG~\parencite{knollmeyer2024benchmarking}.
    \item \textbf{Autores}: son un metadato clave ya que permite analizar impacto académico y coautorías~\parencite{fan2021pretraining}. 
    \item \textbf{Afiliaciones}: aportan contexto institucional y geográfico, lo cual resulta útil para filtrar información por país o institución~\parencite{ibrihich2022review}.
    \item \textbf{Número de citaciones}: es un metadato adicional para priorizar documentos de mayor impacto en la fase de ranking~\parencite{bernard2025fate}.
    \item \textbf{ScopusID}: asegura que cada documento tenga un identificador único, evitando duplicados en la VDB~\parencite{ma2025vector}. 
\end{itemize}

De esta forma los campos título y abstract concentran la mayor parte de semántica relevante para crear embeddings y recuperar documentos, por lo que conviene tratarlos como el bloque principal de indexación.
En cambio, DOI, ScopusID, Autores y Afiliaciones funcionan como metadatos de soporte: ayudan a auditar la procedencia, filtrar por institución o región, destingir entre entidades y evitar duplicados en la base 
vectorial. Separar título/resumen de metadatos de control (DOI, ScopusID, autores, afiliaciones)  mejora la interpretabilidad del sistema y evita sobrecargar el espacio vectorial 
con señales que no aportan significado textual directo.

Con la siguiente función se extrae el corpus y se guardan en un CSV (UTF-8) para su procesamiento en fases posteriores.
\begin{minted}[linenos,frame=single,fontsize=\small]{text}
FUN exportar_articulos_a_csv():
 Consultar Neo4j (filtra Article con scopus_id/title/abstract/doi,
   colecta autores/afiliaciones; ORDER BY scopus_id)
  filas ← ejecutar_cypher(consulta)
  df ← DataFrame(filas)

Aplanar listas → string
  join_list(x): si lista → ", ".join(x) ; si no → x
  para c en [authors, affiliations, affiliation_cities, affiliation_countries]:
    si c ∈ df → df[c] ← map(join_list, df[c])

 Orden final de columnas
  df ← df[[title, abstract, doi, authors,
           affiliations, affiliation_cities, affiliation_countries,
           citation_count, scopus_id]]

Exportar CSV (utf-8, sin índice, sep="|")
  escribir_csv(df, "scopusdata.csv", sep="|", encoding="utf-8", index=False)
\end{minted}


\subsection{Preprocesamiento}
Una vez completada la ingesta de datos provienientes de Scopus, es fundamental preprocesarlos para normalizar y limpiar el texto, reducir ruido y mejorar la calidad semántica de las representaciones.
En esta etapa aplicare el siguiente flujo reproducible compuesto por estos pasos:
\begin{enumerate}
    \item \textbf{Normalización}. Consiste en estandarizar la forma del texto mediante 
    UTF-8, la corrección de caracteres extraños, la conversión a minúsculas, la eliminación de puntuación no informativa, 
    símbolos y números aislados, así como la normalización de espacios y se etiqueta el idioma de cada documento; no se fuerza la traducción, preservando la variación lingüística para análisis por idioma.
    para garantizar la coherencia. Cuando corresponde, se desduplican registros o resúmenes 
    (comparación por DOI o por título+autor+año) con el fin de homogeneizar el vocabulario y reducir sesgos por repeticiones.
Con el siguiente pseudocodigo se procedera a normalizar las columnas que contienen toda la semantica:
\begin{minted}[linenos,frame=single,fontsize=\small]{text}
FUN procesar_csv_a_parquet():
    df ← leer_csv("scopusdata.csv","|")
    normalizar(s): s←fix_text(s or ""); s←unicode_NFC(lower(s)); s←limpiar_simbolos(s); s←colapsar(s); return s
    df["title_norm"]←map(normalizar, df["title"]); df["abstract_norm"]←map(normalizar, df["abstract"])
    df ← dtypes_seguros(df) → objetos_a_json(df)
    guardar_parquet(df,"processed.parquet", prefer fastparquet(gzip) → pyarrow/CSV)

FUN etiquetar_idioma():
    set_langdetect_seed(0)
    df ← cargar_parquet("processed.parquet")
    src ← "abstract_norm" (crear si falta)
    detect_lang_safe(t): si vacío→"und"; si error→"und"; si no→detect(t)
    df["lang"] ← map(detect_lang_safe, df[src])
    guardar_parquet(df,"processed_lbl.parquet", prefer fastparquet(gzip) → pyarrow/CSV)
\end{minted}
Se lee el CSV (separador “|”) y se generan title-norm y abstract-norm corrigiendo codificación, pasando a 
minúsculas y limpiando caracteres no informativos; luego se guarda todo en formato Parquet (columna-orientado y 
comprimido) que conserva el esquema, reduce tamaño y acelera lecturas/filtrado frente a CSV. A continuación se 
carga el Parquet procesado, se detecta el idioma de abstract-norm con langdetect (columna lang, usando “und” si
es indeterminado) y se guarda un nuevo Parquet. La detección de idioma permite tratar corpus multilingües de forma óptima (stopwords/tokenización por lengua y elección del modelo de embeddings multilingüe).

 \item \textbf{Segmentación en oraciones}. Dividir el texto en oraciones completas utilizando delimitadores válidos (puntos, signos de cierre), con el fin de 
    estructurar unidades coherentes para el análisis posterior. 

\begin{minted}[frame=single,fontsize=\small]{text}
FUN segmentar_oraciones():
  IN="processed_lbl.parquet"; OUT="processed_sentences.parquet"; COL="abstract_norm"

  df ← leer_parquet(IN, prefer fastparquet → pyarrow)
  lang ← df["lang"] si existe; si no → "es"

  seg_es←pysbd(es); seg_en←pysbd(en)
  split(text, lg): si vacío→[]; usar seg_en si lg inicia "en", sino seg_es; si error→[text]

  df["sentences"] ← map(split, df[COL], lang)
  out ← explode(df,"sentences"); renombrar a "sentence"
  out ← add row_id_original (índice) y sentence_idx (cumcount por row_id)
  out ← columnas [scopus_id?, title?, abstract?, abstract_norm?, lang?, row_id_original, sentence_idx, sentence]

  guardar_parquet(out, OUT, prefer fastparquet(gzip) → pyarrow → CSV)
\end{minted}
Se lee el Parquet de entrada, elige la columna abstract-norm y segmenta sus textos en oraciones usando pysbd según el idioma que se detecto previamente.
Devuelve una lista de oraciones por fila; luego crea una fila por oración, conserva metadatos útiles 
añadiendo el índice de oración sentence-idx dentro de cada documento y escribe el resultado en processed-sentences.parquet.

    \item \textbf{Tokenización}. Segmentar cada oración en tokens (palabras o subpalabras) preservando frases clave frecuentes (p.\,ej.,\textit{aprendizaje automático}), a fin de mantener unidades semánticas estables.
\begin{minted}[frame=single,fontsize=\small]{text}
FUN extraer_frases_y_tokenizar():
  IN="processed_sentences.parquet"; OUT="corpus_token"; COL="sentence"

Cargar y tokenizar
  df ← leer_parquet(IN)                     
  df["tokens_base"] ← map(simple_tokenize, df[COL])

Frases (Gensim)
  S ← lista(df["tokens_base"])
  bigF ← Phraser(Phrases(S, min_count=5, threshold=10.0, delimiter=" "))
  triF ← Phraser(Phrases(bigF[S], min_count=5, threshold=10.0, delimiter=" "))

Aplicar y métricas
  df["tokens"]     ← [triF[bigF(t)] para t en df["tokens_base"]]
  df["tokens_csv"] ← join_por_comas(df["tokens"])
  df["n_tokens"]   ← len_por_fila(df["tokens"])

Guardar y mostrar
  guardar_parquet(df, OUT, prefer fastparquet(gzip) → pyarrow → CSV)
\end{minted}
Carga el parquet y tokeniza con una expresión regular y entrena detectores de bigramas y trigramas con Gensim usando mincount=5 y threshold=10. 
Luego aplica ambos phrasers para unir collocations frecuentes en un solo token el cual a su vez genera tokens. Los tokens son unidos por comas y se guardan en el nuevo parquet.

    \item \textbf{Eliminación de stop words}. Remover términos de alta frecuencia y escaso valor semántico.
\begin{minted}[frame=single,fontsize=\small]{text}
FUN filtrar_stopwords():
Entrada: df con df["tokens"]; Salida: corpus_token_nostop.parquet
  asegurar NLTK stopwords (ES/EN)
  STOP ← set(stopwords_es ∪ stopwords_en) normalizadas (lower+sin acentos)
  PROTECT ← {"in south america", ...}

  is_stop(t):
    si vacío→True; si t∈PROTECT→False
    parts ← split(t normalizado por espacios/guiones)
    return all(p∈STOP)  

  df["tokens_nostop"] ← map( filtra cada t: descarta numéricos/cortos/stop; conserva resto )
  df["text_for_embed"] ← join_espacios(df["tokens_nostop"])

  guardar df en "corpus_token_nostop.parquet"
\end{minted}
Una vez tokenizado eliminamos las stop words asegura las listas, normaliza a minúsculas y quita tildes.
La función is-stop considera tokens compuestos (espacios/guiones) y solo los descarta si todas sus partes 
son stopwords; además, filter-tokens aplica filtros básicos (longitud mínima, quitar numéricos)

    \item \textbf{Aplicación de Lematización o stemming}. Preferir lematización sobre \textit{stemming} para conservar la forma canónica y reducir 
    la varianza léxica sin pérdida de significado \parencite{tabassum2020}.
\begin{minted}[frame=single,fontsize=\small]{text}
FUN lematizar_spacy():
  df ← cargar "corpus_token_nostop.parquet"; assert "text_for_embed"

  cargar nlp_es (es_core_news_sm) y nlp_en (en_core_web_sm) sin parser/ner/textcat
  is_es(txt): True si tiene acentos o palabras {de, la, el, y, en, ...}

  idx_es ← filas donde is_es(text_for_embed); idx_en ← resto
  lema(doc): join(lemma_.lower() si ≠ "-PRON-" else text.lower())

  text_lemma[idx_es] ← map_pipe(nlp_es, df[idx_es].text_for_embed, batch=512)
  text_lemma[idx_en] ← map_pipe(nlp_en, df[idx_en].text_for_embed, batch=512)
  df["text_lemma"] ← text_lemma.fillna("")

  guardar df en "corpus_token_nostop_lemma.parquet" (fastparquet→pyarrow→CSV)
\end{minted}
Iniciamos leyendo el parquet de entrada de la fase anterior y y lematiza en lote con spaCy usando dos modelos (ES/EN) dependiendo de la etiqueta que habiamos usado agregado anteriormente.
Genera  tokens lematizados en minúsculas, separados por espacios y guarda el resultado en el nuevo parquet.

\item \textbf{\textit{Chunking} para LLM/RAG}. Dividir documentos largos en fragmentos (\(\sim 200\text{--}400\) tokens) con solape (15–30\,\%) para respetar 
    límites de contexto de los transformadores y mejorar la recuperación a nivel de pasaje (Hambarde \& Proença, 2023).
    \begin{minted}[frame=single,fontsize=\small]{text}
FUN chunkear_por_tokens():
    IN="corpus_token_nostop_lemma.parquet"; OUT="corpus_chunks.parquet"
    MAX=300; OVERLAP=0.2; STRIDE=MAX*(1-OVERLAP)
    tok ← AutoTokenizer("intfloat/multilingual-e5-base")

    df ← leer_parquet(IN)
    ordenar por scopus_id y (sentence_idx | row_id_original)
    doc_df ← groupby(scopus_id).join_espacios(text_lemma) → text_for_chunk
    doc_df ← asignar doc_id secuencial

    chunk(text):
    ids ← tok.encode(text, sin_special)
    para s = 0..len(ids) paso STRIDE:
        e ← min(s+MAX, len(ids))
        emitir {start_token:s, end_token:e, token_count:e-s,
                text_chunk: tok.decode(ids[s:e])}
        si e == len(ids) → break

  rows ← ∀ doc en doc_df, j, ch en enumerate(chunk(doc.text_for_chunk)):
            guardar {doc_id, chunk_id:j, chunk_uid:"doc_id-j",
                        scopus_id:doc.scopus_id, start_token, end_token,
                        token_count, text_chunk}

  chunks_df ← DataFrame(rows)
  guardar_parquet(chunks_df, OUT)   
\end{minted}
Finalmente, para fragmentar el texto tomamos el Parquet lematizado, consolidamos un solo documento por artículo 
y, con el tokenizer E5 (bi-encoder basado en transformer), lo separamos por conteo de tokens (tamaño = 300) con un solapamiento de 0.2.
\end{enumerate}
Con el uso de este pipeline se disminuye el ruido de contexto suministrado al LLM en RAG, 
lo que reduce inconsistencias y riesgo de alucinaciones en la generación. En términos de evaluación, una mejor normalización y segmentación
 favorecen la relevancia contextual del material recuperado y la fidelidad de las respuestas, al entregar fragementos más coherentes, comparables y medibles permitiendo
 calcular su similitud y una mejor relevancia contextual del material recuperado.


\subsection{Vectorizacion}
Para el diseño de este sistema se usa un retriever denso de arquitectura bi-encoder tipo BERT para la generación de embeddings, ya que permite codificar por separado la 
consulta y los fragmentos en un espacio vectorial de baja dimensión (vector corto y continuo de tamaño fijo) y comparar por similitud, 
capturando relaciones semánticas más allá de coincidencias léxicas y mejorando la recuperación en escenarios RAG y open-domain QA.
Además, estos embeddings se indexan en bases de datos vectoriales con búsqueda aproximada de vecinos cercanos (ANN), lo que viabiliza el escalamiento eficiente del sistema\parencite{joshi2025vector}.
\begin{minted}[frame=single,fontsize=\small]{text}
Cargar y validar
df = leer_parquet(PARQUET_PATH)
verificar(df, cols=["doc_id","chunk_id","start_token","end_token",
"text_chunk"])
limpiar_tipos(df); limpiar_texto(df)

IDs + textos
df["vec_id"] = 0..N-1
df["chunk_uid"] = f"{doc_id}-{chunk_id}"
textos = "passage: " + df["text_chunk"]

Modelo y vectorización (con backoff)
modelo = cargar_ST(EMB_MODEL, cpu, max_len=EMB_MAX_SEQLEN)
E = []
for lote in partir(textos, tam=BATCH_SIZE, backoff=÷2):
    E += [ modelo.encode(lote, normalizar=True) ]
E = apilar_float32(E); dim = cols(E)

guardar_npy(OUT_EMB_PATH, E)
meta = df[["vec_id","chunk_uid","doc_id","chunk_id",
        "start_token","end_token", "scopus_id?"]]
guardar_pkl(OUT_META_PKL, { "model": EMB_MODEL, "device_used": "cpu", "dim": dim, "ntotal": N, "meta_min": meta })
\end{minted}
Toma los textos del archivo parquet, los limpia y les asigna identificadores (vec-id, chunk-uid), luego usa un modelo SentenceTransformer (E5) en CPU para convertir cada fragmento en un vector numérico normalizado (embedding).
Procesa los textos por lotes (reduce el tamaño si hay errores de memoria), junta todos los embeddings en una matriz (N, dim) y finalmente guarda en dos archivos .npy (donde estan los vectores) y .pkl con la información mínima (modelo, dimensiones e índices que vinculan cada vector con su texto original).


\subsection{Vector DB}
\begin{minted}[frame=single,fontsize=\small]{text}
E    = cargar_npy(IN_EMB_PATH)      
meta = cargar_pkl(IN_META_PKL)       
assert cols(E)==meta.dim and filas(E)==meta.ntotal
idx = FAISS.IndexFlatIP(meta.dim) 
idx.add(E)
guardar_faiss(idx, FAISS_PATH)
guardar_pkl(INFO_PKL_PATH, {"model": meta.model, "dim": meta.dim,
                            "ntotal": idx.ntotal,
                            "embeddings_path": IN_EMB_PATH,
                            "meta_min_path": IN_META_PKL,
                            "faiss_path": FAISS_PATH})
\end{minted}
Carga los embeddings normalizados verifica que las dimensiones coincidan, construye un índice FAISS y  el resultado se guarda en disco (.bin)

\subsection{Recuperación}
\begin{minted}[frame=single,fontsize=\small]{text}
Cargar artefactos
meta = leer_pkl(PKL_MIN_PATH)["meta_min"]         
idx  = faiss_read(FAISS_PATH)
mdl  = ST(leer_pkl(PKL_MIN_PATH)["model"], cpu)   
Encode consulta y buscar
q_emb = mdl.encode(["query: "+Q], normalize=True, to_numpy=True).astype("float32")
scores, ids = idx.search(q_emb, k)

Armar TOP-K mínimo
hits = meta.loc[ids[0]].reset_index(drop=True)
hits.insert(1, "score", scores[0])

Unir con CSV por scopus_id
sc = read_csv(SCOPUS_CSV, sep=SCOPUS_SEP); sc["scopus_id"]=str
hits = merge_left(hits, sc, on="scopus_id")
\end{minted}
Primero cargamos la vector DB, luego vectorizamos  la consulta (query) y normaliza el embedding con ese vector hace búsqueda k-NN en FAISS y obtiene scores e ids.
Para obtener un contexto mejorado se une el resultado con el CSV de Scopus por scopus-id para agregar la demas metadata que contiene el csv.
\subsection{Re-ranking}

\subsection{Construcción de Contexto}

\subsection{Generación (LLM)}

\subsection{Evaluación}
