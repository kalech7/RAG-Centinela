{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad318955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import csv\n",
    "import pandas as pd, unicodedata, regex, json\n",
    "from pathlib import Path\n",
    "from ftfy import fix_text\n",
    "from pathlib import Path\n",
    "import pysbd\n",
    "import regex as re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750cfcf",
   "metadata": {},
   "source": [
    "# Ingesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406ac341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura tus credenciales y URL de conexión\n",
    "URI = \"bolt://localhost:7687\" \n",
    "USER = \"neo4j\"\n",
    "PASSWORD = \"password\"\n",
    "\n",
    "# Crear driver\n",
    "driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43a54a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doi', 'author_count', 'publication_date', 'abstract', 'title', 'scopus_id', 'neo4jImportId', 'affiliation_count', 'pk', 'name', 'country', 'city', 'auth_name', 'citation_count', 'initials', 'current_affiliation', 'first_name', 'last_name', 'updated', 'cursor', 'next_url']\n"
     ]
    }
   ],
   "source": [
    "node_properties_query = \"\"\"\n",
    "MATCH (n)\n",
    "UNWIND keys(n) AS prop\n",
    "RETURN DISTINCT prop AS property_name\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(node_properties_query)\n",
    "    columns = [record[\"property_name\"] for record in result]\n",
    "\n",
    "print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df91f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\n    MATCH (a:Article)\\n    WHERE a.scopus_id IS NOT NULL\\n      AND a.title IS NOT NULL AND a.title <> \"\"\\n      AND a.abstract IS NOT NULL AND a.abstract <> \"\"\\n      AND a.doi IS NOT NULL AND a.doi <> \"\"\\n    MATCH (au:Author)-[:WROTE]->(a)\\n    WITH a, collect(DISTINCT au.first_name + \" \" + au.last_name) AS authors\\n    WHERE size(authors) > 0\\n    OPTIONAL MATCH (a)-[:BELONGS_TO]->(af:Affiliation)\\n    WITH a, authors,\\n         collect(DISTINCT af.name)    AS affiliations,\\n         collect(DISTINCT af.city)    AS affiliation_cities,\\n         collect(DISTINCT af.country) AS affiliation_countries\\n    RETURN\\n      a.scopus_id                    AS scopus_id,\\n      a.title                        AS title,\\n      a.abstract                     AS abstract,\\n      a.doi                          AS doi,\\n      authors                        AS authors,\\n      affiliations                   AS affiliations,\\n      affiliation_cities             AS affiliation_cities,\\n      affiliation_countries          AS affiliation_countries,\\n      coalesce(a.citation_count, 0)  AS citation_count\\n    ORDER BY scopus_id\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportación completada: scopusdata.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def export_articles_to_csv():\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Article)\n",
    "    WHERE a.scopus_id IS NOT NULL\n",
    "      AND a.title IS NOT NULL AND a.title <> \"\"\n",
    "      AND a.abstract IS NOT NULL AND a.abstract <> \"\"\n",
    "      AND a.doi IS NOT NULL AND a.doi <> \"\"\n",
    "    MATCH (au:Author)-[:WROTE]->(a)\n",
    "    WITH a, collect(DISTINCT au.first_name + \" \" + au.last_name) AS authors\n",
    "    WHERE size(authors) > 0\n",
    "    OPTIONAL MATCH (a)-[:BELONGS_TO]->(af:Affiliation)\n",
    "    WITH a, authors,\n",
    "         collect(DISTINCT af.name)    AS affiliations,\n",
    "         collect(DISTINCT af.city)    AS affiliation_cities,\n",
    "         collect(DISTINCT af.country) AS affiliation_countries\n",
    "    RETURN\n",
    "      a.scopus_id                    AS scopus_id,\n",
    "      a.title                        AS title,\n",
    "      a.abstract                     AS abstract,\n",
    "      a.doi                          AS doi,\n",
    "      authors                        AS authors,\n",
    "      affiliations                   AS affiliations,\n",
    "      affiliation_cities             AS affiliation_cities,\n",
    "      affiliation_countries          AS affiliation_countries,\n",
    "      coalesce(a.citation_count, 0)  AS citation_count\n",
    "    ORDER BY scopus_id\n",
    "    \"\"\"\n",
    "\n",
    "    # Ejecuta consulta y arma DataFrame\n",
    "    with driver.session() as session:\n",
    "        rows = [dict(r) for r in session.run(query)]\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Une listas con '; ' (evita introducir comas que confundan a quien lo lea a mano)\n",
    "    def join_list(x):\n",
    "        return \"; \".join(str(v) for v in x if v) if isinstance(x, list) else x\n",
    "\n",
    "    for col in [\"authors\", \"affiliations\", \"affiliation_cities\", \"affiliation_countries\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(join_list)\n",
    "\n",
    "    # Orden de columnas\n",
    "    df = df[[\n",
    "        \"title\", \"abstract\", \"doi\", \"authors\",\n",
    "        \"affiliations\", \"affiliation_cities\", \"affiliation_countries\",\n",
    "        \"citation_count\", \"scopus_id\"\n",
    "    ]]\n",
    "\n",
    "    # Exporta con separador '|'\n",
    "    # - quoting=QUOTE_MINIMAL: si algún campo contiene el separador '|', Pandas lo pondrá entre comillas.\n",
    "    # - lineterminator=\"\\n\": EOL consistente.\n",
    "    df.to_csv(\n",
    "        \"scopusdata.csv\",\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "        sep=\"|\",\n",
    "        quoting=csv.QUOTE_MINIMAL,\n",
    "        lineterminator=\"\\n\",\n",
    "    )\n",
    "    print(\"Exportación completada: scopusdata.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_articles_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c781b1",
   "metadata": {},
   "source": [
    "pseudocodigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf779b",
   "metadata": {},
   "source": [
    "# normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788aa3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo → /run/media/alech/backup/Github/tesis/processed.parquet\n",
      "                                          title_norm  \\\n",
      "0  thou shalt not die in this place : an ethnomet...   \n",
      "1  use of learning frames in climate change commu...   \n",
      "2  free access to public ecuadorian universities:...   \n",
      "\n",
      "                                       abstract_norm  \n",
      "0  ecuador, located in south america, has a popul...  \n",
      "1  differences in climate change learning frames ...  \n",
      "2  a free higher education policy was implemented...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- Configuración --------\n",
    "INPUT_CSV   = \"scopusdata.csv\"      # archivo con separador '|'\n",
    "OUTPUT_PATH = \"processed.parquet\"   # salida recomendada (parquet)\n",
    "REMOVE_ISOLATED_NUMBERS = False     # True si quieres quitar números sueltos\n",
    "\n",
    "# -------- Funciones --------\n",
    "def normalize_unicode_and_case(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = fix_text(s)                        # corrige codificación/caracteres raros\n",
    "    s = s.replace(\"\\u00A0\", \" \")           # NBSP -> espacio normal\n",
    "    s = unicodedata.normalize(\"NFC\", s)    # Unicode canónica\n",
    "    s = s.lower()                          # minúsculas\n",
    "    return s\n",
    "\n",
    "def strip_non_informative(s: str, remove_numbers: bool = False) -> str:\n",
    "    # Conserva letras/números/espacios y signos básicos de textos científicos\n",
    "    s = regex.sub(r\"[^\\p{L}\\p{N}\\s\\-\\.,;:()\\[\\]/%]\", \" \", s)\n",
    "    if remove_numbers:\n",
    "        # Elimina números aislados; conserva casos como \"co2\", \"iso-9001\"\n",
    "        s = regex.sub(r\"\\b\\d+\\b\", \" \", s)\n",
    "    s = regex.sub(r\"\\s+\", \" \", s).strip()  # espacios\n",
    "    return s\n",
    "\n",
    "def normalize_text(s: str, remove_numbers: bool = False) -> str:\n",
    "    s = normalize_unicode_and_case(s)\n",
    "    s = strip_non_informative(s, remove_numbers=remove_numbers)\n",
    "    return s\n",
    "\n",
    "def safe_convert_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convierte a dtypes 'seguros' sin depender de opciones globales.\"\"\"\n",
    "    try:\n",
    "        return df.convert_dtypes(dtype_backend=\"numpy_nullable\")  # pandas nuevos\n",
    "    except TypeError:\n",
    "        return df.convert_dtypes()  # pandas más viejos\n",
    "\n",
    "def sanitize_objects(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convierte objetos no escalares (listas/dicts) a JSON string.\"\"\"\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            df[c] = df[c].map(\n",
    "                lambda x: x if isinstance(x, (str, int, float, bool, type(None)))\n",
    "                else json.dumps(x, ensure_ascii=False)\n",
    "            )\n",
    "    return df\n",
    "\n",
    "def try_save_parquet(df: pd.DataFrame, path: str) -> bool:\n",
    "    \"\"\"Intenta guardar con fastparquet, luego pyarrow. Devuelve True si logra parquet.\"\"\"\n",
    "    # 1) fastparquet\n",
    "    try:\n",
    "        import fastparquet  # noqa: F401\n",
    "        df.to_parquet(path, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) pyarrow\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        df.to_parquet(path, index=False, engine=\"pyarrow\")  # compresión por defecto\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# -------- Proceso --------\n",
    "# Lee CSV con separador pipe. Si tu exportación puso comillas cuando había '|',\n",
    "# pandas las respeta automáticamente.\n",
    "df = pd.read_csv(INPUT_CSV, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "# Asegura presencia de columnas requeridas\n",
    "for col in [\"title\", \"abstract\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "# Normalización SOLO sobre texto analizable\n",
    "df[\"title_norm\"]    = df[\"title\"].fillna(\"\").map(lambda x: normalize_text(x, REMOVE_ISOLATED_NUMBERS))\n",
    "df[\"abstract_norm\"] = df[\"abstract\"].fillna(\"\").map(lambda x: normalize_text(x, REMOVE_ISOLATED_NUMBERS))\n",
    "\n",
    "# Guardar salida con metadatos originales + columnas normalizadas\n",
    "cols_out = list(df.columns)\n",
    "for c in [\"title_norm\", \"abstract_norm\"]:\n",
    "    if c not in cols_out:\n",
    "        cols_out.append(c)\n",
    "\n",
    "# Copia de salida + saneo de tipos\n",
    "out = df[cols_out].copy()\n",
    "out = safe_convert_dtypes(out)\n",
    "\n",
    "# Asegura strings planos en columnas de texto clave\n",
    "for c in [\"title\", \"abstract\", \"title_norm\", \"abstract_norm\"]:\n",
    "    if c in out.columns:\n",
    "        out[c] = out[c].astype(str)\n",
    "\n",
    "# Serializa objetos complejos a JSON para evitar fallos de parquet\n",
    "out = sanitize_objects(out)\n",
    "\n",
    "# -------- Guardado robusto --------\n",
    "parquet_ok = try_save_parquet(out, OUTPUT_PATH)\n",
    "\n",
    "if parquet_ok:\n",
    "    print(f\"Listo → {Path(OUTPUT_PATH).resolve()}\")\n",
    "else:\n",
    "    # Fallback a CSV para no perder progreso\n",
    "    fallback = Path(OUTPUT_PATH).with_suffix(\".csv\")\n",
    "    out.to_csv(fallback, index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "    print(\"No se pudo escribir Parquet con fastparquet ni pyarrow. \"\n",
    "          f\"Se guardó CSV en → {fallback.resolve()}\")\n",
    "\n",
    "# Vista rápida\n",
    "print(out[[\"title_norm\", \"abstract_norm\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d8ac3",
   "metadata": {},
   "source": [
    "### deteccion de idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63916c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lang                                      abstract_norm\n",
      "0   en  ecuador, located in south america, has a popul...\n",
      "1   en  differences in climate change learning frames ...\n",
      "2   en  a free higher education policy was implemented...\n",
      "3   en  this study explored the influence of each fami...\n",
      "4   en  the rapid adoption and the diversification of ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "PATH = \"processed.parquet\"  \n",
    "OUT  = \"processed_lbl.parquet\"   \n",
    "\n",
    "DetectorFactory.seed = 0  # resultados más estables\n",
    "\n",
    "def detect_lang_safe(text: str) -> str:\n",
    "    t = (text or \"\").strip()\n",
    "    if not t:\n",
    "        return \"und\"  # indeterminado\n",
    "    try:\n",
    "        return detect(t)\n",
    "    except Exception:\n",
    "        return \"und\"\n",
    "\n",
    "# --- cargar parquet ---\n",
    "# intenta fastparquet y luego pyarrow\n",
    "try:\n",
    "    df = pd.read_parquet(PATH, engine=\"fastparquet\")\n",
    "except Exception:\n",
    "    df = pd.read_parquet(PATH, engine=\"pyarrow\")\n",
    "\n",
    "# --- elegir fuente para detección\n",
    "source_col = \"abstract_norm\" \n",
    "if source_col not in df.columns:\n",
    "    # si no existe ninguna, crea vacía para no romper\n",
    "    df[source_col] = \"\"\n",
    "\n",
    "# --- detectar idioma ---\n",
    "df[\"lang\"] = df[source_col].map(detect_lang_safe)\n",
    "\n",
    "# --- guardar ---\n",
    "try:\n",
    "    df.to_parquet(OUT, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        df.to_parquet(OUT, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        # último recurso: CSV para no perder el trabajo\n",
    "        Path(OUT).with_suffix(\".csv\")\n",
    "        df.to_csv(Path(OUT).with_suffix(\".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(df[[\"lang\", source_col]].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0979f1",
   "metadata": {},
   "source": [
    "## segmentar oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63baabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo:\n",
      "   row_id_original  sentence_idx  \\\n",
      "0                0             0   \n",
      "1                0             1   \n",
      "2                0             2   \n",
      "3                0             3   \n",
      "4                0             4   \n",
      "5                0             5   \n",
      "6                0             6   \n",
      "7                0             7   \n",
      "8                0             8   \n",
      "9                0             9   \n",
      "\n",
      "                                            sentence  \n",
      "0  ecuador, located in south america, has a popul...  \n",
      "1  according to the national institution of stati...  \n",
      "2  palliative care and hospice are relatively new...  \n",
      "3  in ecuador people usually die at home, in hosp...  \n",
      "4  in 2012, the first ecuadorian hospice was crea...  \n",
      "5  according to symbolic interactionism theory, r...  \n",
      "6  symbolic interactionism proposes that human be...  \n",
      "7  through an ethnomethodological approach, the f...  \n",
      "8  results emerge from the introspection of real ...  \n",
      "9  based on a thematic analysis, the following st...  \n",
      "\n",
      "Guardado → /run/media/alech/backup/Github/tesis/processed_sentences.parquet\n"
     ]
    }
   ],
   "source": [
    "# -------- Configuración --------\n",
    "INPUT_PARQUET  = \"processed_lbl.parquet\"      # entrada\n",
    "OUTPUT_PARQUET = \"processed_sentences.parquet\"  # salida\n",
    "SOURCE_COL     = \"abstract_norm\"               # columna a segmentar\n",
    "\n",
    "# -------- Carga robusta --------\n",
    "def read_parquet_any(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_parquet(path, engine=\"fastparquet\")\n",
    "    except Exception:\n",
    "        return pd.read_parquet(path, engine=\"pyarrow\")\n",
    "\n",
    "# -------- Segmentadores (ES / EN) --------\n",
    "seg_es = pysbd.Segmenter(language=\"es\", clean=False)\n",
    "seg_en = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "\n",
    "def split_by_lang(text: str, lang: str = \"es\") -> list[str]:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    lang = (lang or \"es\").lower()\n",
    "    seg = seg_en if lang.startswith(\"en\") else seg_es\n",
    "    try:\n",
    "        return seg.segment(text.strip())\n",
    "    except Exception:\n",
    "        # fallback simple si falla pysbd\n",
    "        return [text.strip()]\n",
    "\n",
    "# -------- Proceso --------\n",
    "df = read_parquet_any(INPUT_PARQUET)\n",
    "\n",
    "if SOURCE_COL not in df.columns:\n",
    "    df[SOURCE_COL] = \"\"\n",
    "\n",
    "# si tienes columna de idioma, úsala; si no, asume \"es\"\n",
    "lang_series = df[\"lang\"] if \"lang\" in df.columns else [\"es\"] * len(df)\n",
    "\n",
    "# segmentar\n",
    "df[\"sentences\"] = [\n",
    "    split_by_lang(text, lang)\n",
    "    for text, lang in zip(df[SOURCE_COL], lang_series)\n",
    "]\n",
    "\n",
    "# una oración por fila\n",
    "out = df.explode(\"sentences\", ignore_index=False)\n",
    "out = out.rename(columns={\"sentences\": \"sentence\"})\n",
    "out = out.reset_index(names=\"row_id_original\")\n",
    "out[\"sentence_idx\"] = out.groupby(\"row_id_original\").cumcount()\n",
    "\n",
    "# columnas finales\n",
    "keep = []\n",
    "for c in [\"scopus_id\", \"title\", \"abstract\", \"abstract_norm\", \"lang\"]:\n",
    "    if c in out.columns:\n",
    "        keep.append(c)\n",
    "keep += [\"row_id_original\", \"sentence_idx\", \"sentence\"]\n",
    "out = out[keep]\n",
    "\n",
    "# -------- Guardar --------\n",
    "try:\n",
    "    out.to_parquet(OUTPUT_PARQUET, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        out.to_parquet(OUTPUT_PARQUET, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        out.to_csv(Path(OUTPUT_PARQUET).with_suffix(\".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Ejemplo:\")\n",
    "print(out[[\"row_id_original\", \"sentence_idx\", \"sentence\"]].head(10))\n",
    "print(f\"\\nGuardado → {Path(OUTPUT_PARQUET).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbe64b",
   "metadata": {},
   "source": [
    "## tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae16b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0  ecuador, located in south america, has a popul...   \n",
      "1  according to the national institution of stati...   \n",
      "2  palliative care and hospice are relatively new...   \n",
      "3  in ecuador people usually die at home, in hosp...   \n",
      "4  in 2012, the first ecuadorian hospice was crea...   \n",
      "5  according to symbolic interactionism theory, r...   \n",
      "6  symbolic interactionism proposes that human be...   \n",
      "7  through an ethnomethodological approach, the f...   \n",
      "\n",
      "                                          tokens_csv  \n",
      "0  ecuador,located,in south america,has,a,populat...  \n",
      "1  according to the,national,institution,of,stati...  \n",
      "2  palliative care,and,hospice,are,relatively new...  \n",
      "3  in,ecuador,people,usually,die,at home,in,hospi...  \n",
      "4   in,2012,the,first,ecuadorian,hospice,was created  \n",
      "5  according to,symbolic,interactionism,theory,re...  \n",
      "6  symbolic,interactionism,proposes,that,human be...  \n",
      "7  through,an,ethnomethodological,approach,the,fo...  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "INPUT_PARQUET  = \"processed_sentences.parquet\"\n",
    "OUTPUT_PARQUET = \"corpus_token\"\n",
    "SENT_COL = \"sentence\"\n",
    "\n",
    "TOKEN_RE = re.compile(r\"(?:[^\\W_]+(?:[-_][^\\W_]+)+|\\d+(?:\\.\\d+)+|[^\\W_]+)\", re.VERBOSE | re.IGNORECASE | re.UNICODE)\n",
    "def simple_tokenize(s:str):\n",
    "    if not isinstance(s, str): return []\n",
    "    return TOKEN_RE.findall(re.sub(r\"\\s+\", \" \", s.strip()))\n",
    "\n",
    "# 1) Carga y tokeniza todas las oraciones\n",
    "def read_parquet_any(p):\n",
    "    try: return pd.read_parquet(p, engine=\"fastparquet\")\n",
    "    except Exception: return pd.read_parquet(p, engine=\"pyarrow\")\n",
    "\n",
    "df = read_parquet_any(INPUT_PARQUET)\n",
    "df[\"tokens_base\"] = df[SENT_COL].map(simple_tokenize)\n",
    "\n",
    "# 2) Entrena bigramas y (opcional) trigramas\n",
    "sentences = df[\"tokens_base\"].tolist()\n",
    "\n",
    "# ⚠️ delimiter must be str if tokens are str\n",
    "bigram = Phrases(sentences, min_count=5, threshold=10.0, delimiter=\" \")\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "trigram = Phrases(bigram_phraser[sentences], min_count=5, threshold=10.0, delimiter=\" \")\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# 3) Aplica: pega frases automáticamente (p.ej., aprendizaje_automático)\n",
    "df[\"tokens\"] = [trigram_phraser[bigram_phraser[toks]] for toks in df[\"tokens_base\"]]\n",
    "df[\"tokens_csv\"] = df[\"tokens\"].map(lambda xs: \",\".join(xs))\n",
    "df[\"n_tokens\"] = df[\"tokens\"].map(len)\n",
    "\n",
    "# 4) Guarda\n",
    "try:\n",
    "    df.to_parquet(OUTPUT_PARQUET, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try: df.to_parquet(OUTPUT_PARQUET, index=False, engine=\"pyarrow\")\n",
    "    except Exception: df.to_csv(Path(OUTPUT_PARQUET).with_suffix(\".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(df[[SENT_COL, \"tokens_csv\"]].head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0786c93",
   "metadata": {},
   "source": [
    "### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f7801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0  ecuador, located in south america, has a popul...   \n",
      "1  according to the national institution of stati...   \n",
      "2  palliative care and hospice are relatively new...   \n",
      "3  in ecuador people usually die at home, in hosp...   \n",
      "4  in 2012, the first ecuadorian hospice was crea...   \n",
      "5  according to symbolic interactionism theory, r...   \n",
      "6  symbolic interactionism proposes that human be...   \n",
      "7  through an ethnomethodological approach, the f...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [ecuador, located, in south america, has, a, p...   \n",
      "1  [according to the, national, institution, of, ...   \n",
      "2  [palliative care, and, hospice, are, relativel...   \n",
      "3  [in, ecuador, people, usually, die, at home, i...   \n",
      "4  [in, 2012, the, first, ecuadorian, hospice, wa...   \n",
      "5  [according to, symbolic, interactionism, theor...   \n",
      "6  [symbolic, interactionism, proposes, that, hum...   \n",
      "7  [through, an, ethnomethodological, approach, t...   \n",
      "\n",
      "                                       tokens_nostop  \\\n",
      "0  [ecuador, located, in south america, populatio...   \n",
      "1  [according to the, national, institution, stat...   \n",
      "2  [palliative care, hospice, relatively new, con...   \n",
      "3  [ecuador, people, usually, die, at home, hospi...   \n",
      "4          [first, ecuadorian, hospice, was created]   \n",
      "5  [according to, symbolic, interactionism, theor...   \n",
      "6  [symbolic, interactionism, proposes, human bei...   \n",
      "7  [ethnomethodological, approach, following, res...   \n",
      "\n",
      "                                      text_for_embed  \n",
      "0  ecuador located in south america population mi...  \n",
      "1  according to the national institution statisti...  \n",
      "2  palliative care hospice relatively new concept...  \n",
      "3  ecuador people usually die at home hospitals n...  \n",
      "4               first ecuadorian hospice was created  \n",
      "5  according to symbolic interactionism theory re...  \n",
      "6  symbolic interactionism proposes human beings ...  \n",
      "7  ethnomethodological approach following researc...  \n",
      "Guardado en: corpus_token_nostop.parquet\n"
     ]
    }
   ],
   "source": [
    "# === Quitar stopwords sobre df[\"tokens\"] con n-gramas separados por espacio ===\n",
    "import re, unicodedata, os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stop\n",
    "\n",
    "# Asegura recurso stopwords NLTK\n",
    "try:\n",
    "    _ = nltk_stop.words(\"spanish\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "# Idiomas a filtrar (ajusta a [\"spanish\"] si quieres solo ES)\n",
    "LANGS = [\"spanish\", \"english\"]\n",
    "\n",
    "# Construye set de stopwords\n",
    "STOPSET = set()\n",
    "for lang in LANGS:\n",
    "    try:\n",
    "        STOPSET |= set(nltk_stop.words(lang))\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "STOPSET_NORM = {_norm(w) for w in STOPSET}\n",
    "\n",
    "# Frases que no se filtran nunca (escribe aquí con ESPACIOS)\n",
    "PROTECT_PHRASES = {\n",
    "    \"in south america\",\n",
    "    # añade más si quieres: \"public health\", \"quality of life\", ...\n",
    "}\n",
    "\n",
    "def is_stop(tok: str) -> bool:\n",
    "    \"\"\"\n",
    "    Mantén n-gramas con contenido: elimina solo si TODAS las partes\n",
    "    (separadas por espacio o guion) son stopwords; protege frases explícitas.\n",
    "    \"\"\"\n",
    "    if not isinstance(tok, str) or not tok:\n",
    "        return True  # vacío o no-string -> descartar\n",
    "\n",
    "    t = _norm(tok).strip()\n",
    "\n",
    "    # Protección explícita\n",
    "    if t in PROTECT_PHRASES:\n",
    "        return False\n",
    "\n",
    "    # Token simple (sin espacios ni guiones)\n",
    "    if (\" \" not in t) and (\"-\" not in t):\n",
    "        return t in STOPSET_NORM\n",
    "\n",
    "    # Token compuesto: separa por espacios o guiones (uno o más)\n",
    "    parts = [p for p in re.split(r\"[ \\-]+\", t) if p]\n",
    "    if not parts:\n",
    "        return True\n",
    "\n",
    "    # Elimina SOLO si *todas* las partes son stopwords\n",
    "    return all(p in STOPSET_NORM for p in parts)\n",
    "\n",
    "def filter_tokens(tokens, min_len=2, drop_numeric=True):\n",
    "    out = []\n",
    "    if not isinstance(tokens, (list, tuple)):\n",
    "        return out\n",
    "    for t in tokens:\n",
    "        if not isinstance(t, str) or not t:\n",
    "            continue\n",
    "        if drop_numeric and t.isnumeric():\n",
    "            continue\n",
    "        if len(t) < min_len:\n",
    "            continue\n",
    "        if is_stop(t):\n",
    "            continue\n",
    "        out.append(t)\n",
    "    return out\n",
    "\n",
    "# --- Aplicar al DF (requiere df[\"tokens\"] como lista de strings) ---\n",
    "if \"tokens\" not in df.columns:\n",
    "    raise KeyError(\"Se esperaba df['tokens']. Asegúrate de haber generado los n-gramas antes.\")\n",
    "\n",
    "df[\"tokens_nostop\"] = df[\"tokens\"].map(filter_tokens)\n",
    "\n",
    "# (Opcional) Texto para embeddings (bi-encoder): tokens unidos por espacio\n",
    "df[\"text_for_embed\"] = df[\"tokens_nostop\"].map(lambda xs: \" \".join(xs))\n",
    "\n",
    "# Vistazo rápido (muestra si existen)\n",
    "cols_show = [c for c in [\"sentence\", \"tokens\", \"tokens_nostop\", \"text_for_embed\"] if c in df.columns]\n",
    "print(df[cols_show].head(8))\n",
    "\n",
    "# --- Guardar a nuevo archivo para no sobrescribir el original ---\n",
    "OUT_BASE = \"corpus_token_nostop\"\n",
    "parquet_path = f\"{OUT_BASE}.parquet\"\n",
    "try:\n",
    "    df.to_parquet(parquet_path, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        df.to_parquet(parquet_path, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        df.to_csv(f\"{OUT_BASE}.csv\", index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Guardado en:\",\n",
    "      parquet_path if os.path.exists(parquet_path) else f\"{OUT_BASE}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263b065",
   "metadata": {},
   "source": [
    "## lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef675ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      text_for_embed  \\\n",
      "0  ecuador located in south america population mi...   \n",
      "1  according to the national institution statisti...   \n",
      "2  palliative care hospice relatively new concept...   \n",
      "3  ecuador people usually die at home hospitals n...   \n",
      "4               first ecuadorian hospice was created   \n",
      "5  according to symbolic interactionism theory re...   \n",
      "6  symbolic interactionism proposes human beings ...   \n",
      "7  ethnomethodological approach following researc...   \n",
      "\n",
      "                                          text_lemma  \n",
      "0  ecuador locate in south america population mil...  \n",
      "1  accord to the national institution statistic e...  \n",
      "2  palliative care hospice relatively new concept...  \n",
      "3  ecuador people usually die at home hospital nu...  \n",
      "4                 first ecuadorian hospice be create  \n",
      "5  accord to symbolic interactionism theory resea...  \n",
      "6  symbolic interactionism propose human being ca...  \n",
      "7  ethnomethodological approach follow research a...  \n",
      "Guardado en: corpus_token_nostop_lemma.parquet\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZE text_for_embed (ES/EN) con spaCy en batch\n",
    "import re, unicodedata, os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"corpus_token_nostop.parquet\")\n",
    "\n",
    "assert \"text_for_embed\" in df.columns, \"Falta la columna 'text_for_embed'.\"\n",
    "\n",
    "# Carga modelos spaCy (puedes usar *_md/_lg si los tienes)\n",
    "nlp_es = spacy.load(\"es_core_news_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "nlp_en = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\",\"textcat\"])\n",
    "\n",
    "# Heurística ligera para detectar español\n",
    "SPANISH_CUES = {\"de\",\"la\",\"el\",\"los\",\"las\",\"y\",\"en\",\"para\",\"con\",\"por\",\"del\",\"al\",\"un\",\"una\",\"unos\",\"unas\",\"se\",\"su\",\"sus\"}\n",
    "ACCENTS_RE = re.compile(r\"[áéíóúñüÁÉÍÓÚÑÜ]\")\n",
    "\n",
    "def _is_spanish_like(text: str) -> bool:\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    if ACCENTS_RE.search(text):\n",
    "        return True\n",
    "    words = [w.lower() for w in re.split(r\"\\s+\", text.strip()) if w]\n",
    "    # si al menos una palabra típica española aparece -> ES\n",
    "    return any(w in SPANISH_CUES for w in words)\n",
    "\n",
    "def _lemma_doc(doc):\n",
    "    # cuida pronombres (algunos modelos antiguos devuelven \"-PRON-\")\n",
    "    toks = []\n",
    "    for t in doc:\n",
    "        lem = t.lemma_ if t.lemma_ and t.lemma_ != \"-PRON-\" else t.text\n",
    "        toks.append(lem.lower())\n",
    "    # une con espacios (mismo formato que text_for_embed)\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# Divide índices por idioma con la heurística\n",
    "idx_es = df.index[df[\"text_for_embed\"].apply(_is_spanish_like)].tolist()\n",
    "idx_en = df.index.difference(idx_es).tolist()\n",
    "\n",
    "# Lematiza en batch por idioma (más rápido que fila a fila)\n",
    "text_lemma = pd.Series(index=df.index, dtype=object)\n",
    "\n",
    "if idx_es:\n",
    "    docs_es = nlp_es.pipe(df.loc[idx_es, \"text_for_embed\"].fillna(\"\"), batch_size=512, n_process=1)\n",
    "    for i, doc in zip(idx_es, docs_es):\n",
    "        text_lemma.loc[i] = _lemma_doc(doc)\n",
    "\n",
    "if idx_en:\n",
    "    docs_en = nlp_en.pipe(df.loc[idx_en, \"text_for_embed\"].fillna(\"\"), batch_size=512, n_process=1)\n",
    "    for i, doc in zip(idx_en, docs_en):\n",
    "        text_lemma.loc[i] = _lemma_doc(doc)\n",
    "\n",
    "# Asigna columna nueva\n",
    "df[\"text_lemma\"] = text_lemma.fillna(\"\")\n",
    "\n",
    "# Vista rápida\n",
    "print(df[[\"text_for_embed\", \"text_lemma\"]].head(8))\n",
    "\n",
    "# Guardar (nuevo archivo para no pisar el anterior)\n",
    "OUT = \"corpus_token_nostop_lemma.parquet\"\n",
    "try:\n",
    "    df.to_parquet(OUT, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "except Exception:\n",
    "    try:\n",
    "        df.to_parquet(OUT, index=False, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        df.to_csv(\"corpus_token_nostop_lemma.csv\", index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Guardado en:\", OUT if os.path.exists(OUT) else \"corpus_token_nostop_lemma.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788917fb",
   "metadata": {},
   "source": [
    "## chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07593bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc_id  chunk_id chunk_uid    scopus_id  start_token  end_token  \\\n",
      "0       0         0       0-0  85059061481            0        156   \n",
      "1       1         0       1-0  85061967853            0         94   \n",
      "2       2         0       2-0  85067792389            0         60   \n",
      "3       3         0       3-0  85068192726            0        255   \n",
      "4       4         0       4-0  85069901345            0        173   \n",
      "5       5         0       5-0  85070472925            0        205   \n",
      "6       6         0       6-0  85071977997            0        195   \n",
      "7       7         0       7-0  85072017885            0        169   \n",
      "\n",
      "   token_count                                         text_chunk  \n",
      "0          156  ecuador locate in south america population mil...  \n",
      "1           94  difference climate change learning frame pedag...  \n",
      "2           60  free high education policy be implement ecuado...  \n",
      "3          255  this study explore influence family member lif...  \n",
      "4          173  rapid adoption diversification cloud compute t...  \n",
      "5          205  traditional pretest prove homoscedasticity e g...  \n",
      "6          195  polymer electrolyte fuel cell pefc produce ele...  \n",
      "7          169  traffic prediction high accuracy have become v...  \n",
      "N docs (únicos scopus_id): 19233 | N chunks: 21005\n",
      "Guardado en: corpus_chunks.parquet\n"
     ]
    }
   ],
   "source": [
    "# ====== CHUNKING *SIEMPRE* DESDE text_lemma (agrupado por scopus_id) ======\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "IN_PARQUET  = os.environ.get(\"IN_PARQUET\", \"corpus_token_nostop_lemma.parquet\")\n",
    "OUT_CHUNKS  = os.environ.get(\"OUT_CHUNKS\", \"corpus_chunks.parquet\")\n",
    "\n",
    "# --- Parámetros de chunking ---\n",
    "MAX_TOKENS      = int(os.environ.get(\"MAX_TOKENS\", \"300\"))      # 200–400 recomendado\n",
    "OVERLAP_RATIO   = float(os.environ.get(\"OVERLAP_RATIO\", \"0.2\"))  # 15–30% recomendado\n",
    "OVERLAP_TOKENS  = int(MAX_TOKENS * OVERLAP_RATIO)\n",
    "STRIDE          = max(1, MAX_TOKENS - OVERLAP_TOKENS)\n",
    "\n",
    "# Tokenizer E5 (coherente con embeddings e5*)\n",
    "TOKENIZER_NAME = \"intfloat/multilingual-e5-base\"\n",
    "tok = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "# ---------- 0) Carga y validación ----------\n",
    "df = pd.read_parquet(IN_PARQUET)\n",
    "\n",
    "assert \"scopus_id\" in df.columns, \"Falta columna 'scopus_id' en el parquet.\"\n",
    "assert \"text_lemma\" in df.columns, \"Falta columna 'text_lemma' (se usa siempre).\"\n",
    "\n",
    "# Normaliza tipos / limpieza básica\n",
    "df[\"scopus_id\"]  = df[\"scopus_id\"].astype(str)\n",
    "df[\"text_lemma\"] = (\n",
    "    df[\"text_lemma\"].fillna(\"\").astype(str)\n",
    "      .str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    ")\n",
    "\n",
    "# ---------- 1) Construir texto lematizado por documento ----------\n",
    "# Orden preferido: sentence_idx > row_id_original > orden actual\n",
    "sort_keys = [\"scopus_id\"]\n",
    "if \"sentence_idx\" in df.columns:\n",
    "    sort_keys += [\"sentence_idx\"]\n",
    "elif \"row_id_original\" in df.columns:\n",
    "    sort_keys += [\"row_id_original\"]\n",
    "\n",
    "df = df.sort_values(sort_keys, kind=\"mergesort\")\n",
    "\n",
    "# Texto lematizado consolidado por scopus_id\n",
    "agg_text = df.groupby(\"scopus_id\")[\"text_lemma\"].apply(\n",
    "    lambda s: \" \".join([t for t in s.astype(str) if t])\n",
    ").rename(\"text_for_chunk\")\n",
    "\n",
    "# Solo scopus_id + texto consolidado\n",
    "doc_df = agg_text.to_frame().reset_index()\n",
    "doc_df[\"doc_id\"] = np.arange(len(doc_df), dtype=\"int64\")\n",
    "\n",
    "# Limpieza final del texto\n",
    "doc_df[\"text_for_chunk\"] = (\n",
    "    doc_df[\"text_for_chunk\"].fillna(\"\").astype(str)\n",
    "       .str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    ")\n",
    "doc_df = doc_df[doc_df[\"text_for_chunk\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# ---------- 2) Chunker por tokens ----------\n",
    "def chunk_text_by_tokens(text: str, max_tokens: int = MAX_TOKENS, stride: int = STRIDE):\n",
    "    ids = tok.encode(text, add_special_tokens=False)\n",
    "    n = len(ids)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = min(start + max_tokens, n)\n",
    "        sl = ids[start:end]\n",
    "        chunk_txt = tok.decode(sl, skip_special_tokens=True).strip()\n",
    "        if chunk_txt:\n",
    "            chunks.append({\n",
    "                \"start_token\": start,\n",
    "                \"end_token\": end,\n",
    "                \"token_count\": end - start,\n",
    "                \"text_chunk\": chunk_txt\n",
    "            })\n",
    "        if end == n:\n",
    "            break\n",
    "        start += stride\n",
    "    return chunks\n",
    "\n",
    "# ---------- 3) Generar filas de chunks (solo campos mínimos + scopus_id) ----------\n",
    "rows = []\n",
    "for _, r in doc_df.iterrows():\n",
    "    doc_id = int(r[\"doc_id\"])\n",
    "    scid   = str(r[\"scopus_id\"])\n",
    "    text   = r[\"text_for_chunk\"]\n",
    "    for j, ch in enumerate(chunk_text_by_tokens(text, MAX_TOKENS, STRIDE)):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": j,\n",
    "            \"chunk_uid\": f\"{doc_id}-{j}\",\n",
    "            \"scopus_id\": scid,\n",
    "            \"start_token\": ch[\"start_token\"],\n",
    "            \"end_token\": ch[\"end_token\"],\n",
    "            \"token_count\": ch[\"token_count\"],\n",
    "            \"text_chunk\": ch[\"text_chunk\"],\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(rows)\n",
    "\n",
    "print(chunks_df.head(8))\n",
    "print(\"N docs (únicos scopus_id):\", doc_df.shape[0], \"| N chunks:\", chunks_df.shape[0])\n",
    "\n",
    "# ---------- 4) Guardar (solo mínimos) ----------\n",
    "save_cols = [\n",
    "    \"doc_id\",\"chunk_id\",\"chunk_uid\",\"scopus_id\",\n",
    "    \"start_token\",\"end_token\",\"token_count\",\"text_chunk\"\n",
    "]\n",
    "chunks_out = chunks_df[save_cols].copy()\n",
    "\n",
    "# Parquet (pyarrow preferente)\n",
    "try:\n",
    "    chunks_out.to_parquet(OUT_CHUNKS, index=False, engine=\"pyarrow\")\n",
    "except Exception:\n",
    "    try:\n",
    "        chunks_out.to_parquet(OUT_CHUNKS, index=False, engine=\"fastparquet\", compression=\"gzip\")\n",
    "    except Exception:\n",
    "        chunks_out.to_csv(OUT_CHUNKS.replace(\".parquet\", \".csv\"), index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Guardado en:\", OUT_CHUNKS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afbb74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device encode: cpu; model=intfloat/multilingual-e5-small\n",
      "[INFO] model.max_seq_length = 300\n",
      "[INFO] FAISS-CPU\n",
      "[PROG] 32/21005 (0.2%) - bs=32\n",
      "[PROG] 64/21005 (0.3%) - bs=32\n",
      "[PROG] 96/21005 (0.5%) - bs=32\n",
      "[PROG] 128/21005 (0.6%) - bs=32\n",
      "[PROG] 160/21005 (0.8%) - bs=32\n",
      "[PROG] 192/21005 (0.9%) - bs=32\n",
      "[PROG] 224/21005 (1.1%) - bs=32\n",
      "[PROG] 256/21005 (1.2%) - bs=32\n",
      "[PROG] 288/21005 (1.4%) - bs=32\n",
      "[PROG] 320/21005 (1.5%) - bs=32\n",
      "[PROG] 352/21005 (1.7%) - bs=32\n",
      "[PROG] 384/21005 (1.8%) - bs=32\n",
      "[PROG] 416/21005 (2.0%) - bs=32\n",
      "[PROG] 448/21005 (2.1%) - bs=32\n",
      "[PROG] 480/21005 (2.3%) - bs=32\n",
      "[PROG] 512/21005 (2.4%) - bs=32\n",
      "[PROG] 544/21005 (2.6%) - bs=32\n",
      "[PROG] 576/21005 (2.7%) - bs=32\n",
      "[PROG] 608/21005 (2.9%) - bs=32\n",
      "[PROG] 640/21005 (3.0%) - bs=32\n",
      "[PROG] 672/21005 (3.2%) - bs=32\n",
      "[PROG] 704/21005 (3.4%) - bs=32\n",
      "[PROG] 736/21005 (3.5%) - bs=32\n",
      "[PROG] 768/21005 (3.7%) - bs=32\n",
      "[PROG] 800/21005 (3.8%) - bs=32\n",
      "[PROG] 832/21005 (4.0%) - bs=32\n",
      "[PROG] 864/21005 (4.1%) - bs=32\n",
      "[PROG] 896/21005 (4.3%) - bs=32\n",
      "[PROG] 928/21005 (4.4%) - bs=32\n",
      "[PROG] 960/21005 (4.6%) - bs=32\n",
      "[PROG] 992/21005 (4.7%) - bs=32\n",
      "[PROG] 1024/21005 (4.9%) - bs=32\n",
      "[PROG] 1056/21005 (5.0%) - bs=32\n",
      "[PROG] 1088/21005 (5.2%) - bs=32\n",
      "[PROG] 1120/21005 (5.3%) - bs=32\n",
      "[PROG] 1152/21005 (5.5%) - bs=32\n",
      "[PROG] 1184/21005 (5.6%) - bs=32\n",
      "[PROG] 1216/21005 (5.8%) - bs=32\n",
      "[PROG] 1248/21005 (5.9%) - bs=32\n",
      "[PROG] 1280/21005 (6.1%) - bs=32\n",
      "[PROG] 1312/21005 (6.2%) - bs=32\n",
      "[PROG] 1344/21005 (6.4%) - bs=32\n",
      "[PROG] 1376/21005 (6.6%) - bs=32\n",
      "[PROG] 1408/21005 (6.7%) - bs=32\n",
      "[PROG] 1440/21005 (6.9%) - bs=32\n",
      "[PROG] 1472/21005 (7.0%) - bs=32\n",
      "[PROG] 1504/21005 (7.2%) - bs=32\n",
      "[PROG] 1536/21005 (7.3%) - bs=32\n",
      "[PROG] 1568/21005 (7.5%) - bs=32\n",
      "[PROG] 1600/21005 (7.6%) - bs=32\n",
      "[PROG] 1632/21005 (7.8%) - bs=32\n",
      "[PROG] 1664/21005 (7.9%) - bs=32\n",
      "[PROG] 1696/21005 (8.1%) - bs=32\n",
      "[PROG] 1728/21005 (8.2%) - bs=32\n",
      "[PROG] 1760/21005 (8.4%) - bs=32\n",
      "[PROG] 1792/21005 (8.5%) - bs=32\n",
      "[PROG] 1824/21005 (8.7%) - bs=32\n",
      "[PROG] 1856/21005 (8.8%) - bs=32\n",
      "[PROG] 1888/21005 (9.0%) - bs=32\n",
      "[PROG] 1920/21005 (9.1%) - bs=32\n",
      "[PROG] 1952/21005 (9.3%) - bs=32\n",
      "[PROG] 1984/21005 (9.4%) - bs=32\n",
      "[PROG] 2016/21005 (9.6%) - bs=32\n",
      "[PROG] 2048/21005 (9.8%) - bs=32\n",
      "[PROG] 2080/21005 (9.9%) - bs=32\n",
      "[PROG] 2112/21005 (10.1%) - bs=32\n",
      "[PROG] 2144/21005 (10.2%) - bs=32\n",
      "[PROG] 2176/21005 (10.4%) - bs=32\n",
      "[PROG] 2208/21005 (10.5%) - bs=32\n",
      "[PROG] 2240/21005 (10.7%) - bs=32\n",
      "[PROG] 2272/21005 (10.8%) - bs=32\n",
      "[PROG] 2304/21005 (11.0%) - bs=32\n",
      "[PROG] 2336/21005 (11.1%) - bs=32\n",
      "[PROG] 2368/21005 (11.3%) - bs=32\n",
      "[PROG] 2400/21005 (11.4%) - bs=32\n",
      "[PROG] 2432/21005 (11.6%) - bs=32\n",
      "[PROG] 2464/21005 (11.7%) - bs=32\n",
      "[PROG] 2496/21005 (11.9%) - bs=32\n",
      "[PROG] 2528/21005 (12.0%) - bs=32\n",
      "[PROG] 2560/21005 (12.2%) - bs=32\n",
      "[PROG] 2592/21005 (12.3%) - bs=32\n",
      "[PROG] 2624/21005 (12.5%) - bs=32\n",
      "[PROG] 2656/21005 (12.6%) - bs=32\n",
      "[PROG] 2688/21005 (12.8%) - bs=32\n",
      "[PROG] 2720/21005 (12.9%) - bs=32\n",
      "[PROG] 2752/21005 (13.1%) - bs=32\n",
      "[PROG] 2784/21005 (13.3%) - bs=32\n",
      "[PROG] 2816/21005 (13.4%) - bs=32\n",
      "[PROG] 2848/21005 (13.6%) - bs=32\n",
      "[PROG] 2880/21005 (13.7%) - bs=32\n",
      "[PROG] 2912/21005 (13.9%) - bs=32\n",
      "[PROG] 2944/21005 (14.0%) - bs=32\n",
      "[PROG] 2976/21005 (14.2%) - bs=32\n",
      "[PROG] 3008/21005 (14.3%) - bs=32\n",
      "[PROG] 3040/21005 (14.5%) - bs=32\n",
      "[PROG] 3072/21005 (14.6%) - bs=32\n",
      "[PROG] 3104/21005 (14.8%) - bs=32\n",
      "[PROG] 3136/21005 (14.9%) - bs=32\n",
      "[PROG] 3168/21005 (15.1%) - bs=32\n",
      "[PROG] 3200/21005 (15.2%) - bs=32\n",
      "[PROG] 3232/21005 (15.4%) - bs=32\n",
      "[PROG] 3264/21005 (15.5%) - bs=32\n",
      "[PROG] 3296/21005 (15.7%) - bs=32\n",
      "[PROG] 3328/21005 (15.8%) - bs=32\n",
      "[PROG] 3360/21005 (16.0%) - bs=32\n",
      "[PROG] 3392/21005 (16.1%) - bs=32\n",
      "[PROG] 3424/21005 (16.3%) - bs=32\n",
      "[PROG] 3456/21005 (16.5%) - bs=32\n",
      "[PROG] 3488/21005 (16.6%) - bs=32\n",
      "[PROG] 3520/21005 (16.8%) - bs=32\n",
      "[PROG] 3552/21005 (16.9%) - bs=32\n",
      "[PROG] 3584/21005 (17.1%) - bs=32\n",
      "[PROG] 3616/21005 (17.2%) - bs=32\n",
      "[PROG] 3648/21005 (17.4%) - bs=32\n",
      "[PROG] 3680/21005 (17.5%) - bs=32\n",
      "[PROG] 3712/21005 (17.7%) - bs=32\n",
      "[PROG] 3744/21005 (17.8%) - bs=32\n",
      "[PROG] 3776/21005 (18.0%) - bs=32\n",
      "[PROG] 3808/21005 (18.1%) - bs=32\n",
      "[PROG] 3840/21005 (18.3%) - bs=32\n",
      "[PROG] 3872/21005 (18.4%) - bs=32\n",
      "[PROG] 3904/21005 (18.6%) - bs=32\n",
      "[PROG] 3936/21005 (18.7%) - bs=32\n",
      "[PROG] 3968/21005 (18.9%) - bs=32\n",
      "[PROG] 4000/21005 (19.0%) - bs=32\n",
      "[PROG] 4032/21005 (19.2%) - bs=32\n",
      "[PROG] 4064/21005 (19.3%) - bs=32\n",
      "[PROG] 4096/21005 (19.5%) - bs=32\n",
      "[PROG] 4128/21005 (19.7%) - bs=32\n",
      "[PROG] 4160/21005 (19.8%) - bs=32\n",
      "[PROG] 4192/21005 (20.0%) - bs=32\n",
      "[PROG] 4224/21005 (20.1%) - bs=32\n",
      "[PROG] 4256/21005 (20.3%) - bs=32\n",
      "[PROG] 4288/21005 (20.4%) - bs=32\n",
      "[PROG] 4320/21005 (20.6%) - bs=32\n",
      "[PROG] 4352/21005 (20.7%) - bs=32\n",
      "[PROG] 4384/21005 (20.9%) - bs=32\n",
      "[PROG] 4416/21005 (21.0%) - bs=32\n",
      "[PROG] 4448/21005 (21.2%) - bs=32\n",
      "[PROG] 4480/21005 (21.3%) - bs=32\n",
      "[PROG] 4512/21005 (21.5%) - bs=32\n",
      "[PROG] 4544/21005 (21.6%) - bs=32\n",
      "[PROG] 4576/21005 (21.8%) - bs=32\n",
      "[PROG] 4608/21005 (21.9%) - bs=32\n",
      "[PROG] 4640/21005 (22.1%) - bs=32\n",
      "[PROG] 4672/21005 (22.2%) - bs=32\n",
      "[PROG] 4704/21005 (22.4%) - bs=32\n",
      "[PROG] 4736/21005 (22.5%) - bs=32\n",
      "[PROG] 4768/21005 (22.7%) - bs=32\n",
      "[PROG] 4800/21005 (22.9%) - bs=32\n",
      "[PROG] 4832/21005 (23.0%) - bs=32\n",
      "[PROG] 4864/21005 (23.2%) - bs=32\n",
      "[PROG] 4896/21005 (23.3%) - bs=32\n",
      "[PROG] 4928/21005 (23.5%) - bs=32\n",
      "[PROG] 4960/21005 (23.6%) - bs=32\n",
      "[PROG] 4992/21005 (23.8%) - bs=32\n",
      "[PROG] 5024/21005 (23.9%) - bs=32\n",
      "[PROG] 5056/21005 (24.1%) - bs=32\n",
      "[PROG] 5088/21005 (24.2%) - bs=32\n",
      "[PROG] 5120/21005 (24.4%) - bs=32\n",
      "[PROG] 5152/21005 (24.5%) - bs=32\n",
      "[PROG] 5184/21005 (24.7%) - bs=32\n",
      "[PROG] 5216/21005 (24.8%) - bs=32\n",
      "[PROG] 5248/21005 (25.0%) - bs=32\n",
      "[PROG] 5280/21005 (25.1%) - bs=32\n",
      "[PROG] 5312/21005 (25.3%) - bs=32\n",
      "[PROG] 5344/21005 (25.4%) - bs=32\n",
      "[PROG] 5376/21005 (25.6%) - bs=32\n",
      "[PROG] 5408/21005 (25.7%) - bs=32\n",
      "[PROG] 5440/21005 (25.9%) - bs=32\n",
      "[PROG] 5472/21005 (26.1%) - bs=32\n",
      "[PROG] 5504/21005 (26.2%) - bs=32\n",
      "[PROG] 5536/21005 (26.4%) - bs=32\n",
      "[PROG] 5568/21005 (26.5%) - bs=32\n",
      "[PROG] 5600/21005 (26.7%) - bs=32\n",
      "[PROG] 5632/21005 (26.8%) - bs=32\n",
      "[PROG] 5664/21005 (27.0%) - bs=32\n",
      "[PROG] 5696/21005 (27.1%) - bs=32\n",
      "[PROG] 5728/21005 (27.3%) - bs=32\n",
      "[PROG] 5760/21005 (27.4%) - bs=32\n",
      "[PROG] 5792/21005 (27.6%) - bs=32\n",
      "[PROG] 5824/21005 (27.7%) - bs=32\n",
      "[PROG] 5856/21005 (27.9%) - bs=32\n",
      "[PROG] 5888/21005 (28.0%) - bs=32\n",
      "[PROG] 5920/21005 (28.2%) - bs=32\n",
      "[PROG] 5952/21005 (28.3%) - bs=32\n",
      "[PROG] 5984/21005 (28.5%) - bs=32\n",
      "[PROG] 6016/21005 (28.6%) - bs=32\n",
      "[PROG] 6048/21005 (28.8%) - bs=32\n",
      "[PROG] 6080/21005 (28.9%) - bs=32\n",
      "[PROG] 6112/21005 (29.1%) - bs=32\n",
      "[PROG] 6144/21005 (29.3%) - bs=32\n",
      "[PROG] 6176/21005 (29.4%) - bs=32\n",
      "[PROG] 6208/21005 (29.6%) - bs=32\n",
      "[PROG] 6240/21005 (29.7%) - bs=32\n",
      "[PROG] 6272/21005 (29.9%) - bs=32\n",
      "[PROG] 6304/21005 (30.0%) - bs=32\n",
      "[PROG] 6336/21005 (30.2%) - bs=32\n",
      "[PROG] 6368/21005 (30.3%) - bs=32\n",
      "[PROG] 6400/21005 (30.5%) - bs=32\n",
      "[PROG] 6432/21005 (30.6%) - bs=32\n",
      "[PROG] 6464/21005 (30.8%) - bs=32\n",
      "[PROG] 6496/21005 (30.9%) - bs=32\n",
      "[PROG] 6528/21005 (31.1%) - bs=32\n",
      "[PROG] 6560/21005 (31.2%) - bs=32\n",
      "[PROG] 6592/21005 (31.4%) - bs=32\n",
      "[PROG] 6624/21005 (31.5%) - bs=32\n",
      "[PROG] 6656/21005 (31.7%) - bs=32\n",
      "[PROG] 6688/21005 (31.8%) - bs=32\n",
      "[PROG] 6720/21005 (32.0%) - bs=32\n",
      "[PROG] 6752/21005 (32.1%) - bs=32\n",
      "[PROG] 6784/21005 (32.3%) - bs=32\n",
      "[PROG] 6816/21005 (32.4%) - bs=32\n",
      "[PROG] 6848/21005 (32.6%) - bs=32\n",
      "[PROG] 6880/21005 (32.8%) - bs=32\n",
      "[PROG] 6912/21005 (32.9%) - bs=32\n",
      "[PROG] 6944/21005 (33.1%) - bs=32\n",
      "[PROG] 6976/21005 (33.2%) - bs=32\n",
      "[PROG] 7008/21005 (33.4%) - bs=32\n",
      "[PROG] 7040/21005 (33.5%) - bs=32\n",
      "[PROG] 7072/21005 (33.7%) - bs=32\n",
      "[PROG] 7104/21005 (33.8%) - bs=32\n",
      "[PROG] 7136/21005 (34.0%) - bs=32\n",
      "[PROG] 7168/21005 (34.1%) - bs=32\n",
      "[PROG] 7200/21005 (34.3%) - bs=32\n",
      "[PROG] 7232/21005 (34.4%) - bs=32\n",
      "[PROG] 7264/21005 (34.6%) - bs=32\n",
      "[PROG] 7296/21005 (34.7%) - bs=32\n",
      "[PROG] 7328/21005 (34.9%) - bs=32\n",
      "[PROG] 7360/21005 (35.0%) - bs=32\n",
      "[PROG] 7392/21005 (35.2%) - bs=32\n",
      "[PROG] 7424/21005 (35.3%) - bs=32\n",
      "[PROG] 7456/21005 (35.5%) - bs=32\n",
      "[PROG] 7488/21005 (35.6%) - bs=32\n",
      "[PROG] 7520/21005 (35.8%) - bs=32\n",
      "[PROG] 7552/21005 (36.0%) - bs=32\n",
      "[PROG] 7584/21005 (36.1%) - bs=32\n",
      "[PROG] 7616/21005 (36.3%) - bs=32\n",
      "[PROG] 7648/21005 (36.4%) - bs=32\n",
      "[PROG] 7680/21005 (36.6%) - bs=32\n",
      "[PROG] 7712/21005 (36.7%) - bs=32\n",
      "[PROG] 7744/21005 (36.9%) - bs=32\n",
      "[PROG] 7776/21005 (37.0%) - bs=32\n",
      "[PROG] 7808/21005 (37.2%) - bs=32\n",
      "[PROG] 7840/21005 (37.3%) - bs=32\n",
      "[PROG] 7872/21005 (37.5%) - bs=32\n",
      "[PROG] 7904/21005 (37.6%) - bs=32\n",
      "[PROG] 7936/21005 (37.8%) - bs=32\n",
      "[PROG] 7968/21005 (37.9%) - bs=32\n",
      "[PROG] 8000/21005 (38.1%) - bs=32\n",
      "[PROG] 8032/21005 (38.2%) - bs=32\n",
      "[PROG] 8064/21005 (38.4%) - bs=32\n",
      "[PROG] 8096/21005 (38.5%) - bs=32\n",
      "[PROG] 8128/21005 (38.7%) - bs=32\n",
      "[PROG] 8160/21005 (38.8%) - bs=32\n",
      "[PROG] 8192/21005 (39.0%) - bs=32\n",
      "[PROG] 8224/21005 (39.2%) - bs=32\n",
      "[PROG] 8256/21005 (39.3%) - bs=32\n",
      "[PROG] 8288/21005 (39.5%) - bs=32\n",
      "[PROG] 8320/21005 (39.6%) - bs=32\n",
      "[PROG] 8352/21005 (39.8%) - bs=32\n",
      "[PROG] 8384/21005 (39.9%) - bs=32\n",
      "[PROG] 8416/21005 (40.1%) - bs=32\n",
      "[PROG] 8448/21005 (40.2%) - bs=32\n",
      "[PROG] 8480/21005 (40.4%) - bs=32\n",
      "[PROG] 8512/21005 (40.5%) - bs=32\n",
      "[PROG] 8544/21005 (40.7%) - bs=32\n",
      "[PROG] 8576/21005 (40.8%) - bs=32\n",
      "[PROG] 8608/21005 (41.0%) - bs=32\n",
      "[PROG] 8640/21005 (41.1%) - bs=32\n",
      "[PROG] 8672/21005 (41.3%) - bs=32\n",
      "[PROG] 8704/21005 (41.4%) - bs=32\n",
      "[PROG] 8736/21005 (41.6%) - bs=32\n",
      "[PROG] 8768/21005 (41.7%) - bs=32\n",
      "[PROG] 8800/21005 (41.9%) - bs=32\n",
      "[PROG] 8832/21005 (42.0%) - bs=32\n",
      "[PROG] 8864/21005 (42.2%) - bs=32\n",
      "[PROG] 8896/21005 (42.4%) - bs=32\n",
      "[PROG] 8928/21005 (42.5%) - bs=32\n",
      "[PROG] 8960/21005 (42.7%) - bs=32\n",
      "[PROG] 8992/21005 (42.8%) - bs=32\n",
      "[PROG] 9024/21005 (43.0%) - bs=32\n",
      "[PROG] 9056/21005 (43.1%) - bs=32\n",
      "[PROG] 9088/21005 (43.3%) - bs=32\n",
      "[PROG] 9120/21005 (43.4%) - bs=32\n",
      "[PROG] 9152/21005 (43.6%) - bs=32\n",
      "[PROG] 9184/21005 (43.7%) - bs=32\n",
      "[PROG] 9216/21005 (43.9%) - bs=32\n",
      "[PROG] 9248/21005 (44.0%) - bs=32\n",
      "[PROG] 9280/21005 (44.2%) - bs=32\n",
      "[PROG] 9312/21005 (44.3%) - bs=32\n",
      "[PROG] 9344/21005 (44.5%) - bs=32\n",
      "[PROG] 9376/21005 (44.6%) - bs=32\n",
      "[PROG] 9408/21005 (44.8%) - bs=32\n",
      "[PROG] 9440/21005 (44.9%) - bs=32\n",
      "[PROG] 9472/21005 (45.1%) - bs=32\n",
      "[PROG] 9504/21005 (45.2%) - bs=32\n",
      "[PROG] 9536/21005 (45.4%) - bs=32\n",
      "[PROG] 9568/21005 (45.6%) - bs=32\n",
      "[PROG] 9600/21005 (45.7%) - bs=32\n",
      "[PROG] 9632/21005 (45.9%) - bs=32\n",
      "[PROG] 9664/21005 (46.0%) - bs=32\n",
      "[PROG] 9696/21005 (46.2%) - bs=32\n",
      "[PROG] 9728/21005 (46.3%) - bs=32\n",
      "[PROG] 9760/21005 (46.5%) - bs=32\n",
      "[PROG] 9792/21005 (46.6%) - bs=32\n",
      "[PROG] 9824/21005 (46.8%) - bs=32\n",
      "[PROG] 9856/21005 (46.9%) - bs=32\n",
      "[PROG] 9888/21005 (47.1%) - bs=32\n",
      "[PROG] 9920/21005 (47.2%) - bs=32\n",
      "[PROG] 9952/21005 (47.4%) - bs=32\n",
      "[PROG] 9984/21005 (47.5%) - bs=32\n",
      "[PROG] 10016/21005 (47.7%) - bs=32\n",
      "[PROG] 10048/21005 (47.8%) - bs=32\n",
      "[PROG] 10080/21005 (48.0%) - bs=32\n",
      "[PROG] 10112/21005 (48.1%) - bs=32\n",
      "[PROG] 10144/21005 (48.3%) - bs=32\n",
      "[PROG] 10176/21005 (48.4%) - bs=32\n",
      "[PROG] 10208/21005 (48.6%) - bs=32\n",
      "[PROG] 10240/21005 (48.8%) - bs=32\n",
      "[PROG] 10272/21005 (48.9%) - bs=32\n",
      "[PROG] 10304/21005 (49.1%) - bs=32\n",
      "[PROG] 10336/21005 (49.2%) - bs=32\n",
      "[PROG] 10368/21005 (49.4%) - bs=32\n",
      "[PROG] 10400/21005 (49.5%) - bs=32\n",
      "[PROG] 10432/21005 (49.7%) - bs=32\n",
      "[PROG] 10464/21005 (49.8%) - bs=32\n",
      "[PROG] 10496/21005 (50.0%) - bs=32\n",
      "[PROG] 10528/21005 (50.1%) - bs=32\n",
      "[PROG] 10560/21005 (50.3%) - bs=32\n",
      "[PROG] 10592/21005 (50.4%) - bs=32\n",
      "[PROG] 10624/21005 (50.6%) - bs=32\n",
      "[PROG] 10656/21005 (50.7%) - bs=32\n",
      "[PROG] 10688/21005 (50.9%) - bs=32\n",
      "[PROG] 10720/21005 (51.0%) - bs=32\n",
      "[PROG] 10752/21005 (51.2%) - bs=32\n",
      "[PROG] 10784/21005 (51.3%) - bs=32\n",
      "[PROG] 10816/21005 (51.5%) - bs=32\n",
      "[PROG] 10848/21005 (51.6%) - bs=32\n",
      "[PROG] 10880/21005 (51.8%) - bs=32\n",
      "[PROG] 10912/21005 (51.9%) - bs=32\n",
      "[PROG] 10944/21005 (52.1%) - bs=32\n",
      "[PROG] 10976/21005 (52.3%) - bs=32\n",
      "[PROG] 11008/21005 (52.4%) - bs=32\n",
      "[PROG] 11040/21005 (52.6%) - bs=32\n",
      "[PROG] 11072/21005 (52.7%) - bs=32\n",
      "[PROG] 11104/21005 (52.9%) - bs=32\n",
      "[PROG] 11136/21005 (53.0%) - bs=32\n",
      "[PROG] 11168/21005 (53.2%) - bs=32\n",
      "[PROG] 11200/21005 (53.3%) - bs=32\n",
      "[PROG] 11232/21005 (53.5%) - bs=32\n",
      "[PROG] 11264/21005 (53.6%) - bs=32\n",
      "[PROG] 11296/21005 (53.8%) - bs=32\n",
      "[PROG] 11328/21005 (53.9%) - bs=32\n",
      "[PROG] 11360/21005 (54.1%) - bs=32\n",
      "[PROG] 11392/21005 (54.2%) - bs=32\n",
      "[PROG] 11424/21005 (54.4%) - bs=32\n",
      "[PROG] 11456/21005 (54.5%) - bs=32\n",
      "[PROG] 11488/21005 (54.7%) - bs=32\n",
      "[PROG] 11520/21005 (54.8%) - bs=32\n",
      "[PROG] 11552/21005 (55.0%) - bs=32\n",
      "[PROG] 11584/21005 (55.1%) - bs=32\n",
      "[PROG] 11616/21005 (55.3%) - bs=32\n",
      "[PROG] 11648/21005 (55.5%) - bs=32\n",
      "[PROG] 11680/21005 (55.6%) - bs=32\n",
      "[PROG] 11712/21005 (55.8%) - bs=32\n",
      "[PROG] 11744/21005 (55.9%) - bs=32\n",
      "[PROG] 11776/21005 (56.1%) - bs=32\n",
      "[PROG] 11808/21005 (56.2%) - bs=32\n",
      "[PROG] 11840/21005 (56.4%) - bs=32\n",
      "[PROG] 11872/21005 (56.5%) - bs=32\n",
      "[PROG] 11904/21005 (56.7%) - bs=32\n",
      "[PROG] 11936/21005 (56.8%) - bs=32\n",
      "[PROG] 11968/21005 (57.0%) - bs=32\n",
      "[PROG] 12000/21005 (57.1%) - bs=32\n",
      "[PROG] 12032/21005 (57.3%) - bs=32\n",
      "[PROG] 12064/21005 (57.4%) - bs=32\n",
      "[PROG] 12096/21005 (57.6%) - bs=32\n",
      "[PROG] 12128/21005 (57.7%) - bs=32\n",
      "[PROG] 12160/21005 (57.9%) - bs=32\n",
      "[PROG] 12192/21005 (58.0%) - bs=32\n",
      "[PROG] 12224/21005 (58.2%) - bs=32\n",
      "[PROG] 12256/21005 (58.3%) - bs=32\n",
      "[PROG] 12288/21005 (58.5%) - bs=32\n",
      "[PROG] 12320/21005 (58.7%) - bs=32\n",
      "[PROG] 12352/21005 (58.8%) - bs=32\n",
      "[PROG] 12384/21005 (59.0%) - bs=32\n",
      "[PROG] 12416/21005 (59.1%) - bs=32\n",
      "[PROG] 12448/21005 (59.3%) - bs=32\n",
      "[PROG] 12480/21005 (59.4%) - bs=32\n",
      "[PROG] 12512/21005 (59.6%) - bs=32\n",
      "[PROG] 12544/21005 (59.7%) - bs=32\n",
      "[PROG] 12576/21005 (59.9%) - bs=32\n",
      "[PROG] 12608/21005 (60.0%) - bs=32\n",
      "[PROG] 12640/21005 (60.2%) - bs=32\n",
      "[PROG] 12672/21005 (60.3%) - bs=32\n",
      "[PROG] 12704/21005 (60.5%) - bs=32\n",
      "[PROG] 12736/21005 (60.6%) - bs=32\n",
      "[PROG] 12768/21005 (60.8%) - bs=32\n",
      "[PROG] 12800/21005 (60.9%) - bs=32\n",
      "[PROG] 12832/21005 (61.1%) - bs=32\n",
      "[PROG] 12864/21005 (61.2%) - bs=32\n",
      "[PROG] 12896/21005 (61.4%) - bs=32\n",
      "[PROG] 12928/21005 (61.5%) - bs=32\n",
      "[PROG] 12960/21005 (61.7%) - bs=32\n",
      "[PROG] 12992/21005 (61.9%) - bs=32\n",
      "[PROG] 13024/21005 (62.0%) - bs=32\n",
      "[PROG] 13056/21005 (62.2%) - bs=32\n",
      "[PROG] 13088/21005 (62.3%) - bs=32\n",
      "[PROG] 13120/21005 (62.5%) - bs=32\n",
      "[PROG] 13152/21005 (62.6%) - bs=32\n",
      "[PROG] 13184/21005 (62.8%) - bs=32\n",
      "[PROG] 13216/21005 (62.9%) - bs=32\n",
      "[PROG] 13248/21005 (63.1%) - bs=32\n",
      "[PROG] 13280/21005 (63.2%) - bs=32\n",
      "[PROG] 13312/21005 (63.4%) - bs=32\n",
      "[PROG] 13344/21005 (63.5%) - bs=32\n",
      "[PROG] 13376/21005 (63.7%) - bs=32\n",
      "[PROG] 13408/21005 (63.8%) - bs=32\n",
      "[PROG] 13440/21005 (64.0%) - bs=32\n",
      "[PROG] 13472/21005 (64.1%) - bs=32\n",
      "[PROG] 13504/21005 (64.3%) - bs=32\n",
      "[PROG] 13536/21005 (64.4%) - bs=32\n",
      "[PROG] 13568/21005 (64.6%) - bs=32\n",
      "[PROG] 13600/21005 (64.7%) - bs=32\n",
      "[PROG] 13632/21005 (64.9%) - bs=32\n",
      "[PROG] 13664/21005 (65.1%) - bs=32\n",
      "[PROG] 13696/21005 (65.2%) - bs=32\n",
      "[PROG] 13728/21005 (65.4%) - bs=32\n",
      "[PROG] 13760/21005 (65.5%) - bs=32\n",
      "[PROG] 13792/21005 (65.7%) - bs=32\n",
      "[PROG] 13824/21005 (65.8%) - bs=32\n",
      "[PROG] 13856/21005 (66.0%) - bs=32\n",
      "[PROG] 13888/21005 (66.1%) - bs=32\n",
      "[PROG] 13920/21005 (66.3%) - bs=32\n",
      "[PROG] 13952/21005 (66.4%) - bs=32\n",
      "[PROG] 13984/21005 (66.6%) - bs=32\n",
      "[PROG] 14016/21005 (66.7%) - bs=32\n",
      "[PROG] 14048/21005 (66.9%) - bs=32\n",
      "[PROG] 14080/21005 (67.0%) - bs=32\n",
      "[PROG] 14112/21005 (67.2%) - bs=32\n",
      "[PROG] 14144/21005 (67.3%) - bs=32\n",
      "[PROG] 14176/21005 (67.5%) - bs=32\n",
      "[PROG] 14208/21005 (67.6%) - bs=32\n",
      "[PROG] 14240/21005 (67.8%) - bs=32\n",
      "[PROG] 14272/21005 (67.9%) - bs=32\n",
      "[PROG] 14304/21005 (68.1%) - bs=32\n",
      "[PROG] 14336/21005 (68.3%) - bs=32\n",
      "[PROG] 14368/21005 (68.4%) - bs=32\n",
      "[PROG] 14400/21005 (68.6%) - bs=32\n",
      "[PROG] 14432/21005 (68.7%) - bs=32\n",
      "[PROG] 14464/21005 (68.9%) - bs=32\n",
      "[PROG] 14496/21005 (69.0%) - bs=32\n",
      "[PROG] 14528/21005 (69.2%) - bs=32\n",
      "[PROG] 14560/21005 (69.3%) - bs=32\n",
      "[PROG] 14592/21005 (69.5%) - bs=32\n",
      "[PROG] 14624/21005 (69.6%) - bs=32\n",
      "[PROG] 14656/21005 (69.8%) - bs=32\n",
      "[PROG] 14688/21005 (69.9%) - bs=32\n",
      "[PROG] 14720/21005 (70.1%) - bs=32\n",
      "[PROG] 14752/21005 (70.2%) - bs=32\n",
      "[PROG] 14784/21005 (70.4%) - bs=32\n",
      "[PROG] 14816/21005 (70.5%) - bs=32\n",
      "[PROG] 14848/21005 (70.7%) - bs=32\n",
      "[PROG] 14880/21005 (70.8%) - bs=32\n",
      "[PROG] 14912/21005 (71.0%) - bs=32\n",
      "[PROG] 14944/21005 (71.1%) - bs=32\n",
      "[PROG] 14976/21005 (71.3%) - bs=32\n",
      "[PROG] 15008/21005 (71.4%) - bs=32\n",
      "[PROG] 15040/21005 (71.6%) - bs=32\n",
      "[PROG] 15072/21005 (71.8%) - bs=32\n",
      "[PROG] 15104/21005 (71.9%) - bs=32\n",
      "[PROG] 15136/21005 (72.1%) - bs=32\n",
      "[PROG] 15168/21005 (72.2%) - bs=32\n",
      "[PROG] 15200/21005 (72.4%) - bs=32\n",
      "[PROG] 15232/21005 (72.5%) - bs=32\n",
      "[PROG] 15264/21005 (72.7%) - bs=32\n",
      "[PROG] 15296/21005 (72.8%) - bs=32\n",
      "[PROG] 15328/21005 (73.0%) - bs=32\n",
      "[PROG] 15360/21005 (73.1%) - bs=32\n",
      "[PROG] 15392/21005 (73.3%) - bs=32\n",
      "[PROG] 15424/21005 (73.4%) - bs=32\n",
      "[PROG] 15456/21005 (73.6%) - bs=32\n",
      "[PROG] 15488/21005 (73.7%) - bs=32\n",
      "[PROG] 15520/21005 (73.9%) - bs=32\n",
      "[PROG] 15552/21005 (74.0%) - bs=32\n",
      "[PROG] 15584/21005 (74.2%) - bs=32\n",
      "[PROG] 15616/21005 (74.3%) - bs=32\n",
      "[PROG] 15648/21005 (74.5%) - bs=32\n",
      "[PROG] 15680/21005 (74.6%) - bs=32\n",
      "[PROG] 15712/21005 (74.8%) - bs=32\n",
      "[PROG] 15744/21005 (75.0%) - bs=32\n",
      "[PROG] 15776/21005 (75.1%) - bs=32\n",
      "[PROG] 15808/21005 (75.3%) - bs=32\n",
      "[PROG] 15840/21005 (75.4%) - bs=32\n",
      "[PROG] 15872/21005 (75.6%) - bs=32\n",
      "[PROG] 15904/21005 (75.7%) - bs=32\n",
      "[PROG] 15936/21005 (75.9%) - bs=32\n",
      "[PROG] 15968/21005 (76.0%) - bs=32\n",
      "[PROG] 16000/21005 (76.2%) - bs=32\n",
      "[PROG] 16032/21005 (76.3%) - bs=32\n",
      "[PROG] 16064/21005 (76.5%) - bs=32\n",
      "[PROG] 16096/21005 (76.6%) - bs=32\n",
      "[PROG] 16128/21005 (76.8%) - bs=32\n",
      "[PROG] 16160/21005 (76.9%) - bs=32\n",
      "[PROG] 16192/21005 (77.1%) - bs=32\n",
      "[PROG] 16224/21005 (77.2%) - bs=32\n",
      "[PROG] 16256/21005 (77.4%) - bs=32\n",
      "[PROG] 16288/21005 (77.5%) - bs=32\n",
      "[PROG] 16320/21005 (77.7%) - bs=32\n",
      "[PROG] 16352/21005 (77.8%) - bs=32\n",
      "[PROG] 16384/21005 (78.0%) - bs=32\n",
      "[PROG] 16416/21005 (78.2%) - bs=32\n",
      "[PROG] 16448/21005 (78.3%) - bs=32\n",
      "[PROG] 16480/21005 (78.5%) - bs=32\n",
      "[PROG] 16512/21005 (78.6%) - bs=32\n",
      "[PROG] 16544/21005 (78.8%) - bs=32\n",
      "[PROG] 16576/21005 (78.9%) - bs=32\n",
      "[PROG] 16608/21005 (79.1%) - bs=32\n",
      "[PROG] 16640/21005 (79.2%) - bs=32\n",
      "[PROG] 16672/21005 (79.4%) - bs=32\n",
      "[PROG] 16704/21005 (79.5%) - bs=32\n",
      "[PROG] 16736/21005 (79.7%) - bs=32\n",
      "[PROG] 16768/21005 (79.8%) - bs=32\n",
      "[PROG] 16800/21005 (80.0%) - bs=32\n",
      "[PROG] 16832/21005 (80.1%) - bs=32\n",
      "[PROG] 16864/21005 (80.3%) - bs=32\n",
      "[PROG] 16896/21005 (80.4%) - bs=32\n",
      "[PROG] 16928/21005 (80.6%) - bs=32\n",
      "[PROG] 16960/21005 (80.7%) - bs=32\n",
      "[PROG] 16992/21005 (80.9%) - bs=32\n",
      "[PROG] 17024/21005 (81.0%) - bs=32\n",
      "[PROG] 17056/21005 (81.2%) - bs=32\n",
      "[PROG] 17088/21005 (81.4%) - bs=32\n",
      "[PROG] 17120/21005 (81.5%) - bs=32\n",
      "[PROG] 17152/21005 (81.7%) - bs=32\n",
      "[PROG] 17184/21005 (81.8%) - bs=32\n",
      "[PROG] 17216/21005 (82.0%) - bs=32\n",
      "[PROG] 17248/21005 (82.1%) - bs=32\n",
      "[PROG] 17280/21005 (82.3%) - bs=32\n",
      "[PROG] 17312/21005 (82.4%) - bs=32\n",
      "[PROG] 17344/21005 (82.6%) - bs=32\n",
      "[PROG] 17376/21005 (82.7%) - bs=32\n",
      "[PROG] 17408/21005 (82.9%) - bs=32\n",
      "[PROG] 17440/21005 (83.0%) - bs=32\n",
      "[PROG] 17472/21005 (83.2%) - bs=32\n",
      "[PROG] 17504/21005 (83.3%) - bs=32\n",
      "[PROG] 17536/21005 (83.5%) - bs=32\n",
      "[PROG] 17568/21005 (83.6%) - bs=32\n",
      "[PROG] 17600/21005 (83.8%) - bs=32\n",
      "[PROG] 17632/21005 (83.9%) - bs=32\n",
      "[PROG] 17664/21005 (84.1%) - bs=32\n",
      "[PROG] 17696/21005 (84.2%) - bs=32\n",
      "[PROG] 17728/21005 (84.4%) - bs=32\n",
      "[PROG] 17760/21005 (84.6%) - bs=32\n",
      "[PROG] 17792/21005 (84.7%) - bs=32\n",
      "[PROG] 17824/21005 (84.9%) - bs=32\n",
      "[PROG] 17856/21005 (85.0%) - bs=32\n",
      "[PROG] 17888/21005 (85.2%) - bs=32\n",
      "[PROG] 17920/21005 (85.3%) - bs=32\n",
      "[PROG] 17952/21005 (85.5%) - bs=32\n",
      "[PROG] 17984/21005 (85.6%) - bs=32\n",
      "[PROG] 18016/21005 (85.8%) - bs=32\n",
      "[PROG] 18048/21005 (85.9%) - bs=32\n",
      "[PROG] 18080/21005 (86.1%) - bs=32\n",
      "[PROG] 18112/21005 (86.2%) - bs=32\n",
      "[PROG] 18144/21005 (86.4%) - bs=32\n",
      "[PROG] 18176/21005 (86.5%) - bs=32\n",
      "[PROG] 18208/21005 (86.7%) - bs=32\n",
      "[PROG] 18240/21005 (86.8%) - bs=32\n",
      "[PROG] 18272/21005 (87.0%) - bs=32\n",
      "[PROG] 18304/21005 (87.1%) - bs=32\n",
      "[PROG] 18336/21005 (87.3%) - bs=32\n",
      "[PROG] 18368/21005 (87.4%) - bs=32\n",
      "[PROG] 18400/21005 (87.6%) - bs=32\n",
      "[PROG] 18432/21005 (87.8%) - bs=32\n",
      "[PROG] 18464/21005 (87.9%) - bs=32\n",
      "[PROG] 18496/21005 (88.1%) - bs=32\n",
      "[PROG] 18528/21005 (88.2%) - bs=32\n",
      "[PROG] 18560/21005 (88.4%) - bs=32\n",
      "[PROG] 18592/21005 (88.5%) - bs=32\n",
      "[PROG] 18624/21005 (88.7%) - bs=32\n",
      "[PROG] 18656/21005 (88.8%) - bs=32\n",
      "[PROG] 18688/21005 (89.0%) - bs=32\n",
      "[PROG] 18720/21005 (89.1%) - bs=32\n",
      "[PROG] 18752/21005 (89.3%) - bs=32\n",
      "[PROG] 18784/21005 (89.4%) - bs=32\n",
      "[PROG] 18816/21005 (89.6%) - bs=32\n",
      "[PROG] 18848/21005 (89.7%) - bs=32\n",
      "[PROG] 18880/21005 (89.9%) - bs=32\n",
      "[PROG] 18912/21005 (90.0%) - bs=32\n",
      "[PROG] 18944/21005 (90.2%) - bs=32\n",
      "[PROG] 18976/21005 (90.3%) - bs=32\n",
      "[PROG] 19008/21005 (90.5%) - bs=32\n",
      "[PROG] 19040/21005 (90.6%) - bs=32\n",
      "[PROG] 19072/21005 (90.8%) - bs=32\n",
      "[PROG] 19104/21005 (90.9%) - bs=32\n",
      "[PROG] 19136/21005 (91.1%) - bs=32\n",
      "[PROG] 19168/21005 (91.3%) - bs=32\n",
      "[PROG] 19200/21005 (91.4%) - bs=32\n",
      "[PROG] 19232/21005 (91.6%) - bs=32\n",
      "[PROG] 19264/21005 (91.7%) - bs=32\n",
      "[PROG] 19296/21005 (91.9%) - bs=32\n",
      "[PROG] 19328/21005 (92.0%) - bs=32\n",
      "[PROG] 19360/21005 (92.2%) - bs=32\n",
      "[PROG] 19392/21005 (92.3%) - bs=32\n",
      "[PROG] 19424/21005 (92.5%) - bs=32\n",
      "[PROG] 19456/21005 (92.6%) - bs=32\n",
      "[PROG] 19488/21005 (92.8%) - bs=32\n",
      "[PROG] 19520/21005 (92.9%) - bs=32\n",
      "[PROG] 19552/21005 (93.1%) - bs=32\n",
      "[PROG] 19584/21005 (93.2%) - bs=32\n",
      "[PROG] 19616/21005 (93.4%) - bs=32\n",
      "[PROG] 19648/21005 (93.5%) - bs=32\n",
      "[PROG] 19680/21005 (93.7%) - bs=32\n",
      "[PROG] 19712/21005 (93.8%) - bs=32\n",
      "[PROG] 19744/21005 (94.0%) - bs=32\n",
      "[PROG] 19776/21005 (94.1%) - bs=32\n",
      "[PROG] 19808/21005 (94.3%) - bs=32\n",
      "[PROG] 19840/21005 (94.5%) - bs=32\n",
      "[PROG] 19872/21005 (94.6%) - bs=32\n",
      "[PROG] 19904/21005 (94.8%) - bs=32\n",
      "[PROG] 19936/21005 (94.9%) - bs=32\n",
      "[PROG] 19968/21005 (95.1%) - bs=32\n",
      "[PROG] 20000/21005 (95.2%) - bs=32\n",
      "[PROG] 20032/21005 (95.4%) - bs=32\n",
      "[PROG] 20064/21005 (95.5%) - bs=32\n",
      "[PROG] 20096/21005 (95.7%) - bs=32\n",
      "[PROG] 20128/21005 (95.8%) - bs=32\n",
      "[PROG] 20160/21005 (96.0%) - bs=32\n",
      "[PROG] 20192/21005 (96.1%) - bs=32\n",
      "[PROG] 20224/21005 (96.3%) - bs=32\n",
      "[PROG] 20256/21005 (96.4%) - bs=32\n",
      "[PROG] 20288/21005 (96.6%) - bs=32\n",
      "[PROG] 20320/21005 (96.7%) - bs=32\n",
      "[PROG] 20352/21005 (96.9%) - bs=32\n",
      "[PROG] 20384/21005 (97.0%) - bs=32\n",
      "[PROG] 20416/21005 (97.2%) - bs=32\n",
      "[PROG] 20448/21005 (97.3%) - bs=32\n",
      "[PROG] 20480/21005 (97.5%) - bs=32\n",
      "[PROG] 20512/21005 (97.7%) - bs=32\n",
      "[PROG] 20544/21005 (97.8%) - bs=32\n",
      "[PROG] 20576/21005 (98.0%) - bs=32\n",
      "[PROG] 20608/21005 (98.1%) - bs=32\n",
      "[PROG] 20640/21005 (98.3%) - bs=32\n",
      "[PROG] 20672/21005 (98.4%) - bs=32\n",
      "[PROG] 20704/21005 (98.6%) - bs=32\n",
      "[PROG] 20736/21005 (98.7%) - bs=32\n",
      "[PROG] 20768/21005 (98.9%) - bs=32\n",
      "[PROG] 20800/21005 (99.0%) - bs=32\n",
      "[PROG] 20832/21005 (99.2%) - bs=32\n",
      "[PROG] 20864/21005 (99.3%) - bs=32\n",
      "[PROG] 20896/21005 (99.5%) - bs=32\n",
      "[PROG] 20928/21005 (99.6%) - bs=32\n",
      "[PROG] 20960/21005 (99.8%) - bs=32\n",
      "[PROG] 20992/21005 (99.9%) - bs=32\n",
      "[PROG] 21005/21005 (100.0%) - bs=32\n",
      "[OK] FAISS guardado: faiss_index_ip.bin | ntotal=21005 | dim=384\n",
      "[OK] PKL (meta_min) guardado: embeddings_meta_min.pkl\n"
     ]
    }
   ],
   "source": [
    "import os, gc, pickle, numpy as np, pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import Optional\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "PARQUET_PATH   = os.environ.get(\"PARQUET_PATH\", \"corpus_chunks.parquet\")   # <-- solo contiene los chunks\n",
    "FAISS_PATH     = os.environ.get(\"FAISS_PATH\", \"faiss_index_ip.bin\")\n",
    "PKL_MIN_PATH   = os.environ.get(\"PKL_MIN_PATH\", \"embeddings_meta_min.pkl\")\n",
    "\n",
    "\n",
    "# Modelo recomendado para CPU\n",
    "EMB_MODEL        = os.environ.get(\"EMB_MODEL\", \"intfloat/multilingual-e5-small\")\n",
    "EMB_MAX_SEQ_LEN  = int(os.environ.get(\"EMB_MAX_SEQ_LEN\", \"300\"))   # <=512\n",
    "INIT_BATCH       = int(os.environ.get(\"BATCH_SIZE\", \"32\"))\n",
    "MIN_BATCH        = 1\n",
    "\n",
    "# -------------------- 0) Carga parquet de chunks + limpieza --------------------\n",
    "try:\n",
    "    chunks_df = pd.read_parquet(PARQUET_PATH, engine=\"pyarrow\")\n",
    "except Exception:\n",
    "    chunks_df = pd.read_parquet(PARQUET_PATH, engine=\"fastparquet\")\n",
    "\n",
    "required_cols = {\"doc_id\",\"chunk_id\",\"start_token\",\"end_token\",\"text_chunk\"}\n",
    "missing = required_cols - set(chunks_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas requeridas en {PARQUET_PATH}: {missing}\")\n",
    "\n",
    "# Normaliza tipos\n",
    "for c in [\"doc_id\",\"chunk_id\",\"start_token\",\"end_token\"]:\n",
    "    if chunks_df[c].dtype.kind not in \"iu\":\n",
    "        chunks_df[c] = pd.to_numeric(chunks_df[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "# Limpieza texto\n",
    "chunks_df[\"text_chunk\"] = chunks_df[\"text_chunk\"].astype(str).str.strip()\n",
    "chunks_df = chunks_df[chunks_df[\"text_chunk\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# chunk_uid y scopus_id opcional\n",
    "if \"chunk_uid\" not in chunks_df.columns:\n",
    "    chunks_df[\"chunk_uid\"] = chunks_df[\"doc_id\"].astype(str) + \"-\" + chunks_df[\"chunk_id\"].astype(str)\n",
    "if \"scopus_id\" in chunks_df.columns:\n",
    "    chunks_df[\"scopus_id\"] = chunks_df[\"scopus_id\"].astype(str)\n",
    "\n",
    "# IDs vectoriales alineados 0..N-1\n",
    "N = len(chunks_df)\n",
    "chunks_df[\"vec_id\"] = np.arange(N, dtype=\"int64\")\n",
    "chunks_df[\"embedding_model\"] = EMB_MODEL\n",
    "\n",
    "# -------------------- 1) Modelo (CPU) alineado con chunking --------------------\n",
    "print(f\"[INFO] device encode: cpu; model={EMB_MODEL}\")\n",
    "model = SentenceTransformer(EMB_MODEL, device=\"cpu\")\n",
    "model.max_seq_length = min(EMB_MAX_SEQ_LEN, 512)\n",
    "print(f\"[INFO] model.max_seq_length = {model.max_seq_length}\")\n",
    "\n",
    "# Prefijo E5\n",
    "passages = (\"passage: \" + chunks_df[\"text_chunk\"]).tolist()\n",
    "\n",
    "# -------------------- 2) FAISS (IP con embeddings normalizados -> coseno) --------------------\n",
    "def make_faiss_index(dim: int):\n",
    "    print(\"[INFO] FAISS-CPU\")\n",
    "    return faiss.IndexFlatIP(dim)\n",
    "\n",
    "def st_encode_cpu(texts, batch_size, normalize=True, to_numpy=True):\n",
    "    embs = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=False,\n",
    "        normalize_embeddings=normalize,\n",
    "        convert_to_numpy=to_numpy\n",
    "    )\n",
    "    return np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "# -------------------- 3) Bucle por lotes (CPU) --------------------\n",
    "def stream_encode_and_build(passages, init_bs=INIT_BATCH, min_bs=MIN_BATCH):\n",
    "    i, bs = 0, init_bs\n",
    "    index = None\n",
    "    dim = None\n",
    "\n",
    "    while i < N:\n",
    "        j = min(i + bs, N)\n",
    "        batch = passages[i:j]\n",
    "        try:\n",
    "            emb = st_encode_cpu(batch, batch_size=bs, normalize=True, to_numpy=True)\n",
    "\n",
    "            if dim is None:\n",
    "                dim = emb.shape[1]\n",
    "                index = make_faiss_index(dim)\n",
    "\n",
    "            index.add(emb)\n",
    "\n",
    "            i = j\n",
    "            print(f\"[PROG] {i}/{N} ({100.0*i/N:.1f}%) - bs={bs}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            prev_bs = bs\n",
    "            bs = max(min_bs, bs // 2)\n",
    "            gc.collect()\n",
    "            if prev_bs == bs and bs == min_bs:\n",
    "                raise RuntimeError(f\"Fallo persistente en CPU con batch={bs}: {e}\") from e\n",
    "            print(f\"[WARN] Error en i={i}. Bajo batch {prev_bs}->{bs} y reintento…\")\n",
    "            continue\n",
    "\n",
    "    return index, dim\n",
    "\n",
    "# -------------------- 4) Ejecutar pipeline --------------------\n",
    "index_cpu, dim = stream_encode_and_build(passages)\n",
    "\n",
    "# -------------------- 5) Guardar FAISS + PKL (mapa mínimo) --------------------\n",
    "faiss.write_index(index_cpu, FAISS_PATH)\n",
    "print(f\"[OK] FAISS guardado: {FAISS_PATH} | ntotal={index_cpu.ntotal} | dim={dim}\")\n",
    "\n",
    "# PKL: guardar meta_min con scopus_id si existe\n",
    "min_cols = [\"vec_id\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"start_token\",\"end_token\"]\n",
    "if \"scopus_id\" in chunks_df.columns:\n",
    "    min_cols.append(\"scopus_id\")\n",
    "\n",
    "with open(PKL_MIN_PATH, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"model\": EMB_MODEL,\n",
    "        \"device_used\": \"cpu\",\n",
    "        \"dim\": dim,\n",
    "        \"meta_min\": chunks_df[min_cols].copy()\n",
    "    }, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"[OK] PKL (meta_min) guardado: {PKL_MIN_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a9a44",
   "metadata": {},
   "source": [
    "## recuperacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471578c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP-K (meta mínimo) ===\n",
      "[INFO] Modelo: intfloat/multilingual-e5-small | max_seq_length=300\n",
      "[INFO] meta_min columnas: ['vec_id', 'chunk_uid', 'doc_id', 'chunk_id', 'start_token', 'end_token', 'scopus_id'] | filas=21005\n",
      "[INFO] Índice FAISS cargado: ntotal=21005\n",
      "   vec_id     score chunk_uid  doc_id  chunk_id    scopus_id  start_token  \\\n",
      "0    3933  0.874905    3534-0    3534         0  85126742544            0   \n",
      "1   11456  0.867839   10461-0   10461         0  85161272258            0   \n",
      "2    7683  0.866706    6982-0    6982         0  85142396040            0   \n",
      "3   10501  0.866331    9586-0    9586         0  85152377788            0   \n",
      "4   20104  0.866028   18406-0   18406         0  85198537488            0   \n",
      "\n",
      "   end_token  \n",
      "0        137  \n",
      "1        103  \n",
      "2         65  \n",
      "3        158  \n",
      "4        168  \n",
      "\n",
      "=== TOP-K + TODA la metadata del CSV ===\n",
      "[INFO] scoupusdata.csv: filas=19233 | cols=9\n",
      "   vec_id     score chunk_uid  doc_id  chunk_id    scopus_id  start_token  \\\n",
      "0    3933  0.874905    3534-0    3534         0  85126742544            0   \n",
      "1   11456  0.867839   10461-0   10461         0  85161272258            0   \n",
      "2    7683  0.866706    6982-0    6982         0  85142396040            0   \n",
      "3   10501  0.866331    9586-0    9586         0  85152377788            0   \n",
      "4   20104  0.866028   18406-0   18406         0  85198537488            0   \n",
      "\n",
      "   end_token                                              title  \\\n",
      "0        137  ANCESTRAL SEEDS OF FREEDOM: CIMARRONA WISDOM A...   \n",
      "1        103  Indigeneity coalesced: The 2022 national strik...   \n",
      "2         65  Linking communication in crisis and citizen me...   \n",
      "3        158  THE NATIONAL MOBILIZATION OF OCTOBER 2019 IN E...   \n",
      "4        168  EARTHQUAKE OF APRIL 16, 2016, IN ECUADOR: A GE...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  In Ecuador, a population of African origin was...   \n",
      "1  This article analyses the national strike in E...   \n",
      "2  In the context of the pandemic, the government...   \n",
      "3  Ecuador lived eleven days of social protest in...   \n",
      "4  In 2016, Ecuador suffered an earthquake of 7.8...   \n",
      "\n",
      "                               doi  \\\n",
      "0  10.4067/S0719-26812020000300025   \n",
      "1          10.1111/1467-8322.12814   \n",
      "2       10.37467/revvisual.v9.3680   \n",
      "3             10.3989/dra.2022.034   \n",
      "4          10.55467/reder.v8i2.159   \n",
      "\n",
      "                                             authors  \\\n",
      "0                                    José Chalá Cruz   \n",
      "1     Camila del Mármol; Jordi Gascón; Víctor Bretón   \n",
      "2                                  Paola Ulloa-López   \n",
      "3                               Andrea Madrid Tamayo   \n",
      "4  José Luis Sánchez-Cortez; Marco Simbaña-Tasiguano   \n",
      "\n",
      "                                        affiliations affiliation_cities  \\\n",
      "0          Universidad Andina Simón Bolívar, Ecuador              Quito   \n",
      "1           FLACSO Ecuador; Universitat de Barcelona   Quito; Barcelona   \n",
      "2   Escuela Superior Politecnica del Litoral Ecuador          Guayaquil   \n",
      "3  Instituto de la Democracia del Consejo Naciona...                NaN   \n",
      "4  Universidad Nacional Autónoma de México; Yacha...      Mexico; Quito   \n",
      "\n",
      "  affiliation_countries  citation_count  \n",
      "0               Ecuador               0  \n",
      "1        Ecuador; Spain               0  \n",
      "2               Ecuador               0  \n",
      "3               Ecuador               0  \n",
      "4       Mexico; Ecuador               0  \n"
     ]
    }
   ],
   "source": [
    "import os, pickle\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---- Rutas (ajústalas o usa variables de entorno) ----\n",
    "PKL_MIN_PATH = os.environ.get(\"PKL_MIN_PATH\", \"embeddings_meta_min.pkl\")\n",
    "FAISS_PATH   = os.environ.get(\"FAISS_PATH\", \"faiss_index_ip.bin\")\n",
    "SCOPUS_CSV   = os.environ.get(\"SCOPUS_CSV\", \"scopusdata.csv\")\n",
    "SCOPUS_SEP   = os.environ.get(\"SCOPUS_SEP\", \"|\") \n",
    "\n",
    "# ---- Caches simples ----\n",
    "_model_cache = None\n",
    "_meta_min_cache = None\n",
    "_index_cache = None\n",
    "_scopus_cache = None\n",
    "\n",
    "def load_pkl_and_model(emb_max_seq_len=300):\n",
    "    global _model_cache, _meta_min_cache\n",
    "    if _model_cache is not None and _meta_min_cache is not None:\n",
    "        return _model_cache, _meta_min_cache\n",
    "    with open(PKL_MIN_PATH, \"rb\") as f:\n",
    "        pkl = pickle.load(f)\n",
    "\n",
    "    meta_min = pkl[\"meta_min\"].copy()  # DataFrame: vec_id, chunk_uid, doc_id, chunk_id, (scopus_id), start/end\n",
    "    _meta_min_cache = meta_min\n",
    "\n",
    "    model_name = pkl.get(\"model\", \"intfloat/multilingual-e5-small\")\n",
    "    model = SentenceTransformer(model_name, device=\"cpu\")\n",
    "    model.max_seq_length = min(int(emb_max_seq_len), 512)\n",
    "    _model_cache = model\n",
    "\n",
    "    print(f\"[INFO] Modelo: {model_name} | max_seq_length={model.max_seq_length}\")\n",
    "    print(f\"[INFO] meta_min columnas: {list(meta_min.columns)} | filas={len(meta_min)}\")\n",
    "    return _model_cache, _meta_min_cache\n",
    "\n",
    "def load_faiss():\n",
    "    global _index_cache\n",
    "    if _index_cache is None:\n",
    "        _index_cache = faiss.read_index(FAISS_PATH)\n",
    "        print(f\"[INFO] Índice FAISS cargado: ntotal={_index_cache.ntotal}\")\n",
    "    return _index_cache\n",
    "\n",
    "def load_scopus_csv():\n",
    "    global _scopus_cache\n",
    "    if _scopus_cache is None:\n",
    "        df = pd.read_csv(SCOPUS_CSV, sep=SCOPUS_SEP)\n",
    "        if \"scopus_id\" not in df.columns:\n",
    "            raise ValueError(f\"{SCOPUS_CSV} no tiene columna 'scopus_id'\")\n",
    "        df[\"scopus_id\"] = df[\"scopus_id\"].astype(str)\n",
    "        _scopus_cache = df\n",
    "        print(f\"[INFO] scoupusdata.csv: filas={len(df)} | cols={len(df.columns)}\")\n",
    "    return _scopus_cache\n",
    "\n",
    "def e5_encode_query(model, query_text: str):\n",
    "    return model.encode([f\"query: {query_text}\"],\n",
    "                        normalize_embeddings=True,\n",
    "                        convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "def search_min(query_text: str, topk: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve SOLO el meta mínimo del PKL (sin CSV):\n",
    "    vec_id, score, chunk_uid, doc_id, chunk_id, (scopus_id si existe), start/end\n",
    "    \"\"\"\n",
    "    model, meta_min = load_pkl_and_model()\n",
    "    index = load_faiss()\n",
    "\n",
    "    q = e5_encode_query(model, query_text)\n",
    "    D, I = index.search(q, topk)\n",
    "    vec_ids = I[0].tolist()\n",
    "\n",
    "    hits = meta_min.set_index(\"vec_id\").loc[vec_ids].reset_index()\n",
    "    hits.insert(1, \"score\", D[0])\n",
    "\n",
    "    cols_front = [c for c in [\"vec_id\",\"score\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"scopus_id\",\"start_token\",\"end_token\"] if c in hits.columns]\n",
    "    rest = [c for c in hits.columns if c not in cols_front]\n",
    "    return hits[cols_front + rest].reset_index(drop=True)\n",
    "\n",
    "def search_full_scopus(query_text: str, topk: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Une el TOP-K con TODAS las columnas de scoupusdata.csv por scopus_id.\n",
    "    \"\"\"\n",
    "    model, meta_min = load_pkl_and_model()\n",
    "    index = load_faiss()\n",
    "    sc = load_scopus_csv()\n",
    "\n",
    "    q = e5_encode_query(model, query_text)\n",
    "    D, I = index.search(q, topk)\n",
    "    vec_ids = I[0].tolist()\n",
    "\n",
    "    hits = meta_min.set_index(\"vec_id\").loc[vec_ids].reset_index()\n",
    "    hits.insert(1, \"score\", D[0])\n",
    "\n",
    "    if \"scopus_id\" not in hits.columns:\n",
    "        raise ValueError(\"meta_min en PKL no contiene 'scopus_id'; no puedo unir con el CSV.\")\n",
    "\n",
    "    out = hits.merge(sc, how=\"left\", on=\"scopus_id\")\n",
    "\n",
    "    # Orden: primero claves/score/offsets, luego TODO el CSV\n",
    "    front = [c for c in [\"vec_id\",\"score\",\"chunk_uid\",\"doc_id\",\"chunk_id\",\"scopus_id\",\"start_token\",\"end_token\"] if c in out.columns]\n",
    "    csv_cols = [c for c in sc.columns if c not in front]\n",
    "    return out[front + csv_cols].reset_index(drop=True)\n",
    "\n",
    "# ====== DEMO RÁPIDA ======\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Ecuador\"   # cambia por tu consulta\n",
    "    print(\"\\n=== TOP-K (meta mínimo) ===\")\n",
    "    print(search_min(query, topk=5))\n",
    "\n",
    "    print(\"\\n=== TOP-K + TODA la metadata del CSV ===\")\n",
    "    df = search_full_scopus(query, topk=5)\n",
    "    # Si quieres guardar para revisar en Excel:\n",
    "    # df.to_csv(\"search_results_full_scopus.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(df.head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
